<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear model estimation | A Progressive Introduction to Linear Models</title>
  <meta name="description" content="A collection of material that progressively introduces how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear model estimation | A Progressive Introduction to Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of material that progressively introduces how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear model estimation | A Progressive Introduction to Linear Models" />
  
  <meta name="twitter:description" content="A collection of material that progressively introduces how to fit and use linear models." />
  

<meta name="author" content="Joshua French" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-cleaning-and-exploration.html"/>
<link rel="next" href="interp-chapter.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Progressive Introduction to Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminaries</a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#setting-up-r-and-rstudio-desktop"><i class="fa fa-check"></i><b>1.1</b> Setting up R and RStudio Desktop</a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.2</b> Running code, scripts, and comments</a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.3</b> Assignment</a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#functions"><i class="fa fa-check"></i><b>1.4</b> Functions</a></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages</a></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help</a></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types</a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types</a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.8</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.8.1</b> Creation</a></li>
<li class="chapter" data-level="1.8.2" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.8.2</b> Categorical vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.8.3</b> Extracting parts of a vector</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.9</b> Helpful functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.9.1</b> General functions</a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.9.2</b> Functions related to statistical distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.10</b> Data Frames</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#direct-creation"><i class="fa fa-check"></i><b>1.10.1</b> Direct creation</a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.10.2</b> Importing Data</a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.10.3</b> Extracting parts of a data frame</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#using-the-pipe-operator"><i class="fa fa-check"></i><b>1.11</b> Using the pipe operator</a></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#dealing-with-common-problems"><i class="fa fa-check"></i><b>1.12</b> Dealing with common problems</a></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.13</b> Ecosystem debate</a></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#additional-information"><i class="fa fa-check"></i><b>1.14</b> Additional information</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="r-foundations.html"><a href="r-foundations.html#comparing-assignment-operators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing assignment operators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html"><i class="fa fa-check"></i><b>2</b> Data cleaning and exploration</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#raw-palmer-penguins-data"><i class="fa fa-check"></i><b>2.1</b> Raw Palmer penguins data</a></li>
<li class="chapter" data-level="2.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#initial-data-cleaning"><i class="fa fa-check"></i><b>2.2</b> Initial data cleaning</a></li>
<li class="chapter" data-level="2.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numerical-summarization-of-data"><i class="fa fa-check"></i><b>2.3</b> Numerical summarization of data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numeric-data"><i class="fa fa-check"></i><b>2.3.1</b> Numeric data</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#categorical-data"><i class="fa fa-check"></i><b>2.3.2</b> Categorical data</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-summary-function"><i class="fa fa-check"></i><b>2.3.3</b> The <code>summary</code> function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.4</b> Visual summaries of data</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-ggplot-recipe"><i class="fa fa-check"></i><b>2.4.1</b> The ggplot recipe</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#univariate-plots"><i class="fa fa-check"></i><b>2.4.2</b> Univariate plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#bivariate-plots"><i class="fa fa-check"></i><b>2.4.3</b> Bivariate plots</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#multivariate-plots"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate plots</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#a-plan-for-data-cleaning-and-exploration"><i class="fa fa-check"></i><b>2.5</b> A plan for data cleaning and exploration</a></li>
<li class="chapter" data-level="2.6" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#final-notes-on-missing-or-erroneous-data"><i class="fa fa-check"></i><b>2.6</b> Final notes on missing or erroneous data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>3</b> Linear model estimation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>3.1</b> A simple motivating example</a></li>
<li class="chapter" data-level="3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s-slr-estimation"><i class="fa fa-check"></i><b>3.2</b> Estimation of the simple linear regression model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>3.2.1</b> Model definition, fitted values, residuals, and RSS</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>3.2.2</b> OLS estimators of the simple linear regression parameters</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>3.3</b> Penguins simple linear regression example</a></li>
<li class="chapter" data-level="3.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>3.4</b> Defining a linear model</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>3.4.1</b> Necessary components and notation</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>3.4.2</b> Standard definition of linear model</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5</b> Estimation of the multiple linear regression model</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>3.5.1</b> Using matrix notation to represent a linear model</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>3.5.2</b> Residuals, fitted values, and RSS for multiple linear regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.5.3</b> OLS estimator of the regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr"><i class="fa fa-check"></i><b>3.6</b> Penguins multiple linear regression example</a></li>
<li class="chapter" data-level="3.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#model-types"><i class="fa fa-check"></i><b>3.7</b> Types of linear models</a></li>
<li class="chapter" data-level="3.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#categorical-predictors"><i class="fa fa-check"></i><b>3.8</b> Categorical predictors</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#indicator-variables"><i class="fa fa-check"></i><b>3.8.1</b> Indicator variables</a></li>
<li class="chapter" data-level="3.8.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#parallel-and-separate-lines-models"><i class="fa fa-check"></i><b>3.8.2</b> Parallel and separate lines models</a></li>
<li class="chapter" data-level="3.8.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#extensions"><i class="fa fa-check"></i><b>3.8.3</b> Extensions</a></li>
<li class="chapter" data-level="3.8.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#avoiding-an-easy-mistake"><i class="fa fa-check"></i><b>3.8.4</b> Avoiding an easy mistake</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr2"><i class="fa fa-check"></i><b>3.9</b> Penguins example with categorical predictor</a></li>
<li class="chapter" data-level="3.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#evaluating-model-fit"><i class="fa fa-check"></i><b>3.10</b> Evaluating model fit</a></li>
<li class="chapter" data-level="3.11" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary"><i class="fa fa-check"></i><b>3.11</b> Summary</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:term-summary"><i class="fa fa-check"></i><b>3.11.1</b> Summary of terms</a></li>
<li class="chapter" data-level="3.11.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-functions"><i class="fa fa-check"></i><b>3.11.2</b> Summary of functions</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#going-deeper"><i class="fa fa-check"></i><b>3.12</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#degrees-of-freedom"><i class="fa fa-check"></i><b>3.12.1</b> Degrees of freedom</a></li>
<li class="chapter" data-level="3.12.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#slr-derivation"><i class="fa fa-check"></i><b>3.12.2</b> Derivation of the OLS estimators of the simple linear regression model coefficients</a></li>
<li class="chapter" data-level="3.12.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#unbiasedness-of-ols-estimators"><i class="fa fa-check"></i><b>3.12.3</b> Unbiasedness of OLS estimators</a></li>
<li class="chapter" data-level="3.12.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>3.12.4</b> Manual calculation Penguins simple linear regression example</a></li>
<li class="chapter" data-level="3.12.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#mlr-derivation"><i class="fa fa-check"></i><b>3.12.5</b> Derivation of the OLS estimator for the multiple linear regression model coefficients</a></li>
<li class="chapter" data-level="3.12.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example"><i class="fa fa-check"></i><b>3.12.6</b> Manual calculation of Penguins multiple linear regression example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interp-chapter.html"><a href="interp-chapter.html"><i class="fa fa-check"></i><b>4</b> Interpreting a fitted linear model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="interp-chapter.html"><a href="interp-chapter.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>4.1</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="interp-chapter.html"><a href="interp-chapter.html#interpretation-for-simple-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation for simple linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="interp-chapter.html"><a href="interp-chapter.html#interp-1st-order-ml"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation for first-order multiple linear regression models</a></li>
<li class="chapter" data-level="4.1.3" data-path="interp-chapter.html"><a href="interp-chapter.html#regressor-roles"><i class="fa fa-check"></i><b>4.1.3</b> Roles of regressor variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots"><i class="fa fa-check"></i><b>4.2</b> Effect plots</a></li>
<li class="chapter" data-level="4.3" data-path="interp-chapter.html"><a href="interp-chapter.html#interp-cat-predictor"><i class="fa fa-check"></i><b>4.3</b> Interpretation for categorical predictors</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="interp-chapter.html"><a href="interp-chapter.html#pl-interp"><i class="fa fa-check"></i><b>4.3.1</b> Coefficient interpretation for parallel lines models</a></li>
<li class="chapter" data-level="4.3.2" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots-for-fitted-models-with-non-interacting-categorical-predictors"><i class="fa fa-check"></i><b>4.3.2</b> Effect plots for fitted models with non-interacting categorical predictors</a></li>
<li class="chapter" data-level="4.3.3" data-path="interp-chapter.html"><a href="interp-chapter.html#sl-interp"><i class="fa fa-check"></i><b>4.3.3</b> Coefficient interpretation for separate lines models</a></li>
<li class="chapter" data-level="4.3.4" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots-for-interacting-categorical-predictors"><i class="fa fa-check"></i><b>4.3.4</b> Effect plots for interacting categorical predictors</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="interp-chapter.html"><a href="interp-chapter.html#added-variable-and-leverage-plots"><i class="fa fa-check"></i><b>4.4</b> Added-variable and leverage plots</a></li>
<li class="chapter" data-level="4.5" data-path="interp-chapter.html"><a href="interp-chapter.html#going-deeper-1"><i class="fa fa-check"></i><b>4.5</b> Going deeper</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="interp-chapter.html"><a href="interp-chapter.html#orthogonality"><i class="fa fa-check"></i><b>4.5.1</b> Orthogonality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-model-theory.html"><a href="linear-model-theory.html"><i class="fa fa-check"></i><b>5</b> Basic theoretical results for linear models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-model-theory.html"><a href="linear-model-theory.html#standard-assumptions"><i class="fa fa-check"></i><b>5.1</b> Standard assumptions</a></li>
<li class="chapter" data-level="5.2" data-path="linear-model-theory.html"><a href="linear-model-theory.html#summary-of-results"><i class="fa fa-check"></i><b>5.2</b> Summary of results</a></li>
<li class="chapter" data-level="5.3" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-mathbfy"><i class="fa fa-check"></i><b>5.3</b> Results for <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-hatboldsymbolbeta"><i class="fa fa-check"></i><b>5.4</b> Results for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-the-residuals"><i class="fa fa-check"></i><b>5.5</b> Results for the residuals</a></li>
<li class="chapter" data-level="5.6" data-path="linear-model-theory.html"><a href="linear-model-theory.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.6</b> The Gauss-Markov Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6</b> Linear model inference and prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html#overview-of-inference-and-prediction"><i class="fa fa-check"></i><b>6.1</b> Overview of inference and prediction</a></li>
<li class="chapter" data-level="6.2" data-path="inference.html"><a href="inference.html#necessary-notation"><i class="fa fa-check"></i><b>6.2</b> Necessary notation</a></li>
<li class="chapter" data-level="6.3" data-path="inference.html"><a href="inference.html#properties-betahat"><i class="fa fa-check"></i><b>6.3</b> Assumptions and properties of the OLS estimator</a></li>
<li class="chapter" data-level="6.4" data-path="inference.html"><a href="inference.html#parametric-confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i><b>6.4</b> Parametric confidence intervals for regression coefficients</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inference.html"><a href="inference.html#tci"><i class="fa fa-check"></i><b>6.4.1</b> Standard <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="inference.html"><a href="inference.html#mcp"><i class="fa fa-check"></i><b>6.5</b> The multiple comparisons problem</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="inference.html"><a href="inference.html#adjusted-cis-betas"><i class="fa fa-check"></i><b>6.5.1</b> Adjusted confidence intervals for regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference.html"><a href="inference.html#prediction-mean-response-versus-new-response"><i class="fa fa-check"></i><b>6.6</b> Prediction: mean response versus new response</a></li>
<li class="chapter" data-level="6.7" data-path="inference.html"><a href="inference.html#parametric-ci-mean-response"><i class="fa fa-check"></i><b>6.7</b> Confidence interval for the mean response</a></li>
<li class="chapter" data-level="6.8" data-path="inference.html"><a href="inference.html#pi-new-response"><i class="fa fa-check"></i><b>6.8</b> Prediction interval for a new response</a></li>
<li class="chapter" data-level="6.9" data-path="inference.html"><a href="inference.html#hypothesis-tests-for-a-single-regression-coefficient"><i class="fa fa-check"></i><b>6.9</b> Hypothesis tests for a single regression coefficient</a></li>
<li class="chapter" data-level="6.10" data-path="inference.html"><a href="inference.html#hypothesis-tests-for-multiple-regression-coefficients"><i class="fa fa-check"></i><b>6.10</b> Hypothesis tests for multiple regression coefficients</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="inference.html"><a href="inference.html#test-for-a-regression-relationship"><i class="fa fa-check"></i><b>6.10.1</b> Test for a regression relationship</a></li>
<li class="chapter" data-level="6.10.2" data-path="inference.html"><a href="inference.html#a-more-general-f-test"><i class="fa fa-check"></i><b>6.10.2</b> A more general F test</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="inference.html"><a href="inference.html#going-deeper-2"><i class="fa fa-check"></i><b>6.11</b> Going deeper</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="inference.html"><a href="inference.html#manual-t-cis"><i class="fa fa-check"></i><b>6.11.1</b> Manual calculation of the standard <span class="math inline">\(t\)</span>-based confidence interval for a regression coefficient</a></li>
<li class="chapter" data-level="6.11.2" data-path="inference.html"><a href="inference.html#mean-response-calculations"><i class="fa fa-check"></i><b>6.11.2</b> Details about estimation of the mean response</a></li>
<li class="chapter" data-level="6.11.3" data-path="inference.html"><a href="inference.html#manual-calc-ci-mean-response"><i class="fa fa-check"></i><b>6.11.3</b> Manual calculation of confidence intervals for the mean response</a></li>
<li class="chapter" data-level="6.11.4" data-path="inference.html"><a href="inference.html#new-response-pi-calculations"><i class="fa fa-check"></i><b>6.11.4</b> Details about prediction interval for a new response</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html"><i class="fa fa-check"></i><b>A</b> Overview of matrix facts</a>
<ul>
<li class="chapter" data-level="A.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#notation"><i class="fa fa-check"></i><b>A.1</b> Notation</a></li>
<li class="chapter" data-level="A.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>A.2</b> Basic mathematical operations</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>A.2.1</b> Addition and subtraction</a></li>
<li class="chapter" data-level="A.2.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>A.2.2</b> Scalar multiplication</a></li>
<li class="chapter" data-level="A.2.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>A.2.3</b> Matrix multiplication</a></li>
<li class="chapter" data-level="A.2.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose"><i class="fa fa-check"></i><b>A.2.4</b> Transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>A.3</b> Basic mathematical properties</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>A.3.1</b> Associative property</a></li>
<li class="chapter" data-level="A.3.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>A.3.2</b> Distributive property</a></li>
<li class="chapter" data-level="A.3.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>A.3.3</b> No commutative property</a></li>
<li class="chapter" data-level="A.3.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose-related-properties"><i class="fa fa-check"></i><b>A.3.4</b> Transpose-related properties</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>A.4</b> Special matrices</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>A.4.1</b> Square matrices</a></li>
<li class="chapter" data-level="A.4.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>A.4.2</b> Identity matrix</a></li>
<li class="chapter" data-level="A.4.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#diagonal-matrices"><i class="fa fa-check"></i><b>A.4.3</b> Diagonal matrices</a></li>
<li class="chapter" data-level="A.4.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#symmetric-matrices"><i class="fa fa-check"></i><b>A.4.4</b> Symmetric matrices</a></li>
<li class="chapter" data-level="A.4.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#idempotent-matrices"><i class="fa fa-check"></i><b>A.4.5</b> Idempotent matrices</a></li>
<li class="chapter" data-level="A.4.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.4.6</b> Positive definite matrices</a></li>
<li class="chapter" data-level="A.4.7" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#inverse-matrix"><i class="fa fa-check"></i><b>A.4.7</b> Inverse matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>A.5</b> Matrix derivatives</a></li>
<li class="chapter" data-level="A.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#additional-topics"><i class="fa fa-check"></i><b>A.6</b> Additional topics</a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#determinant"><i class="fa fa-check"></i><b>A.6.1</b> Determinant</a></li>
<li class="chapter" data-level="A.6.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#linearly-independent-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Linearly independent vectors</a></li>
<li class="chapter" data-level="A.6.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#rank"><i class="fa fa-check"></i><b>A.6.3</b> Rank</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="prob-review.html"><a href="prob-review.html"><i class="fa fa-check"></i><b>B</b> Overview of probability, random variables, and random vectors</a>
<ul>
<li class="chapter" data-level="B.1" data-path="prob-review.html"><a href="prob-review.html#probability-basics"><i class="fa fa-check"></i><b>B.1</b> Probability Basics</a></li>
<li class="chapter" data-level="B.2" data-path="prob-review.html"><a href="prob-review.html#random-variables"><i class="fa fa-check"></i><b>B.2</b> Random Variables</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="prob-review.html"><a href="prob-review.html#discrete-random-variables"><i class="fa fa-check"></i><b>B.2.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="B.2.2" data-path="prob-review.html"><a href="prob-review.html#continuous-random-variables"><i class="fa fa-check"></i><b>B.2.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="B.2.3" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-random-variables"><i class="fa fa-check"></i><b>B.2.3</b> Useful facts for transformations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="prob-review.html"><a href="prob-review.html#multivariate-distributions"><i class="fa fa-check"></i><b>B.3</b> Multivariate distributions</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="prob-review.html"><a href="prob-review.html#basic-properties"><i class="fa fa-check"></i><b>B.3.1</b> Basic properties</a></li>
<li class="chapter" data-level="B.3.2" data-path="prob-review.html"><a href="prob-review.html#marginal-distributions"><i class="fa fa-check"></i><b>B.3.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="B.3.3" data-path="prob-review.html"><a href="prob-review.html#independence-of-random-variables"><i class="fa fa-check"></i><b>B.3.3</b> Independence of random variables</a></li>
<li class="chapter" data-level="B.3.4" data-path="prob-review.html"><a href="prob-review.html#conditional-distributions"><i class="fa fa-check"></i><b>B.3.4</b> Conditional distributions</a></li>
<li class="chapter" data-level="B.3.5" data-path="prob-review.html"><a href="prob-review.html#covariance"><i class="fa fa-check"></i><b>B.3.5</b> Covariance</a></li>
<li class="chapter" data-level="B.3.6" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>B.3.6</b> Useful facts for transformations of multiple random variables</a></li>
<li class="chapter" data-level="B.3.7" data-path="prob-review.html"><a href="prob-review.html#example-binomial"><i class="fa fa-check"></i><b>B.3.7</b> Example (Binomial)</a></li>
<li class="chapter" data-level="B.3.8" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example"><i class="fa fa-check"></i><b>B.3.8</b> Example (Continuous bivariate distribution)</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="prob-review.html"><a href="prob-review.html#random-vectors"><i class="fa fa-check"></i><b>B.4</b> Random vectors</a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="prob-review.html"><a href="prob-review.html#definition"><i class="fa fa-check"></i><b>B.4.1</b> Definition</a></li>
<li class="chapter" data-level="B.4.2" data-path="prob-review.html"><a href="prob-review.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>B.4.2</b> Mean, variance, and covariance</a></li>
<li class="chapter" data-level="B.4.3" data-path="prob-review.html"><a href="prob-review.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Properties of transformations of random vectors</a></li>
<li class="chapter" data-level="B.4.4" data-path="prob-review.html"><a href="prob-review.html#example-continuous-bivariate-distribution-continued"><i class="fa fa-check"></i><b>B.4.4</b> Example (Continuous bivariate distribution continued)</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="prob-review.html"><a href="prob-review.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>B.5</b> Multivariate normal (Gaussian) distribution</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="prob-review.html"><a href="prob-review.html#definition-1"><i class="fa fa-check"></i><b>B.5.1</b> Definition</a></li>
<li class="chapter" data-level="B.5.2" data-path="prob-review.html"><a href="prob-review.html#linear-functions-of-a-multivariate-normal-random-vector"><i class="fa fa-check"></i><b>B.5.2</b> Linear functions of a multivariate normal random vector</a></li>
<li class="chapter" data-level="B.5.3" data-path="prob-review.html"><a href="prob-review.html#example-ols-matrix-form"><i class="fa fa-check"></i><b>B.5.3</b> Example (OLS matrix form)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="est-infer-review.html"><a href="est-infer-review.html"><i class="fa fa-check"></i><b>C</b> Review of Estimation and Inference</a>
<ul>
<li class="chapter" data-level="C.1" data-path="est-infer-review.html"><a href="est-infer-review.html#estimation"><i class="fa fa-check"></i><b>C.1</b> Estimation</a></li>
<li class="chapter" data-level="C.2" data-path="est-infer-review.html"><a href="est-infer-review.html#hypothesis-testing"><i class="fa fa-check"></i><b>C.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="C.3" data-path="est-infer-review.html"><a href="est-infer-review.html#confidence-intervals"><i class="fa fa-check"></i><b>C.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="C.4" data-path="est-infer-review.html"><a href="est-infer-review.html#linking-hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>C.4</b> Linking Hypothesis Tests and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Progressive Introduction to Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-estimation" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Linear model estimation<a href="linear-model-estimation.html#linear-model-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="a-simple-motivating-example" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> A simple motivating example<a href="linear-model-estimation.html#a-simple-motivating-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we observe data related to the heights of 5 mothers and their adult daughters. The observed heights (measured in inches) are provided in Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>. Figure <a href="linear-model-estimation.html#fig:mdheights-plot">3.1</a> displays a scatter plot of the height data provided in Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>. Would it be reasonable to use a mother’s height to predict the height of her adult daughter?</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mdheights">Table 3.1: </span>Heights of mothers and their adult daughters (in).
</caption>
<thead>
<tr>
<th style="text-align:right;">
observation
</th>
<th style="text-align:right;">
mother’s height (in)
</th>
<th style="text-align:right;">
daughter’s height (in)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
57.5
</td>
<td style="text-align:right;">
61.5
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
60.5
</td>
<td style="text-align:right;">
63.5
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
63.5
</td>
<td style="text-align:right;">
63.5
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
66.5
</td>
<td style="text-align:right;">
66.5
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
69.5
</td>
<td style="text-align:right;">
66.5
</td>
</tr>
</tbody>
</table>
<div class="figure"><span style="display:block;" id="fig:mdheights-plot"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/mdheights-plot-1.png" alt="A scatter plot displaying pairs of heights for a mother and her adult daughter." width="672" />
<p class="caption">
Figure 3.1: A scatter plot displaying pairs of heights for a mother and her adult daughter.
</p>
</div>
<p>A <strong>regression analysis</strong> is the process of building a model describing the typical relationship between a set of observed variables. A regression analysis builds the model using observed values of the variables for <span class="math inline">\(n\)</span> subjects sampled from a population. In the present context, we want to model the height of adult daughters using the height of their mothers. The model we build is known as a <strong>regression model</strong>.</p>
<p>The variables in a regression analysis may be divided into two types: the response variable and the predictor variables.</p>
<p>The outcome variable we are trying to predict is known as the <strong>response variable</strong>. Response variables are also known as <strong>outcome</strong>, <strong>output</strong>, or <strong>dependent</strong> variables. The response variable is denoted by <span class="math inline">\(Y\)</span>. The observed value of <span class="math inline">\(Y\)</span> for observation <span class="math inline">\(i\)</span> is denoted <span class="math inline">\(Y_i\)</span>.</p>
<p>The variables available to model the response variable are known as <strong>predictors variables</strong>. Predictor variables are also known as <strong>explanatory</strong>, <strong>regressor</strong>, <strong>input</strong>, <strong>independent</strong> variables, or simply as <strong>features</strong>. Following the convention of <span class="citation">Weisberg (<a href="#ref-alr4">2014</a>)</span>, we use the term <strong>regressor</strong> to refer to the variables used in our regression model, whether that is the original predictor variable, some transformation of a predictor, some combination of predictors, etc. Thus, every predictor can be a regressor but not all regressors are a predictor. The regressor variables are denoted as <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span>. The value of <span class="math inline">\(X_j\)</span> for observation <span class="math inline">\(i\)</span> is denoted by <span class="math inline">\(x_{i,j}\)</span>. If there is only a single regressor in the model, we can denote the single regressor as <span class="math inline">\(X\)</span> and the observed values of <span class="math inline">\(X\)</span> as <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p>
<p>For the height data, the 5 pairs of observed data are denoted
<span class="math display">\[(x_1, Y_1), (x_2, Y_2), \ldots, (x_5, Y_5),\]</span>
with <span class="math inline">\((x_i, Y_i)\)</span> denoting the data for observation <span class="math inline">\(i\)</span>. <span class="math inline">\(x_i\)</span> denotes the mother’s height for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> denotes the daughter’s height for
observation <span class="math inline">\(i\)</span>. Referring to Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>, we see that, e.g., <span class="math inline">\(x_3 = 63.5\)</span> and <span class="math inline">\(Y_5= 66.5\)</span>.</p>
<p>Suppose we want to find the straight line that best fits the plot of mother and daughter heights in Figure <a href="linear-model-estimation.html#fig:mdheights-plot">3.1</a>. How do we determine the “best fitting” model? Consider Figure <a href="linear-model-estimation.html#fig:two-fitted-lines">3.2</a>, in which 2 potential “best fitting” lines are drawn on the scatter plot of the height data. Which one is best?</p>
<p>The rest of this chapter focuses on defining and estimating the parameters of a <em>linear</em> regression model. We will start with the simplest type of linear regression, called simple linear regression, which only uses a single regressor variable to model the response. We will then start looking at more complicated linear regression models. After that, we discuss how to evaluate how well an estimated regression model fits the data. We conclude with a summary of some important concepts from the chapter.</p>
<div class="figure"><span style="display:block;" id="fig:two-fitted-lines"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/two-fitted-lines-1.png" alt="Comparison of two potential fitted models to some observed data. The fitted models are shown in grey." width="672" />
<p class="caption">
Figure 3.2: Comparison of two potential fitted models to some observed data. The fitted models are shown in grey.
</p>
</div>
</div>
<div id="s-slr-estimation" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Estimation of the simple linear regression model<a href="linear-model-estimation.html#s-slr-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Parameter estimation</strong> is the process of using observed data to estimate parameters of a model. There are many different methods of parameter estimation in statistics: method-of-moments, maximum likelihood, Bayesian, etc. The most common parameter estimation method for linear models is the <strong>least squares method</strong>, which is commonly called <strong>Ordinary Least Squares (OLS)</strong> estimation. OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly.</p>
<div id="ss:fv-resid-rss" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Model definition, fitted values, residuals, and RSS<a href="linear-model-estimation.html#ss:fv-resid-rss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The regression model for <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(E(Y \mid X)\)</span>, is the expected value of <span class="math inline">\(Y\)</span> conditional on the regressor <span class="math inline">\(X\)</span>. Thus, a regression model specifically refers to the expected relationship between the response and regressors.</p>
<p>The <strong>simple linear regression model</strong> for a response variable assumes the mean of <span class="math inline">\(Y\)</span> conditional on a single regressor <span class="math inline">\(X\)</span> is
<span class="math display">\[
E(Y\mid X) = \beta_0 + \beta_1 X.
\]</span></p>
<p>The response variable <span class="math inline">\(Y\)</span> is modeled as
<span class="math display" id="eq:slr-model-Y">\[
\begin{aligned}
Y &amp;= E(Y \mid X) + \epsilon \\
&amp;= \beta_0 + \beta_1 X + \epsilon,
\end{aligned}
\tag{3.1}
\]</span>
where <span class="math inline">\(\epsilon\)</span> is known as the model error.</p>
<p>The error term <span class="math inline">\(\epsilon\)</span> is literally the deviation of the response variable from its mean.
It is standard to assume that conditional on the regressor variable, the error term has mean 0 and variance <span class="math inline">\(\sigma^2\)</span>, which can be written as
<span class="math display" id="eq:error-mean">\[
E(\epsilon \mid X) = 0 \tag{3.2}
\]</span>
and
<span class="math display" id="eq:error-var">\[
\mathrm{var}(\epsilon \mid X) = \sigma^2.\tag{3.3}
\]</span>
<!-- These assumptions imply that  -->
<!-- \[ -->
<!-- E(Y \mid X) = \beta_0 + \beta_1X(\#eq:Y-mean), -->
<!-- \] which means that the model of the response in Equation \@ref(eq:slr-model-Y) can be written as -->
<!-- \[ -->
<!-- Y = E(Y \mid X) + \epsilon. -->
<!-- \] -->
<!-- How do we derive the result in Equation \@ref(eq:Y-mean)? -->
<!-- The assumptions in Equations \@ref(eq:error-mean) and \@ref(eq:error-var) imply that -->
<!-- \[ -->
<!-- \begin{align} -->
<!-- E(Y\mid X) &= E(\beta_0 + \beta_1 X + \epsilon \mid X) & \tiny\text{(subsitute definition of }Y\text{)}\\ -->
<!-- &= E(\beta_0 \mid X) + E(\beta_1 X\mid X) + E(\epsilon \mid X) & \tiny\text{(linearity of expectation)}\\ -->
<!-- &= \beta_0 + \beta_1 X + E(\epsilon \mid X) & \tiny\text{(the }\beta\text{s and }X\text{ are non-random)}\\ -->
<!-- &= \beta_0 + \beta_1 X + 0 & \tiny\text{(using the error assumption above)}\\ -->
<!-- &=\beta_0 + \beta_1 X. -->
<!-- \end{align} -->
<!-- \] --></p>
<!-- \[E(Y\mid X) = \beta_0 + \beta_1 X\] -->
<!-- is the expected value (mean) of $Y$ conditional on the regressor variable $X$ and $\epsilon$ is known as the *error*. -->
<p>Using the response values <span class="math inline">\(Y_1, \ldots, Y_n\)</span> and their associated regressor values <span class="math inline">\(x_1, \ldots, x_n\)</span>, the observed data are modeled as
<span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i \\
&amp;= E(Y\mid X = x_i) + \epsilon_i,
\end{aligned}
\]</span>
for <span class="math inline">\(i=1\)</span>, <span class="math inline">\(2\)</span>,<span class="math inline">\(\ldots\)</span>,<span class="math inline">\(n\)</span>, where <span class="math inline">\(\epsilon_i\)</span> denotes the error for observation <span class="math inline">\(i\)</span>.</p>
<p>The <strong>estimated regression model</strong> is defined as
<span class="math display">\[\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X,\]</span>
where <span class="math inline">\(\hat{\beta}_j\)</span> denotes the estimated value of <span class="math inline">\(\beta_j\)</span> for <span class="math inline">\(j=0,1\)</span>.</p>
<p>The <span class="math inline">\(i\)</span>th <strong>fitted value</strong> is defined as
<span class="math display" id="eq:def-fitted-value-slr">\[
\hat{Y}_i = \hat{E}(Y|X = x_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i. \tag{3.4}
\]</span>
Thus, the <span class="math inline">\(i\)</span>th fitted value is the estimated mean of <span class="math inline">\(Y\)</span> when the regressor <span class="math inline">\(X=x_i\)</span>. More specifically, the <span class="math inline">\(i\)</span>th fitted value is the estimated mean response based on the regressor value observed for the <span class="math inline">\(i\)</span>th observation.</p>
<p>The <span class="math inline">\(i\)</span>th <strong>residual</strong> is defined as
<span class="math display" id="eq:def-residual-slr">\[
\hat{\epsilon}_i = Y_i - \hat{Y}_i. \tag{3.5}
\]</span>
The <span class="math inline">\(i\)</span>th residual is the difference between the response and estimated
mean response of observation <span class="math inline">\(i\)</span>.</p>
<p>The <strong>residual sum of squares (RSS)</strong> of a regression model is the sum of its squared residuals. The RSS for a simple linear regression model, as a function of the estimated regression coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, is defined as
<span class="math display" id="eq:def-rss-slr">\[
RSS(\hat{\beta}_0, \hat{\beta}_1) = \sum_{i=1}^n \hat{\epsilon}_i^2. \tag{3.6}
\]</span></p>
<p>Using the various objects defined above, there are many equivalent expressions for the RSS. Notably, Equation <a href="linear-model-estimation.html#eq:def-rss-slr">(3.6)</a> can be rewritten using Equations <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(3.4)</a> and <a href="linear-model-estimation.html#eq:def-residual-slr">(3.5)</a> as
<span class="math display" id="eq:equiv-def-rss-slr">\[
\begin{aligned}
RSS(\hat{\beta}_0, \hat{\beta}_1) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 &amp; \\
&amp;= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
&amp;= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{aligned}
\tag{3.7}
\]</span></p>
<p>The <strong>fitted model</strong> is the estimated model that minimizes the RSS and is written as
<span class="math display" id="eq:def-fitted-model-slr">\[
\hat{Y}=\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X. \tag{3.8}
\]</span>
Both <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(\hat{E}(Y|X)\)</span> are used to denote a fitted model. <span class="math inline">\(\hat{Y}\)</span> is used for brevity while <span class="math inline">\(\hat{E}(Y|X)\)</span> is used for clarity. In a simple linear regression context, the fitted model is known as the <strong>line of best fit</strong>.</p>
<p>In Figure <a href="linear-model-estimation.html#fig:rss-viz2">3.3</a>, we visualize the response values, fitted values, residuals, and fitted model in a simple linear regression context. Note that:</p>
<ul>
<li>The fitted model is shown as the dashed grey line and minimizes the RSS.</li>
<li>The response values, shown as black dots, are the observed values of <span class="math inline">\(Y\)</span>.</li>
<li>The fitted values, shown as blue x’s, are the values returned by evaluating the fitted
model at the observed regressor values.</li>
<li>The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative.</li>
<li>The RSS is the sum of the squared vertical distances between the response and fitted values.</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:rss-viz2"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/rss-viz2-1.png" alt="Visualization of the fitted model, response values, fitted values, and residuals." width="672" />
<p class="caption">
Figure 3.3: Visualization of the fitted model, response values, fitted values, and residuals.
</p>
</div>
</div>
<div id="ols-estimators-of-the-simple-linear-regression-parameters" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> OLS estimators of the simple linear regression parameters<a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus under minimal assumptions. Specifically, the optimal analytical solutions for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are valid as long as the regressor values are not a constant value, i.e, <span class="math inline">\(x_i \neq x_j\)</span> for at least some <span class="math inline">\(i,j\in \{1,2,\ldots,n\}\)</span>.</p>
<p>Define <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i\)</span> and
<span class="math inline">\(\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i\)</span>. The expression <span class="math inline">\(\bar{x}\)</span> is read “x bar”, and it is the sample mean of the observed <span class="math inline">\(x_i\)</span> values. The OLS estimators of the simple linear regression coefficients that minimize the RSS are
<span class="math display" id="eq:slr-beta1hat">\[
\begin{aligned}
\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n x_i Y_i - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)\biggl(\sum_{i=1}^n Y_i\biggr)}{\sum_{i=1}^n x_i^2 - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)^2} \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})Y_i}{\sum_{i=1}^n (x_i - \bar{x})x_i}
\end{aligned}
\tag{3.9}
\]</span>
and
<span class="math display" id="eq:slr-beta0hat">\[
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}. \tag{3.10}
\]</span>
The various expressions given in Equation <a href="linear-model-estimation.html#eq:slr-beta1hat">(3.9)</a> are equivalent. In fact, in Equation <a href="linear-model-estimation.html#eq:slr-beta1hat">(3.9)</a>, all of the numerators are equivalent, and all of the denominators are equivalent. We provide derivations of the estimators for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> in Section <a href="linear-model-estimation.html#slr-derivation">3.12.2</a>.</p>
<p>In addition to the regression coefficients, the other parameter we mentioned in Section <a href="linear-model-estimation.html#ss:fv-resid-rss">3.2.1</a> is the error variance, <span class="math inline">\(\sigma^2\)</span>. The most common estimator of the error variance is
<span class="math display" id="eq:sigmasq-hat">\[
\hat{\sigma}^2 = \frac{RSS}{\mathrm{df}_{RSS}}. \tag{3.11}
\]</span>
where <span class="math inline">\(\mathrm{df}_{RSS}\)</span> is the <strong>degrees of freedom</strong> of the RSS. In a simple linear regression context, the denominator of Equation <a href="linear-model-estimation.html#eq:sigmasq-hat">(3.11)</a> is <span class="math inline">\(n-2\)</span>. See Section <a href="linear-model-estimation.html#degrees-of-freedom">3.12.1</a> for more comments about degrees of freedom.</p>
</div>
</div>
<div id="s:penguins-slr" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Penguins simple linear regression example<a href="linear-model-estimation.html#s:penguins-slr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use the <code>penguins</code> data set in the <strong>palmerpenguins</strong> package <span class="citation">(<a href="#ref-R-palmerpenguins">Horst, Hill, and Gorman 2022</a>)</span> to illustrate a very basic simple linear regression analysis.</p>
<p>The <code>penguins</code> data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by <span class="citation">Gorman, Williams, and Fraser (<a href="#ref-GormanEtAl2014">2014</a>)</span>. We start by loading the data into memory.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="linear-model-estimation.html#cb88-1" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span></code></pre></div>
<p>The data set includes 344 observations of
8 variables. The variables are:</p>
<ul>
<li><code>species</code>: a <code>factor</code> indicating the penguin species.</li>
<li><code>island</code>: a <code>factor</code> indicating the island the penguin was observed.</li>
<li><code>bill_length_mm</code>: a <code>numeric</code> variable indicating the bill length in millimeters.</li>
<li><code>bill_depth_mm</code>: a <code>numeric</code> variable indicating the bill depth in millimeters.</li>
<li><code>flipper_length_mm</code>: an <code>integer</code> variable indicating the flipper
length in millimeters</li>
<li><code>body_mass_g</code>: an <code>integer</code> variable indicating the body mass in grams.</li>
<li><code>sex</code>: a <code>factor</code> indicating the penguin sex (<code>female</code>, <code>male</code>).</li>
<li><code>year</code>: an integer denoting the study year the penguin was observed (<code>2007</code>, <code>2008</code>, or <code>2009</code>).</li>
</ul>
<p>We begin by creating a scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code> (y-axis versus x-axis) in Figure <a href="linear-model-estimation.html#fig:penguin-plot-2">3.4</a>.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="linear-model-estimation.html#cb89-1" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins,</span>
<span id="cb89-2"><a href="linear-model-estimation.html#cb89-2" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>,</span>
<span id="cb89-3"><a href="linear-model-estimation.html#cb89-3" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:penguin-plot-2"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/penguin-plot-2-1.png" alt="A scatter plot of penguin bill length (mm) versus body mass (g)" width="672" />
<p class="caption">
Figure 3.4: A scatter plot of penguin bill length (mm) versus body mass (g)
</p>
</div>
<p>We see a clear positive association between body mass and bill length: as the body mass increases, the bill length tends to increase. The pattern is linear, i.e., roughly a straight line.</p>
<p>We will build a simple linear regression model that regresses <code>bill_length_mm</code> on <code>body_mass_g</code>. More specifically, we want to estimate the parameters of the regression model <span class="math inline">\(E(Y\mid X)=\beta_0+\beta_1\,X\)</span>, with <span class="math inline">\(Y=\mathtt{bill\_length\_mm}\)</span> and <span class="math inline">\(X=\mathtt{body\_mass\_g}\)</span>, i.e., we want to estimate the parameters of the model
<span class="math display">\[
E(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g})=\beta_0+\beta_1\,\mathtt{body\_mass\_g}.
\]</span></p>
<p>The <code>lm</code> function uses OLS estimation to fit a linear model to data. The function has two main arguments:</p>
<ul>
<li><code>data</code>: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory.</li>
<li><code>formula</code>: a <span class="citation">Wilkinson and Rogers (<a href="#ref-wilkinsonrogers1973">1973</a>)</span> style formula describing the linear regression model. For complete details, run <code>?stats::formula</code> in the Console. If <code>y</code> is the response variable and <code>x</code> is an available numeric predictor, then <code>formula = y ~ x</code> tells <code>lm</code> to fit the simple linear regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1 X\)</span>.</li>
</ul>
<p>We use the code below to fit a linear model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> using the <code>penguins</code> data frame and assign the result the name <code>lmod</code>. <code>lmod</code> is an object of class <code>lm</code>.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="linear-model-estimation.html#cb90-1" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins) <span class="co"># fit model</span></span>
<span id="cb90-2"><a href="linear-model-estimation.html#cb90-2" tabindex="-1"></a><span class="fu">class</span>(lmod) <span class="co"># class of lmod</span></span>
<span id="cb90-3"><a href="linear-model-estimation.html#cb90-3" tabindex="-1"></a><span class="do">## [1] &quot;lm&quot;</span></span></code></pre></div>
<p>The <code>summary</code> function is commonly used to summarize the results of our fitted model. When an <code>lm</code> object is supplied to the <code>summary</code> function, it returns:</p>
<ul>
<li><code>Call</code>: the function call used to fit the model.</li>
<li><code>Residuals</code>: A 5-number summary of <span class="math inline">\(\hat{\epsilon}_1, \ldots, \hat{\epsilon}_n\)</span>.</li>
<li><code>Coefficients</code>: A table that lists:
<ul>
<li>The regressors in the fitted model.</li>
<li><code>Estimate</code>: the estimated coefficient for each regressor.</li>
<li><code>Std. Error</code>: the <em>estimated</em> standard error of the estimated coefficients.</li>
<li><code>t value</code>: the computed test statistic associated with testing <span class="math inline">\(H_0: \beta_j = 0\)</span> versus <span class="math inline">\(H_a: \beta_j \neq 0\)</span> for each regression coefficient in the model.</li>
<li><code>Pr(&gt;|t|)</code>: the associated p-value of each test.</li>
</ul></li>
<li>Various summary statistics:
<ul>
<li><code>Residual standard error</code> is the value of <span class="math inline">\(\hat{\sigma}\)</span>, the estimate of the error standard deviation. The degrees of freedom is <span class="math inline">\(\mathrm{df}_{RSS}\)</span>, the number of observations minus the number of estimated coefficients in the model.</li>
<li><code>Multiple R-squared</code> is a measure of model fit discussed in Section <a href="linear-model-estimation.html#evaluating-model-fit">3.10</a>.</li>
<li><code>Adjusted R-squared</code> is a modified version of <code>Multiple R-squared</code>.</li>
<li><code>F-statistic</code> is the test statistic for the test that compares the model with an only an intercept to the fitted model. The <code>DF</code> (degrees of freedom) values relate to the statistic under the null hypothesis, and the <code>p-value</code> is the p-value for the test.</li>
</ul></li>
</ul>
<p>We use the <code>summary</code> function on <code>lmod</code> to produce the output below.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="linear-model-estimation.html#cb91-1" tabindex="-1"></a><span class="co"># summarize results stored in lmod</span></span>
<span id="cb91-2"><a href="linear-model-estimation.html#cb91-2" tabindex="-1"></a><span class="fu">summary</span>(lmod)</span>
<span id="cb91-3"><a href="linear-model-estimation.html#cb91-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb91-4"><a href="linear-model-estimation.html#cb91-4" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb91-5"><a href="linear-model-estimation.html#cb91-5" tabindex="-1"></a><span class="do">## lm(formula = bill_length_mm ~ body_mass_g, data = penguins)</span></span>
<span id="cb91-6"><a href="linear-model-estimation.html#cb91-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb91-7"><a href="linear-model-estimation.html#cb91-7" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb91-8"><a href="linear-model-estimation.html#cb91-8" tabindex="-1"></a><span class="do">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb91-9"><a href="linear-model-estimation.html#cb91-9" tabindex="-1"></a><span class="do">## -10.1251  -3.0434  -0.8089   2.0711  16.1109 </span></span>
<span id="cb91-10"><a href="linear-model-estimation.html#cb91-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb91-11"><a href="linear-model-estimation.html#cb91-11" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb91-12"><a href="linear-model-estimation.html#cb91-12" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb91-13"><a href="linear-model-estimation.html#cb91-13" tabindex="-1"></a><span class="do">## (Intercept) 2.690e+01  1.269e+00   21.19   &lt;2e-16 ***</span></span>
<span id="cb91-14"><a href="linear-model-estimation.html#cb91-14" tabindex="-1"></a><span class="do">## body_mass_g 4.051e-03  2.967e-04   13.65   &lt;2e-16 ***</span></span>
<span id="cb91-15"><a href="linear-model-estimation.html#cb91-15" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb91-16"><a href="linear-model-estimation.html#cb91-16" tabindex="-1"></a><span class="do">## Signif. codes:  </span></span>
<span id="cb91-17"><a href="linear-model-estimation.html#cb91-17" tabindex="-1"></a><span class="do">## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb91-18"><a href="linear-model-estimation.html#cb91-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb91-19"><a href="linear-model-estimation.html#cb91-19" tabindex="-1"></a><span class="do">## Residual standard error: 4.394 on 340 degrees of freedom</span></span>
<span id="cb91-20"><a href="linear-model-estimation.html#cb91-20" tabindex="-1"></a><span class="do">##   (2 observations deleted due to missingness)</span></span>
<span id="cb91-21"><a href="linear-model-estimation.html#cb91-21" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.3542, Adjusted R-squared:  0.3523 </span></span>
<span id="cb91-22"><a href="linear-model-estimation.html#cb91-22" tabindex="-1"></a><span class="do">## F-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Using the output above, we see that the estimated parameters are <span class="math inline">\(\hat{\beta}_0=26.9\)</span> and <span class="math inline">\(\hat{\beta}_1=0.004\)</span>. Thus, our fitted model is
<span class="math display">\[
\widehat{\mathtt{bill\_length\_mm}}=26.9+0.004 \,\mathtt{body\_mass\_g}.
\]</span></p>
<p>In the context of a simple linear regression model, the intercept term is the expected response when the value of the regressor is zero, while the slope is the expected change in the response when the regressor increases by 1 unit. Thus, based on the model we fit to the <code>penguins</code> data, we can make the following interpretations:</p>
<ul>
<li><span class="math inline">\(\hat{\beta}_1\)</span>: If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguin’s bill length to be 0.004 millimeters longer.</li>
<li><span class="math inline">\(\hat{\beta}_0\)</span>: A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters.</li>
</ul>
<p>The latter interpretation is nonsensical. It doesn’t make sense to discuss a penguin with a body mass of 0 grams unless we are talking about an embryo, in which case it doesn’t even make sense to discuss bill length. This is caused by the fact that we are extrapolating far outside the observed body mass values. Our data only includes information for adult penguins, so we should be cautious about drawing conclusions for penguins at other life stages.</p>
<p>The <code>abline</code> function can be used to automatically overlay the fitted model on the observed data. We run the code below to produce Figure <a href="linear-model-estimation.html#fig:slr-penguin-fit">3.5</a>. The fit of the model to our observed data seems reasonable.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="linear-model-estimation.html#cb92-1" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins, <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>,</span>
<span id="cb92-2"><a href="linear-model-estimation.html#cb92-2" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>)</span>
<span id="cb92-3"><a href="linear-model-estimation.html#cb92-3" tabindex="-1"></a><span class="co"># draw fitted line of plot</span></span>
<span id="cb92-4"><a href="linear-model-estimation.html#cb92-4" tabindex="-1"></a><span class="fu">abline</span>(lmod)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:slr-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/slr-penguin-fit-1.png" alt="The fitted model overlaid on the penguin data." width="672" />
<p class="caption">
Figure 3.5: The fitted model overlaid on the penguin data.
</p>
</div>
<p>R provides many additional methods (generic functions that do something specific when applied to a certain type of object) for <code>lm</code> objects. Commonly used ones include:</p>
<ul>
<li><code>residuals</code>: extracts the residuals, <span class="math inline">\(\hat{\epsilon}_1, \ldots, \hat{\epsilon}_n\)</span> from an <code>lm</code> object.</li>
<li><code>fitted</code>: extracts the fitted values, <span class="math inline">\(\hat{Y}_1, \ldots, \hat{Y}_n\)</span> from an <code>lm</code> object.</li>
<li><code>predict</code>: by default, computes <span class="math inline">\(\hat{Y}_1, \ldots, \hat{Y}_n\)</span> for an <code>lm</code> object. It can also be used to make arbitrary predictions for the <code>lm</code> object.</li>
<li><code>coef</code> or <code>coefficients</code>: extracts the estimated coefficients from an <code>lm</code> object.</li>
<li><code>deviance</code>: extracts the RSS from an <code>lm</code> object.</li>
<li><code>df.residual</code>: extracts <span class="math inline">\(\mathrm{df}_{RSS}\)</span>, the degrees of freedom for the RSS, from an <code>lm</code> object.</li>
<li><code>sigma</code>: extracts <span class="math inline">\(\hat{\sigma}\)</span> from an <code>lm</code> object.</li>
</ul>
<p>We now use some of the methods to extract important characteristics of our fitted model.</p>
<p>We extract the estimated regression coefficients, <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, using the <code>coef</code> function.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="linear-model-estimation.html#cb93-1" tabindex="-1"></a>(coeffs <span class="ot">&lt;-</span> <span class="fu">coef</span>(lmod)) <span class="co"># extract, assign, and print coefficients</span></span>
<span id="cb93-2"><a href="linear-model-estimation.html#cb93-2" tabindex="-1"></a><span class="do">##  (Intercept)  body_mass_g </span></span>
<span id="cb93-3"><a href="linear-model-estimation.html#cb93-3" tabindex="-1"></a><span class="do">## 26.898872424  0.004051417</span></span></code></pre></div>
<p>We extract the vector of residuals, <span class="math inline">\(\hat{\epsilon}_1,\ldots, \hat{\epsilon}_n\)</span>, using the <code>residuals</code> function.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="linear-model-estimation.html#cb94-1" tabindex="-1"></a>ehat <span class="ot">&lt;-</span> <span class="fu">residuals</span>(lmod) <span class="co"># extract and assign residuals</span></span>
<span id="cb94-2"><a href="linear-model-estimation.html#cb94-2" tabindex="-1"></a><span class="fu">head</span>(ehat) <span class="co"># first few residuals</span></span>
<span id="cb94-3"><a href="linear-model-estimation.html#cb94-3" tabindex="-1"></a><span class="do">##          1          2          3          5          6 </span></span>
<span id="cb94-4"><a href="linear-model-estimation.html#cb94-4" tabindex="-1"></a><span class="do">## -2.9916846 -2.7942554  0.2340237 -4.1762596 -2.3865430 </span></span>
<span id="cb94-5"><a href="linear-model-estimation.html#cb94-5" tabindex="-1"></a><span class="do">##          7 </span></span>
<span id="cb94-6"><a href="linear-model-estimation.html#cb94-6" tabindex="-1"></a><span class="do">## -2.6852575</span></span></code></pre></div>
<p>We extract the vector of fitted values, <span class="math inline">\(\hat{Y}_1,\ldots, \hat{Y}_n\)</span>, using the <code>fitted</code> function.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="linear-model-estimation.html#cb95-1" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(lmod) <span class="co"># extract and assign fitted values</span></span>
<span id="cb95-2"><a href="linear-model-estimation.html#cb95-2" tabindex="-1"></a><span class="fu">head</span>(yhat) <span class="co"># first few fitted values</span></span>
<span id="cb95-3"><a href="linear-model-estimation.html#cb95-3" tabindex="-1"></a><span class="do">##        1        2        3        5        6        7 </span></span>
<span id="cb95-4"><a href="linear-model-estimation.html#cb95-4" tabindex="-1"></a><span class="do">## 42.09168 42.29426 40.06598 40.87626 41.68654 41.58526</span></span></code></pre></div>
<p>We can also extract the vector of fitted values using the <code>predict</code> function.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="linear-model-estimation.html#cb96-1" tabindex="-1"></a>yhat2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(lmod) <span class="co"># compute and assign fitted values</span></span>
<span id="cb96-2"><a href="linear-model-estimation.html#cb96-2" tabindex="-1"></a><span class="fu">head</span>(yhat2) <span class="co"># first few fitted values</span></span>
<span id="cb96-3"><a href="linear-model-estimation.html#cb96-3" tabindex="-1"></a><span class="do">##        1        2        3        5        6        7 </span></span>
<span id="cb96-4"><a href="linear-model-estimation.html#cb96-4" tabindex="-1"></a><span class="do">## 42.09168 42.29426 40.06598 40.87626 41.68654 41.58526</span></span></code></pre></div>
<p>We extract the RSS of the model using the <code>deviance</code> function.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="linear-model-estimation.html#cb97-1" tabindex="-1"></a>(rss <span class="ot">&lt;-</span> <span class="fu">deviance</span>(lmod)) <span class="co"># extract, assign, and print rss</span></span>
<span id="cb97-2"><a href="linear-model-estimation.html#cb97-2" tabindex="-1"></a><span class="do">## [1] 6564.494</span></span></code></pre></div>
<p>We extract the residual degrees of freedom using the <code>df.residual</code> function.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="linear-model-estimation.html#cb98-1" tabindex="-1"></a>(dfr <span class="ot">&lt;-</span> <span class="fu">df.residual</span>(lmod)) <span class="co"># extract n - p</span></span>
<span id="cb98-2"><a href="linear-model-estimation.html#cb98-2" tabindex="-1"></a><span class="do">## [1] 340</span></span></code></pre></div>
<p>We extract the estimated error standard deviation, <span class="math inline">\(\hat{\sigma}=\sqrt{\hat{\sigma}^2}\)</span>, using the <code>sigma</code> function. In the code below, we square <span class="math inline">\(\hat{\sigma}\)</span> to estimate the error variance, <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="linear-model-estimation.html#cb99-1" tabindex="-1"></a>(sigmasqhat <span class="ot">&lt;-</span> <span class="fu">sigma</span>(lmod)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># estimated error variance</span></span>
<span id="cb99-2"><a href="linear-model-estimation.html#cb99-2" tabindex="-1"></a><span class="do">## [1] 19.30734</span></span></code></pre></div>
<p>From the output above, we that the the first 3 residuals are -2.99, -2.79, and 0.23. The first 3 fitted values are 42.09, 42.29, and 40.07. The RSS for the fitted model is 6564.49 with 340 degrees of freedom. The estimated error variance, <span class="math inline">\(\hat{\sigma}^2\)</span>, is 19.31.</p>
<p>We use the <code>methods</code> function to obtain a full list of methods available for <code>lm</code> objects using the code below.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="linear-model-estimation.html#cb100-1" tabindex="-1"></a><span class="fu">methods</span>(<span class="at">class =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb100-2"><a href="linear-model-estimation.html#cb100-2" tabindex="-1"></a><span class="do">##  [1] add1           alias          anova         </span></span>
<span id="cb100-3"><a href="linear-model-estimation.html#cb100-3" tabindex="-1"></a><span class="do">##  [4] case.names     coerce         confint       </span></span>
<span id="cb100-4"><a href="linear-model-estimation.html#cb100-4" tabindex="-1"></a><span class="do">##  [7] cooks.distance deviance       dfbeta        </span></span>
<span id="cb100-5"><a href="linear-model-estimation.html#cb100-5" tabindex="-1"></a><span class="do">## [10] dfbetas        drop1          dummy.coef    </span></span>
<span id="cb100-6"><a href="linear-model-estimation.html#cb100-6" tabindex="-1"></a><span class="do">## [13] effects        extractAIC     family        </span></span>
<span id="cb100-7"><a href="linear-model-estimation.html#cb100-7" tabindex="-1"></a><span class="do">## [16] formula        fortify        hatvalues     </span></span>
<span id="cb100-8"><a href="linear-model-estimation.html#cb100-8" tabindex="-1"></a><span class="do">## [19] influence      initialize     kappa         </span></span>
<span id="cb100-9"><a href="linear-model-estimation.html#cb100-9" tabindex="-1"></a><span class="do">## [22] labels         logLik         model.frame   </span></span>
<span id="cb100-10"><a href="linear-model-estimation.html#cb100-10" tabindex="-1"></a><span class="do">## [25] model.matrix   nobs           plot          </span></span>
<span id="cb100-11"><a href="linear-model-estimation.html#cb100-11" tabindex="-1"></a><span class="do">## [28] predict        print          proj          </span></span>
<span id="cb100-12"><a href="linear-model-estimation.html#cb100-12" tabindex="-1"></a><span class="do">## [31] qr             residuals      rstandard     </span></span>
<span id="cb100-13"><a href="linear-model-estimation.html#cb100-13" tabindex="-1"></a><span class="do">## [34] rstudent       show           simulate      </span></span>
<span id="cb100-14"><a href="linear-model-estimation.html#cb100-14" tabindex="-1"></a><span class="do">## [37] slotsFromS3    summary        variable.names</span></span>
<span id="cb100-15"><a href="linear-model-estimation.html#cb100-15" tabindex="-1"></a><span class="do">## [40] vcov          </span></span>
<span id="cb100-16"><a href="linear-model-estimation.html#cb100-16" tabindex="-1"></a><span class="do">## see &#39;?methods&#39; for accessing help and source code</span></span></code></pre></div>
</div>
<div id="defining-a-linear-model" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Defining a linear model<a href="linear-model-estimation.html#defining-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ss-necessary-components" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Necessary components and notation<a href="linear-model-estimation.html#ss-necessary-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now wish to discuss linear models in a broader context. We begin by defining notation for the components of a linear model and provide some of their important properties. We repeat some of the previous discussion for clarity.</p>
<ul>
<li><span class="math inline">\(Y\)</span> denotes the response variable.
<ul>
<li>The response variable is treated as a random variable.</li>
<li>We will observe realizations of this random variable for each observation in our data set.</li>
</ul></li>
<li><span class="math inline">\(X\)</span> denotes a single regressor variable. <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span> denote distinct regressor variables if we are performing regression with multiple regressor variables.
<ul>
<li>The regressor variables are treated as non-random variables.</li>
<li>The observed values of the regressor variables are treated as fixed, known values.</li>
</ul></li>
<li><span class="math inline">\(\mathbb{X}=\{X_0, X_1,\ldots,X_{p-1}\}\)</span> denotes the collection of all regressors under consideration, though this notation is really only needed in the context of multiple regression. <span class="math inline">\(X_0\)</span> is usually the constant regressor 1, which is needed to include an intercept in the regression model.</li>
<li><span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_{p-1}\)</span> denote <strong>regression coefficients</strong>.
<ul>
<li>Regression coefficients are statistical parameters that we will estimate from our data.</li>
<li>The regression coefficients are treated as fixed, non-random but unknown values.</li>
<li>Regression coefficients are not observable.</li>
</ul></li>
<li><span class="math inline">\(\epsilon\)</span> denotes model <strong>error</strong>.
<ul>
<li>The model error is more accurately described as random variation of each observation from the regression model.</li>
<li>The error is treated as a random variable.</li>
<li>The error is assumed to have mean 0 for all values of the regressors. We write this as <span class="math inline">\(E(\epsilon \mid \mathbb{X}) = 0\)</span>, which is read as, “The expected value of <span class="math inline">\(\epsilon\)</span> conditional on knowing all the regressor values equals 0”. The notation “<span class="math inline">\(\mid \mathbb{X}\)</span>” extends the notation used in Equation <a href="interp-chapter.html#eq:slr-equation">(4.3)</a> to multiple regressors.</li>
<li>The variance of the errors is assumed to be a constant value for all values of the regressors. We write this assumption as <span class="math inline">\(\mathrm{var}(\epsilon \mid \mathbb{X})=\sigma^2\)</span>.</li>
<li>The error is never observable (except in the context of a simulation study where the experimenter literally defines the true model).</li>
</ul></li>
</ul>
</div>
<div id="standard-definition-of-linear-model" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Standard definition of linear model<a href="linear-model-estimation.html#standard-definition-of-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In general, a linear regression model can have an arbitrary number of regressors. A <strong>multiple linear regression</strong> model has two or more regressors.</p>
<p>A <strong>linear model</strong> for <span class="math inline">\(Y\)</span> is defined by the equation
<span class="math display" id="eq:lmdef">\[
\begin{aligned}
Y &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1}
X_{p-1} + \epsilon \\
&amp;= E(Y \mid \mathbb{X}) + \epsilon.
\end{aligned}
\tag{3.12}
\]</span></p>
<p>We write the linear model in this way to emphasize the fact <em>the response value equals the expected response for that combination of regressor values plus some error</em>. It should be clear from comparing Equation <a href="linear-model-estimation.html#eq:lmdef">(3.12)</a> with the previous line that
<span class="math display">\[
E(Y \mid \mathbb{X}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1},
\]</span>
which we prove in Chapter <a href="linear-model-theory.html#linear-model-theory">5</a>.</p>
<p>More generally, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values created from our regressor variables, i.e.,
<span class="math display" id="eq:lmdef-cj">\[
E(Y \mid \mathbb{X}) = \sum_{j=0}^{p-1} c_j \beta_j, \tag{3.13}
\]</span>
where <span class="math inline">\(c_0, c_1, \ldots, c_{p-1}\)</span> are known functions of
the regressor variables, e.g., <span class="math inline">\(c_1 = X_1 X_2 X_3\)</span>, <span class="math inline">\(c_3 = X_2^2\)</span>,
<span class="math inline">\(c_8 = \ln(X_1)/X_2^2\)</span>, etc. Thus, if <span class="math inline">\(g_0,\ldots,g_{p-1}\)</span> are functions of <span class="math inline">\(\mathbb{X}\)</span>, then we can say that the regression model is linear if
it can be written as
<span class="math display">\[
E(Y\mid \mathbb{X}) = \sum_{j=0}^{p-1} g_j(\mathbb{X})\beta_j.
\]</span></p>
<p>A model is linear because of its <em>form</em>, not the shape it produces. Many of the linear model examples given below do not result in a straight line or surface, but are curved. Some examples of linear regression models are:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + \beta_1 X + \beta_2 X^2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2^{-1}\)</span>.</li>
<li><span class="math inline">\(E(\ln(Y)|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y^{-1}|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
</ul>
<p>Some examples of non-linear regression models are:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0 + e^{\beta_1 X}\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + \beta_1 X/(\beta_2 + X)\)</span>.</li>
</ul>
<p>The latter regression models are non-linear models because there is no way to express them using the expression in Equation <a href="linear-model-estimation.html#eq:lmdef-cj">(3.13)</a>.</p>
</div>
</div>
<div id="estimation-of-the-multiple-linear-regression-model" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Estimation of the multiple linear regression model<a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we want to estimate the parameters of a linear model with 1 or more regressors, i.e., when we use the model
<span class="math display">\[
Y=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1} + \epsilon.
\]</span></p>
<p>The system of equations relating the responses, the regressors, and the errors for all <span class="math inline">\(n\)</span> observations can be written as
<span class="math display" id="eq:lmSystem">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
\tag{3.14}
\]</span></p>
<div id="using-matrix-notation-to-represent-a-linear-model" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Using matrix notation to represent a linear model<a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can simplify the linear model described in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.14)</a> using matrix notation. It may be useful to refer to Appendix <a href="overview-of-matrix-facts.html#overview-of-matrix-facts">A</a> for a brief overview of matrix-related information.</p>
<p>We use the following notation:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]\)</span> denotes the column vector containing the <span class="math inline">\(n\)</span> observed response values.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> denotes the matrix containing a column of 1s and the observed regressor values for <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span>. This may be written as
<span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p-1} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p-1}
\end{bmatrix}.\]</span></li>
<li><span class="math inline">\(\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]\)</span> denotes the column vector containing the <span class="math inline">\(p\)</span> regression coefficients.</li>
<li><span class="math inline">\(\boldsymbol{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]\)</span> denotes the column vector contained the <span class="math inline">\(n\)</span> errors.</li>
</ul>
<p>The system of equations defining the linear model in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.14)</a> can be written as
<span class="math display">\[
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]</span>
Thus, matrix notation can be used to represent a system of linear equations. A model that cannot be represented as a system of linear equations using matrices is not a linear model.</p>
</div>
<div id="ss:fv-resid-rss-mlr" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Residuals, fitted values, and RSS for multiple linear regression<a href="linear-model-estimation.html#ss:fv-resid-rss-mlr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now discuss of residuals, fitted values, and RSS for the multiple linear regression context using matrix notation.</p>
<p>The vector of estimated values for the coefficients contained in <span class="math inline">\(\boldsymbol{\beta}\)</span> is denoted
<span class="math display" id="eq:def-beta-matrix">\[
\hat{\boldsymbol{\beta}}=[\hat{\beta}_0,\hat{\beta}_1,\ldots,\hat{\beta}_{p-1}]. \tag{3.15}
\]</span></p>
<p>The vector of regressor values for the <span class="math inline">\(i\)</span>th observation is denoted
<span class="math display" id="eq:def-ith-regressor-matrix">\[
\mathbf{x}_i=[1,x_{i,1},\ldots,x_{i,p-1}], \tag{3.16}
\]</span>
where the 1 is needed to account for the intercept in our model.</p>
<p>Extending the original definition of a fitted value in Equation <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(3.4)</a>, the <span class="math inline">\(i\)</span>th <strong>fitted value</strong> in the context of multiple linear regression is defined as
<span class="math display" id="eq:def-fitted-value-matrix">\[
\begin{aligned}
\hat{Y}_i &amp;= \hat{E}(Y \mid \mathbb{X} = \mathbf{x}_i) \\
&amp;= \hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \cdots + \hat{\beta}_{p-1} x_{i,p-1} \\
&amp;= \mathbf{x}_i^T\hat{\boldsymbol{\beta}}.
\end{aligned}
\tag{3.17}
\]</span>
The notation “<span class="math inline">\(\mathbb{X} = \mathbf{x}_i\)</span>” is a concise way of saying “<span class="math inline">\(X_0 = 1, X_1=x_{i,1}, \ldots, X_{p-1}=x_{i,p-1}\)</span>”.</p>
<p>The column vector of fitted values is defined as
<span class="math display" id="eq:def-fitted-values-matrix">\[
\hat{\mathbf{y}} = [\hat{Y}_1,\ldots,\hat{Y}_n], \tag{3.18}
\]</span>
and can be computed as
<span class="math display" id="eq:compute-yhat">\[
\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}. \tag{3.19}
\]</span></p>
<p>Extending the original definition of a residual in Equation <a href="linear-model-estimation.html#eq:def-residual-slr">(3.5)</a>,
the <span class="math inline">\(i\)</span>th <strong>residual</strong> in the context of multiple linear regression can be written as
<span class="math display">\[
\begin{aligned}
\hat{\epsilon}_i = Y_i - \hat{Y}_i=Y_i-\mathbf{x}_i^T\hat{\boldsymbol{\beta}},
\end{aligned}
\]</span>
using Equation <a href="linear-model-estimation.html#eq:def-fitted-value-matrix">(3.17)</a>.</p>
<p>The column vector of residuals is defined as
<span class="math display" id="eq:def-residuals-matrix">\[
\hat{\boldsymbol{\epsilon}} = [\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n]. \tag{3.20}
\]</span>
Using Equations <a href="linear-model-estimation.html#eq:def-fitted-values-matrix">(3.18)</a> and <a href="linear-model-estimation.html#eq:compute-yhat">(3.19)</a>, equivalent expressions for the residual vector are
<span class="math display" id="eq:epsilonhat-expressions">\[
\hat{\boldsymbol{\epsilon}}=\mathbf{y}-\hat{\mathbf{y}}=\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}.\tag{3.21}
\]</span></p>
<p>The RSS for a multiple linear regression model, as a function of the estimated regression coefficients, is
<span class="math display" id="eq:def-rss-matrix">\[
\begin{aligned}
RSS(\hat{\boldsymbol{\beta}}) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \hat{\boldsymbol{\epsilon}}^T \hat{\boldsymbol{\epsilon}} \\
&amp;= (\mathbf{y} - \hat{\mathbf{y}})^T (\mathbf{y} - \hat{\mathbf{y}}) \\
&amp; = (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}).
\end{aligned} \tag{3.22}
\]</span>
The various expressions in Equation <a href="linear-model-estimation.html#eq:def-rss-matrix">(3.22)</a> are equivalent (cf. Equation <a href="linear-model-estimation.html#eq:epsilonhat-expressions">(3.21)</a>).</p>
</div>
<div id="ols-estimator-of-the-regression-coefficients" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> OLS estimator of the regression coefficients<a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The OLS estimator of the regression coefficient vector, <span class="math inline">\(\boldsymbol{\beta}\)</span>, is
<span class="math display" id="eq:betahat">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}. \tag{3.23}
\]</span>
Equation <a href="linear-model-estimation.html#eq:betahat">(3.23)</a> assumes that <span class="math inline">\(\mathbf{X}\)</span> has full-rank (<span class="math inline">\(n&gt;p\)</span> and none of the columns of <span class="math inline">\(\mathbf{X}\)</span> are linear combinations of other columns in <span class="math inline">\(\mathbf{X}\)</span>), which is a very mild assumption. We provide a derivation of the estimator for <span class="math inline">\(\beta\)</span> in Section <a href="linear-model-estimation.html#mlr-derivation">3.12.5</a>.</p>
<p>The general estimator of the <span class="math inline">\(\sigma^2\)</span> in the context of multiple linear regression is
<span class="math display">\[
\hat{\sigma}^2 = \frac{RSS}{n-p},
\]</span>
which is consistent with the previous definition given in Equation <a href="linear-model-estimation.html#eq:sigmasq-hat">(3.11)</a>.</p>
</div>
</div>
<div id="s:penguins-mlr" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Penguins multiple linear regression example<a href="linear-model-estimation.html#s:penguins-mlr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We continue our analysis of the <code>penguins</code> data introduced in Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>. We will fit a multiple linear regression model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>, and will once again do so using the <code>lm</code> function.</p>
<p>Before we do that, we provide some additional discussion of the of the <code>formula</code> argument of the <code>lm</code> function. This will be very important as we discuss more complicated models. Assume <code>y</code> is the response variable and <code>x</code>, <code>x1</code>, <code>x2</code>, <code>x3</code> are available numeric predictors. Then:</p>
<ul>
<li><code>y ~ x</code> describes the simple linear regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1 X\)</span>.</li>
<li><code>y ~ x1 + x2</code> describes the multiple linear regression model <span class="math inline">\(E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><code>y ~ x1 + x2 + x1:x2</code> and <code>y ~ x1 * x2</code> describe the multiple linear regression model
<span class="math inline">\(E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</li>
<li><code>y ~ -1 + x1 + x2</code> describe a multiple linear regression model without an intercept, in this case,
<span class="math inline">\(E(Y|X_1, X_2)=\beta_1 X_1 + \beta_2 X_2\)</span>. The <code>-1</code> tells R not to include an intercept in the fitted model.</li>
<li><code>y ~ x + I(x^2)</code> describe the multiple linear regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1 X + \beta_2 X^2\)</span>. The <code>I()</code> function is a special function that tells R to create a regressor based on the syntax inside the <code>()</code> and include that regressor in the model.</li>
</ul>
<p>In the code below, we fit the linear model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code> and then use the <code>coef</code> and <code>deviance</code> functions to extract the estimated coefficients and RSS of the fitted model, respectively.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="linear-model-estimation.html#cb101-1" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb101-2"><a href="linear-model-estimation.html#cb101-2" tabindex="-1"></a>mlmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> flipper_length_mm, <span class="at">data =</span> penguins)</span>
<span id="cb101-3"><a href="linear-model-estimation.html#cb101-3" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb101-4"><a href="linear-model-estimation.html#cb101-4" tabindex="-1"></a><span class="fu">coef</span>(mlmod)</span>
<span id="cb101-5"><a href="linear-model-estimation.html#cb101-5" tabindex="-1"></a><span class="do">##       (Intercept)       body_mass_g flipper_length_mm </span></span>
<span id="cb101-6"><a href="linear-model-estimation.html#cb101-6" tabindex="-1"></a><span class="do">##     -3.4366939266      0.0006622186      0.2218654584</span></span>
<span id="cb101-7"><a href="linear-model-estimation.html#cb101-7" tabindex="-1"></a><span class="co"># extract RSS</span></span>
<span id="cb101-8"><a href="linear-model-estimation.html#cb101-8" tabindex="-1"></a><span class="fu">deviance</span>(mlmod)</span>
<span id="cb101-9"><a href="linear-model-estimation.html#cb101-9" tabindex="-1"></a><span class="do">## [1] 5764.585</span></span></code></pre></div>
<p>The fitted model is
<span class="math display">\[
\widehat{\mathtt{bill\_length\_mm}}=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\]</span>
We discuss how to interpret the coefficients of a multiple linear regression model in Chapter <a href="interp-chapter.html#interp-chapter">4</a>.</p>
<p>The RSS for the fitted model is 5764.59, which is substantially less than the RSS of the fitted simple linear regression model in Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>.
<!-- It is trivial to add additional numeric regressors to our linear regression model using the `lm` function. But what if we have a categorical predictor? The next section discusses how to transform a categorical predictors into 1 or more numeric regressors that can be included in our linear model. --></p>
</div>
<div id="model-types" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Types of linear models<a href="linear-model-estimation.html#model-types" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We provide a brief overview of different types of linear regression models. Our discussion is not exhaustive, nor are the types exclusive (a model can sometimes be described using more than one of these labels). We have previously discussed some of the terms, but include them for completeness.</p>
<ul>
<li><strong>Simple</strong>: a model with an intercept and a single regressor.</li>
<li><strong>Multiple</strong>: a model with 2 or more regressors.</li>
<li><strong>Polynomial</strong>: a model with squared, cubic, quartic predictors, etc. E.g, <span class="math inline">\(E(Y\mid X) = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3\)</span> is a 4th-degree polynomial.</li>
<li><strong>First-order</strong>: a model in which each predictor is used to create no more than one regressor.</li>
<li><strong>Main effect</strong>: a model in which none of the regressors are functions of more than one predictor. A predictor can be used more than once, but each regressor is only a function of one predictor. E.g., if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are different predictors, then the regression model <span class="math inline">\(E(Y\mid X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2\)</span> would be a main effect model, but not a first-order model since <span class="math inline">\(X_1\)</span> was used to create two regressors.</li>
<li><strong>Interaction</strong>: a model in which some of the regressors are functions of more than 1 predictor. E.g., if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are different predictors, then the regression model <span class="math inline">\(E(Y\mid X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2\)</span> is a very simple interaction model since the third regressor is the product of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</li>
<li><strong>Analysis of variance (ANOVA)</strong>: a model for which all predictors used in the model are categorical.</li>
<li><strong>Analysis of covariance (ANCOVA)</strong>: a model that uses at least one numeric predictor and at least one categorical predictor.</li>
<li><strong>Generalized (GLM)</strong>: a “generalized” linear regression model in which the responses do not come from a normal distribution.</li>
</ul>
</div>
<div id="categorical-predictors" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Categorical predictors<a href="linear-model-estimation.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the variables. We discuss two basic uses of categorical predictors in linear regression models. We will briefly introduce the:</p>
<ul>
<li><strong>parallel lines regression model</strong>, which is a main effect regression model that has a single numeric regressor and a single categorical predictor. The model produces parallel lines for each level of the categorical variable.</li>
<li><strong>separate lines regression model</strong>, which adds an interaction term between the numeric regressor and categorical predictor of the parallel lines regression model. The model produces separate lines for each level of the categorical variable.</li>
</ul>
<!-- Chapter \@ref(more-on-categorical-predictors) provides more information about using categorical predictors in a regression model. -->
<div id="indicator-variables" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Indicator variables<a href="linear-model-estimation.html#indicator-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to compute <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using Equation <a href="linear-model-estimation.html#eq:betahat">(3.23)</a>, both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> must contain numeric values. How can we use a categorical predictor in our regression model when its values are not numeric? To do so, we must transform the categorical predictor into one or more <strong>indicator</strong> or <strong>dummy variables</strong>, which we explain in more detail below.</p>
<p>An <strong>indicator function</strong> is a function that takes the value 1 if a certain property is true and 0 otherwise. An <strong>indicator variable</strong> is the variable that results from applying an indicator function to each observation of a variable. Many notations exist for indicator functions. We use the notation,
<span class="math display">\[
I_S(x) =
\begin{cases}
1 &amp; \textrm{if}\;x \in S\\
0 &amp; \textrm{if}\;x \notin S
\end{cases},
\]</span>
which is shorthand for a function that returns 1 if <span class="math inline">\(x\)</span> is in the set <span class="math inline">\(S\)</span> and 0 otherwise. Some examples are:</p>
<ul>
<li><span class="math inline">\(I_{\{2,3\}}(2) = 1\)</span>.</li>
<li><span class="math inline">\(I_{\{2,3\}}(2.5) = 1\)</span>.</li>
<li><span class="math inline">\(I_{[2,3]}(2.5) = 1\)</span>, where <span class="math inline">\([2,3]\)</span> is the interval from 2 to 3 and not the set containing only the numbers 2 and 3.</li>
<li><span class="math inline">\(I_{\{\text{red},\text{green}\}}(\text{green}) = 1\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(C\)</span> denote a categorical predictor with levels <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span>. The <span class="math inline">\(C\)</span> stands for “categorical”, while the <span class="math inline">\(L\)</span> stands for “level”. Let <span class="math inline">\(c_i\)</span> denote the value of <span class="math inline">\(C\)</span> for observation <span class="math inline">\(i\)</span>.</p>
<p>Let <span class="math inline">\(D_j\)</span> denote the indicator (dummy) variable for factor level <span class="math inline">\(L_j\)</span> of <span class="math inline">\(C\)</span>. The value of <span class="math inline">\(D_j\)</span> for observation <span class="math inline">\(i\)</span> is denoted <span class="math inline">\(d_{i,j}\)</span>, with
<span class="math display">\[
d_{i,j} = I_{\{L_j\}}(c_i),
\]</span>
i.e., <span class="math inline">\(d_{i,j}\)</span> is 1 if <span class="math inline">\(c_i\)</span> has factor level <span class="math inline">\(L_j\)</span> and 0 otherwise.</p>
</div>
<div id="parallel-and-separate-lines-models" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Parallel and separate lines models<a href="linear-model-estimation.html#parallel-and-separate-lines-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume we want to build a linear regression model using a single numeric regressor <span class="math inline">\(X\)</span> and a two-level categorical predictor <span class="math inline">\(C\)</span>.</p>
<p>The standard simple linear regression model is
<span class="math display">\[E(Y\mid X)=\beta_0 + \beta_1 X.\]</span></p>
<p>To create a parallel lines regression model, we add regressor <span class="math inline">\(D_2\)</span> to the simple linear regression model. Thus, the parallel lines regression model is
<span class="math display" id="eq:parallel-lines-model">\[
E(Y\mid X,C)=\beta_{0}+\beta_1 X+\beta_2 D_2. \tag{3.24}
\]</span>
Since <span class="math inline">\(D_2=0\)</span> when <span class="math inline">\(C=L_1\)</span> and <span class="math inline">\(D_2=1\)</span> when <span class="math inline">\(C=L_2\)</span>, we see that the model in Equation <a href="linear-model-estimation.html#eq:parallel-lines-model">(3.24)</a> simplifies to
<span class="math display">\[
E(Y\mid X, C) =
\begin{cases}
  \beta_0+\beta_1 X &amp; \mathrm{if}\;C = L_1 \\
  (\beta_0 + \beta_2) +\beta_1 X &amp; \mathrm{if}\;C = L_2
\end{cases}.
\]</span>
The parallel lines will be separated vertically by the distance <span class="math inline">\(\beta_2\)</span>.</p>
<p>To create a separate lines regression model, we add regressor <span class="math inline">\(D_2\)</span> and the interaction regressor <span class="math inline">\(X D_2\)</span> to our simple linear regression model. Thus, the separate lines regression model is
<span class="math display">\[
E(Y\mid X,C)=\beta_0+\beta_1 X+\beta_2 D_2 + \beta_{3} XD_2,
\]</span>
which, similar to the previous model, simplifies to
<span class="math display">\[
E(Y\mid X, C) =
\begin{cases}
  \beta_{0}+\beta_1 X &amp; \mathrm{if}\;C = L_1 \\
  (\beta_{0} + \beta_{2}) +(\beta_1 + \beta_{3}) X &amp; \mathrm{if}\;C = L_2
\end{cases}.
\]</span></p>
</div>
<div id="extensions" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Extensions<a href="linear-model-estimation.html#extensions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have presented the most basic regression models that include a categorical predictor. If we have a categorical predictor <span class="math inline">\(C\)</span> with <span class="math inline">\(K\)</span> levels <span class="math inline">\(L_1, L_2, \ldots, L_K\)</span>, then we can add indicator variables <span class="math inline">\(D_2, D_3, \ldots, D_K\)</span> to a simple linear regression model to create a parallel lines model for each level of <span class="math inline">\(C\)</span>. Similarly, we can add regressors <span class="math inline">\(D_2, D_3, \ldots, D_K, X D_2, X D_3, \ldots, X D_K\)</span> to a simple linear regression model to create a separate lines model for each level of <span class="math inline">\(C\)</span>.</p>
<p>It is easy to imagine using multiple categorical predictors in a model, interacting one or more categorical predictors with one or more numeric regressors in model, etc. These models can be fit easily using R (as we’ll see below), though interpretation becomes more challenging.</p>
</div>
<div id="avoiding-an-easy-mistake" class="section level3 hasAnchor" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> Avoiding an easy mistake<a href="linear-model-estimation.html#avoiding-an-easy-mistake" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the setting where <span class="math inline">\(C\)</span> has only 2 levels. Why don’t we add <span class="math inline">\(D_1\)</span> to the parallel lines model that already has <span class="math inline">\(D_2\)</span>? Or <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_1 X\)</span> to the separate lines model that already has <span class="math inline">\(D_2\)</span> and <span class="math inline">\(D_2 X\)</span>? First, we notice that we don’t <em>need</em> to add them. E.g., if an observation doesn’t have level <span class="math inline">\(L_2\)</span> (<span class="math inline">\(D_2=0\)</span>), then it must have level <span class="math inline">\(L_1\)</span>. More importantly, we didn’t do this because it will create linear dependencies in the columns of the regressor matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Let <span class="math inline">\(\mathbf{d}_1=[d_{1,1}, d_{2,1}, \ldots, d_{n,1}]\)</span> be the column vector of observed values for indicator variable <span class="math inline">\(D_1\)</span> and <span class="math inline">\(\mathbf{d}_2\)</span> be the column vector for <span class="math inline">\(D_2\)</span>. Then for a two-level categorical variable, <span class="math inline">\(\mathbf{d}_1 + \mathbf{d}_2\)</span> is an <span class="math inline">\(n\times 1\)</span> vector of 1s, meaning that <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> will be linearly dependent with the intercept column of our <span class="math inline">\(\mathbf{X}\)</span> matrix. Thus, adding <span class="math inline">\(D_1\)</span> to the parallel lines model would result in <span class="math inline">\(\mathbf{X}\)</span> having linearly dependent columns, which creates estimation problems.</p>
<p>For a categorical predictor with <span class="math inline">\(K\)</span> levels, we only need indicator variables for <span class="math inline">\(K-1\)</span> levels of the categorical predictor. The level without an indicator variable in the regression model is known as the <strong>reference level</strong>, which is explained in Chapter <a href="interp-chapter.html#interp-chapter">4</a>. Technically, we can choose any level to be our reference level, but R automatically chooses the first level of a categorical (<code>factor</code>) variable to be the reference level, so we adopt that convention.</p>
</div>
</div>
<div id="s:penguins-mlr2" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Penguins example with categorical predictor<a href="linear-model-estimation.html#s:penguins-mlr2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We return once again to the <code>penguins</code> data previously introduced. We use the code below to produce Figure <a href="linear-model-estimation.html#fig:penguins-grouped-scatter">3.6</a>, which displays the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code> that distinguishes the <code>species</code> of each observation. It is very clear that the relationship between bill length and body mass changes depending on the species of penguin.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="linear-model-estimation.html#cb102-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2) <span class="co"># load package</span></span>
<span id="cb102-2"><a href="linear-model-estimation.html#cb102-2" tabindex="-1"></a><span class="co"># create grouped scatterplot</span></span>
<span id="cb102-3"><a href="linear-model-estimation.html#cb102-3" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span></span>
<span id="cb102-4"><a href="linear-model-estimation.html#cb102-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm, <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb102-5"><a href="linear-model-estimation.html#cb102-5" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:penguins-grouped-scatter"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/penguins-grouped-scatter-1.png" alt="A grouped scatter plot of body mass versus bill length that distinguishes penguin species." width="672" />
<p class="caption">
Figure 3.6: A grouped scatter plot of body mass versus bill length that distinguishes penguin species.
</p>
</div>
<p>How do we use a categorical variable in R’s <code>lm</code> function? Recall that we should represent our categorical variables as a <code>factor</code> in R. The <code>lm</code> function will automatically convert a <code>factor</code> variable to the correct number of indicator variables when we include the <code>factor</code> variable in our <code>formula</code> argument. R will automatically choose the reference level to be the first level of the <code>factor</code> variable. To add a main effect term for a categorical predictor, we simply add the term to our <code>lm</code> formula. To create an interaction term, we use <code>:</code> between the interacting variables. E.g., if <code>c</code> is a <code>factor</code> variable and <code>x</code> is a <code>numeric</code> variable, we can use the notation <code>c:x</code> in our <code>formula</code> to get all the interactions between <code>c</code> and <code>x</code>.</p>
<p>In our present context, the categorical predictor of interest is <code>species</code>, which has the levels <code>Adelie</code>, <code>Chinstrap</code>, and <code>Gentoo</code>. The <code>species</code> variable is already a <code>factor</code>. Since the variable has 3 levels, it will be transformed into 2 indicator variables by R. The first level of species is <code>Adelie</code>, so R will treat that level as the reference level, and automatically create indicator variables for the levels <code>Chinstrap</code> and <code>Gentoo</code>. (Reminder: to determine the level order of a <code>factor</code> variable <code>c</code>, run the commend <code>levels(c)</code>, or in this case <code>levels(penguins$species)</code>.)</p>
<p>Let <span class="math inline">\(D_C\)</span> denote the indicator variable for the <code>Chinstrap</code> level and <span class="math inline">\(D_G\)</span> denote the indicator variable for the <code>Gentoo</code> level. To fit the parallel lines regression model
<span class="math display">\[E(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) = \beta_{0} + \beta_1 \mathtt{body\_mass\_g} + \beta_2 D_C + \beta_3 D_G,\]</span>
we run the code below. The <code>coef</code> function is used to extract the estimated coefficients from our fitted model in <code>lmodp</code>.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="linear-model-estimation.html#cb103-1" tabindex="-1"></a><span class="co"># fit parallel lines model</span></span>
<span id="cb103-2"><a href="linear-model-estimation.html#cb103-2" tabindex="-1"></a>lmodp <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species, <span class="at">data =</span> penguins)</span>
<span id="cb103-3"><a href="linear-model-estimation.html#cb103-3" tabindex="-1"></a><span class="co"># extract coefficients</span></span>
<span id="cb103-4"><a href="linear-model-estimation.html#cb103-4" tabindex="-1"></a><span class="fu">coef</span>(lmodp)</span>
<span id="cb103-5"><a href="linear-model-estimation.html#cb103-5" tabindex="-1"></a><span class="do">##      (Intercept)      body_mass_g speciesChinstrap </span></span>
<span id="cb103-6"><a href="linear-model-estimation.html#cb103-6" tabindex="-1"></a><span class="do">##     24.919470977      0.003748497      9.920884113 </span></span>
<span id="cb103-7"><a href="linear-model-estimation.html#cb103-7" tabindex="-1"></a><span class="do">##    speciesGentoo </span></span>
<span id="cb103-8"><a href="linear-model-estimation.html#cb103-8" tabindex="-1"></a><span class="do">##      3.557977539</span></span></code></pre></div>
<p>Thus, the fitted parallel lines model is
<span class="math display" id="eq:pl-model-penguins">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G.
\end{aligned}
\tag{3.25}
\]</span>
Note that <code>speciesChinstrap</code> and <code>speciesGentoo</code> are the indicator variables related to the <code>Chinstrap</code> and <code>Gentoo</code> levels of <code>species</code>, respectively, i.e., they represent <span class="math inline">\(D_C\)</span> and <span class="math inline">\(D_G\)</span>. When an observation has <code>species</code> level <code>Adelie</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.25)</a> simplifies to
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 0 + 3.56 \cdot 0 \\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span>
When an observation has <code>species</code> level <code>Chinstrap</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.25)</a> simplifies to
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 1 + 3.56 \cdot 0 \\
&amp;= 34.84 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span>
Lastly, when an observation has <code>species</code> level <code>Gentoo</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.25)</a> simplifies to
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Gentoo}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 0 + 3.56 \cdot 1 \\
&amp;= 28.48 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span>
Adding fitted lines for each <code>species</code> level to the scatter plot in Figure <a href="linear-model-estimation.html#fig:penguins-grouped-scatter">3.6</a> is a bit more difficult than before. One technique is to use <code>predict</code> to get the fitted values of each observation, use the <code>transform</code> function to add those values as a column to the original the data frame, then use <code>geom_line</code> to connect the fitted values from each group.</p>
<p>We start by adding our fitted values to the <code>penguins</code> data frame. We use the <code>predict</code> function to obtained the fitted values of our fitted model and then use the <code>transform</code> function to add those values as the <code>pl_fitted</code> variable in the <code>penguins</code> data frame.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="linear-model-estimation.html#cb104-1" tabindex="-1"></a>penguins <span class="ot">&lt;-</span></span>
<span id="cb104-2"><a href="linear-model-estimation.html#cb104-2" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb104-3"><a href="linear-model-estimation.html#cb104-3" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">pl_fitted =</span> <span class="fu">predict</span>(lmodp))</span>
<span id="cb104-4"><a href="linear-model-estimation.html#cb104-4" tabindex="-1"></a><span class="do">## Error in data.frame(structure(list(species = structure(c(1L, 1L, 1L, 1L, : arguments imply differing number of rows: 344, 342</span></span></code></pre></div>
<p>We just received a nasty error. What is going on? The original <code>penguins</code> data frame has 344 rows. However, two rows had <code>NA</code> observations such that when we used the <code>lm</code> function to fit our parallel lines model, those observations were removed prior to fitting. The <code>predict</code> function produces fitted values for the observations used in the fitting process, so there are only 342 predicted values. There is a mismatch between the number of rows in <code>penguins</code> and the number of values we attempt to add in the new column <code>pl_fitted</code>, so we get an error.</p>
<p>To handle this error, we refit our model while setting the <code>na.action</code> argument to <code>na.exclude</code>. As stated Details section of the documentation for the <code>lm</code> function (run <code>?lm</code> in the Console):</p>
<blockquote>
<p><span class="math inline">\(\ldots\)</span> when <code>na.exclude</code> is used the residuals and predictions are padded to the correct length by inserting <code>NA</code>s for cases omitted by <code>na.exclude</code>.</p>
</blockquote>
<p>We refit the parallel lines model below with <code>na.action = na.exclude</code>, then use the <code>predict</code> function to add the fitted values to the <code>penguins</code> data frame via the <code>transform</code> function.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="linear-model-estimation.html#cb105-1" tabindex="-1"></a><span class="co"># refit parallel lines model with new na.action behavior</span></span>
<span id="cb105-2"><a href="linear-model-estimation.html#cb105-2" tabindex="-1"></a>lmodp <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species, <span class="at">data =</span> penguins, <span class="at">na.action =</span> na.exclude)</span>
<span id="cb105-3"><a href="linear-model-estimation.html#cb105-3" tabindex="-1"></a><span class="co"># add fitted values to penguins data frame</span></span>
<span id="cb105-4"><a href="linear-model-estimation.html#cb105-4" tabindex="-1"></a>penguins <span class="ot">&lt;-</span></span>
<span id="cb105-5"><a href="linear-model-estimation.html#cb105-5" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb105-6"><a href="linear-model-estimation.html#cb105-6" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">pl_fitted =</span> <span class="fu">predict</span>(lmodp))</span></code></pre></div>
<p>We now use the <code>geom_line</code> function to add the fitted lines for each <code>species</code> level to our scatter plot. Figure <a href="linear-model-estimation.html#fig:pl-penguin-fit">3.7</a> displays the results from running the code below. The parallel lines model shown in Figure <a href="linear-model-estimation.html#fig:pl-penguin-fit">3.7</a> fits the <code>penguins</code> data better than the simple linear regression model shown in Figure <a href="linear-model-estimation.html#fig:slr-penguin-fit">3.5</a>.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="linear-model-estimation.html#cb106-1" tabindex="-1"></a><span class="co"># create plot</span></span>
<span id="cb106-2"><a href="linear-model-estimation.html#cb106-2" tabindex="-1"></a><span class="co"># create scatterplot</span></span>
<span id="cb106-3"><a href="linear-model-estimation.html#cb106-3" tabindex="-1"></a><span class="co"># customize labels</span></span>
<span id="cb106-4"><a href="linear-model-estimation.html#cb106-4" tabindex="-1"></a><span class="co"># add lines for each level of species</span></span>
<span id="cb106-5"><a href="linear-model-estimation.html#cb106-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span></span>
<span id="cb106-6"><a href="linear-model-estimation.html#cb106-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm,</span>
<span id="cb106-7"><a href="linear-model-estimation.html#cb106-7" tabindex="-1"></a>                 <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb106-8"><a href="linear-model-estimation.html#cb106-8" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb106-9"><a href="linear-model-estimation.html#cb106-9" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> pl_fitted, <span class="at">color =</span> species))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pl-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/pl-penguin-fit-1.png" alt="The fitted lines from the separate lines model for each level of `species` is added to the grouped scatter plot of `bill_length_mm` versus `body_mass_g`." width="672" />
<p class="caption">
Figure 3.7: The fitted lines from the separate lines model for each level of <code>species</code> is added to the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code>.
</p>
</div>
<p>We now fit a separate lines regression model to the <code>penguins</code> data. Specifically, we fit the model
<span class="math display">\[
\begin{aligned}
&amp;E(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= \beta_{0} + \beta_1 \mathtt{body\_mass\_g} + \beta_2 D_C + \beta_3 D_G + \beta_4 \mathtt{body\_mass\_g} D_C + \beta_5 \mathtt{body\_mass\_g} D_G ,
\end{aligned}
\]</span>
using the code below, using the <code>coef</code> function to extract the estimated coefficients. The terms with <code>:</code> are interaction variables, e.g., <code>body_mass_g:speciesChinstrap</code> is <span class="math inline">\(\hat{\beta}_4\)</span>, the coefficient for the interaction between regressor <span class="math inline">\(\mathtt{body\_mass\_g} D_C\)</span>.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="linear-model-estimation.html#cb107-1" tabindex="-1"></a><span class="co"># fit separate lines model</span></span>
<span id="cb107-2"><a href="linear-model-estimation.html#cb107-2" tabindex="-1"></a><span class="co"># na.omit = na.exclude used to change predict behavior</span></span>
<span id="cb107-3"><a href="linear-model-estimation.html#cb107-3" tabindex="-1"></a>lmods <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species <span class="sc">+</span> body_mass_g<span class="sc">:</span>species,</span>
<span id="cb107-4"><a href="linear-model-estimation.html#cb107-4" tabindex="-1"></a>            <span class="at">data =</span> penguins, <span class="at">na.action =</span> na.exclude)</span>
<span id="cb107-5"><a href="linear-model-estimation.html#cb107-5" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb107-6"><a href="linear-model-estimation.html#cb107-6" tabindex="-1"></a><span class="fu">coef</span>(lmods)</span>
<span id="cb107-7"><a href="linear-model-estimation.html#cb107-7" tabindex="-1"></a><span class="do">##                  (Intercept)                  body_mass_g </span></span>
<span id="cb107-8"><a href="linear-model-estimation.html#cb107-8" tabindex="-1"></a><span class="do">##                26.9941391367                 0.0031878758 </span></span>
<span id="cb107-9"><a href="linear-model-estimation.html#cb107-9" tabindex="-1"></a><span class="do">##             speciesChinstrap                speciesGentoo </span></span>
<span id="cb107-10"><a href="linear-model-estimation.html#cb107-10" tabindex="-1"></a><span class="do">##                 5.1800537287                -0.2545906615 </span></span>
<span id="cb107-11"><a href="linear-model-estimation.html#cb107-11" tabindex="-1"></a><span class="do">## body_mass_g:speciesChinstrap    body_mass_g:speciesGentoo </span></span>
<span id="cb107-12"><a href="linear-model-estimation.html#cb107-12" tabindex="-1"></a><span class="do">##                 0.0012748183                 0.0009029956</span></span></code></pre></div>
<p>Thus, the fitted separate lines model is</p>
<p><span class="math display" id="eq:sl-model-penguins">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&amp; \quad + 0.001 \mathtt{body\_mass\_g} D_C + 0.0009 \mathtt{body\_mass\_g} D_G. \end{aligned}
\tag{3.26}
\]</span></p>
<p>When an observation has <code>species</code> level <code>Adelie</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.26)</a> simplifies to</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 0 - 0.25 \cdot 0\\
&amp;\quad + 0.001 \cdot \mathtt{body\_mass\_g} \cdot 0 + 0.0009 \cdot \mathtt{body\_mass\_g} \cdot 0\\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span></p>
<p>When an observation has <code>species</code> level <code>Chinstrap</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.26)</a> simplifies to
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 1 - 0.25 \cdot 0 \\
&amp;\quad + 0.001 \cdot \mathtt{body\_mass\_g} \cdot 1 + 0.0009 \cdot \mathtt{body\_mass\_g} \cdot 0 \\
&amp;= 32.17 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span>
When an observation has <code>species</code> level <code>Gentoo</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.26)</a> simplifies to
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 0 - 0.25 \cdot 1 \\
&amp;\quad + 0.001 \cdot \mathtt{body\_mass\_g} \cdot 0 + 0.0009 \cdot \mathtt{body\_mass\_g} \cdot 1 \\
&amp;= 26.74 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span></p>
<p>We use the code below to display the fitted lines for the separate lines model on the <code>penguins</code> data. Figure <a href="linear-model-estimation.html#fig:sl-penguin-fit">3.8</a> shows the results. The fitted lines match the observed data behavior reasonably well.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="linear-model-estimation.html#cb108-1" tabindex="-1"></a><span class="co"># add separate lines fitted values to penguins data frame</span></span>
<span id="cb108-2"><a href="linear-model-estimation.html#cb108-2" tabindex="-1"></a>penguins <span class="ot">&lt;-</span></span>
<span id="cb108-3"><a href="linear-model-estimation.html#cb108-3" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb108-4"><a href="linear-model-estimation.html#cb108-4" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">sl_fitted =</span> <span class="fu">predict</span>(lmods))</span>
<span id="cb108-5"><a href="linear-model-estimation.html#cb108-5" tabindex="-1"></a><span class="co"># use geom_line to add fitted lines to plot</span></span>
<span id="cb108-6"><a href="linear-model-estimation.html#cb108-6" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span></span>
<span id="cb108-7"><a href="linear-model-estimation.html#cb108-7" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm, <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb108-8"><a href="linear-model-estimation.html#cb108-8" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-9"><a href="linear-model-estimation.html#cb108-9" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> sl_fitted, <span class="at">col =</span> species))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:sl-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/sl-penguin-fit-1.png" alt="The fitted model for each level of `species` is added to the grouped scatter plot of `bill_length_mm` versus `body_mass_g`." width="672" />
<p class="caption">
Figure 3.8: The fitted model for each level of <code>species</code> is added to the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code>.
</p>
</div>
<p>Having fit several models for the <code>penguins</code> data, we may be wondering how to evaluate how well the models fit the data. We discuss that in the next section.</p>
</div>
<div id="evaluating-model-fit" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Evaluating model fit<a href="linear-model-estimation.html#evaluating-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most basic statistic measuring the fit of a regression model is the <strong>coefficient of determination</strong>, which is defined as
<span class="math display" id="eq:rsquared">\[
R^2 = 1 - \frac{\sum_{i=1}^n (Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2},\tag{3.27}
\]</span>
where <span class="math inline">\(\bar{Y}\)</span> is the sample mean of the observed response values.</p>
<p>To interpret this statistic, we need to introduce some new “sum-of-squares” statistics similar to the RSS.</p>
<p>The <strong>total sum of squares</strong> (corrected for the mean) is computed as
<span class="math display" id="eq:tss">\[
TSS = \sum_{i=1}^n(Y_i-\bar{Y})^2. \tag{3.28}
\]</span>
The TSS is the sum of the squared deviations of the response values from the sample mean. However, it has a more insightful interpretation. Consider the <strong>constant mean model</strong>, which is the model
<span class="math display" id="eq:constant-mean-model">\[
E(Y)=\beta_0. \tag{3.29}
\]</span>
Using basic calculus, we can show that the OLS estimator of <span class="math inline">\(\beta_0\)</span> for the model in Equation <a href="linear-model-estimation.html#eq:constant-mean-model">(3.29)</a> is <span class="math inline">\(\hat{\beta}_0=\bar{Y}\)</span>. For the constant mean model, the fitted value of every observation is <span class="math inline">\(\hat{\beta}_0\)</span>, i.e., <span class="math inline">\(\hat{Y}_i=\hat{\beta}_0\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>. Thus, the RSS of the constant mean model is <span class="math inline">\(\sum_{i=1}^n(Y_i-\hat{Y}_i)^2=\sum_{i=1}^n(Y_i-\bar{Y})^2\)</span>. Thus, <em>the TSS is the RSS for the constant mean model</em>.</p>
<p>The <strong>regression sum-of-squares</strong> or <strong>model sum-of-squares</strong> is defined as
<span class="math display" id="eq:ssreg">\[
SS_{reg} = \sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2. \tag{3.30}
\]</span>
Thus, SS<sub>reg</sub> is the sum of the squared deviations between the fitted values of a model and the fitted values of the constant mean model. More helpfully, we have the following equation relating TSS, RSS, and SS<sub>reg</sub>:
<span class="math display" id="eq:ss-equality">\[
TSS = RSS + SS_{reg}.\tag{3.31}
\]</span>
Thus, <span class="math inline">\(SS_{reg}=TSS-RSS\)</span>. This means that <em>SS<sub>reg</sub> measures the reduction in RSS when comparing the fitted model to the constant mean model</em>.</p>
<p>Comparing Equations <a href="linear-model-estimation.html#eq:def-rss-slr">(3.6)</a>, <a href="linear-model-estimation.html#eq:rsquared">(3.27)</a>, <a href="linear-model-estimation.html#eq:tss">(3.28)</a>, and <a href="linear-model-estimation.html#eq:ssreg">(3.30)</a>, we can express <span class="math inline">\(R^2\)</span> as:
<span class="math display" id="eq:rsquared2">\[
\begin{aligned}
R^2 &amp;= 1-\frac{\sum_{i=1}^n (Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} \\
&amp;= 1 - \frac{RSS}{TSS} \\
&amp;= \frac{TSS - RSS}{TSS} \\
&amp;= \frac{SS_{reg}}{TSS} \\
&amp;= [\mathrm{cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
\end{aligned}
\tag{3.32}
\]</span>
The last expression is the squared sample correlation between the observed and fitted values, and is a helpful way to express the coefficient of determination because it extends to regression models that are not linear.</p>
<p>Looking at Equation <a href="linear-model-estimation.html#eq:rsquared2">(3.32)</a> in particular, we can say that <em>the coefficient of determination is the proportional reduction in RSS when comparing the fitted model to the constant mean model</em>.</p>
<p>Some comments about the coefficient of determination:</p>
<ul>
<li><span class="math inline">\(0\leq R^2 \leq 1\)</span>.</li>
<li><span class="math inline">\(R^2=0\)</span> for the constant mean model.</li>
<li><span class="math inline">\(R^2=1\)</span> for a fitted model that perfectly fits the data (the fitted values match the observed response values).</li>
<li>Generally, larger values of <span class="math inline">\(R^2\)</span> suggest that the model explains a lot of the variation in the response variable. Smaller <span class="math inline">\(R^2\)</span> values suggest the fitted model does not explain a lot of the response variation.</li>
<li>The <code>Multiple R-squared</code> value printed by the <code>summary</code> of an <code>lm</code> object is <span class="math inline">\(R^2\)</span>.</li>
<li>To extract <span class="math inline">\(R^2\)</span> from a fitted model, we can use the syntax <code>summary(lmod)$r.squared</code>, where <code>lmod</code> is our fitted model.</li>
</ul>
<p>Figure <a href="linear-model-estimation.html#fig:rsquared-examples">3.9</a> provides examples of the <span class="math inline">\(R^2\)</span> value for various fitted simple linear regression models. The closer the points fall to a straight line, the larger <span class="math inline">\(R^2\)</span> tends to be. However, as shown in the bottom right panel of Figure <a href="linear-model-estimation.html#fig:rsquared-examples">3.9</a>, a poorly fit model can result in a lower <span class="math inline">\(R^2\)</span> model even if there is a clear relationship between the points (the points have a perfect quadratic relationship).</p>
<div class="figure"><span style="display:block;" id="fig:rsquared-examples"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/rsquared-examples-1.png" alt="The coefficient of determination values for 4 different data sets." width="672" />
<p class="caption">
Figure 3.9: The coefficient of determination values for 4 different data sets.
</p>
</div>
<p>The coefficient of determination for the parallel lines model fit to the <code>penguins</code> data in Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a> is 0.81, as shown in the R output below. By adding the <code>body_mass_g</code> regressor and <code>species</code> predictor to the constant mean model of <code>bill_length_mm</code>, we reduced the RSS by 81%.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="linear-model-estimation.html#cb109-1" tabindex="-1"></a><span class="fu">summary</span>(lmodp)<span class="sc">$</span>r.squared</span>
<span id="cb109-2"><a href="linear-model-estimation.html#cb109-2" tabindex="-1"></a><span class="do">## [1] 0.8079566</span></span></code></pre></div>
<p>It may seem sensible to choose between models based on the value of <span class="math inline">\(R^2\)</span>. This is unwise for two reasons:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R^2\)</span> never decreases as regressors are added to an existing model. Basically, we can increase <span class="math inline">\(R^2\)</span> by simply adding regressors to our existing model, even if they are non-sensical.</li>
<li><span class="math inline">\(R^2\)</span> doesn’t tell us whether a model adequately describes the pattern of the observed data. <span class="math inline">\(R^2\)</span> is a useful statistic for measuring model fit when there is approximately a linear relationship between the response values and fitted values.</li>
</ol>
<p>Regarding point 1, consider what happens when we add a regressor of random values to the parallel lines model fit to the <code>penguins</code> data. The code below sets a random number seed so that we can get the same results each time we run the code, creates the regressor <code>noisyx</code> by sampling 344 values randomly drawn from a <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution, adds <code>noisyx</code> as a regressor to the parallel lines regression model stored in <code>lmodp</code>, and then extracts the <span class="math inline">\(R^2\)</span> value. We use the <code>update</code> method to update our existing model. The <code>update</code> function takes an existing model as its first argument and then the <code>formula</code> for the updated model. The syntax <code>. ~ .</code> means “keep the same response (on the left) and the same regressors (on the right)”. We can then add or subtract regressors using the typical <code>formula</code> syntax. We use this approach to add the <code>noisyx</code> regressor to the regressors already in <code>lmodp</code>.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="linear-model-estimation.html#cb110-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">28</span>) <span class="co"># for reproducibility</span></span>
<span id="cb110-2"><a href="linear-model-estimation.html#cb110-2" tabindex="-1"></a><span class="co"># create regressor of random noise</span></span>
<span id="cb110-3"><a href="linear-model-estimation.html#cb110-3" tabindex="-1"></a>noisyx <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">344</span>)</span>
<span id="cb110-4"><a href="linear-model-estimation.html#cb110-4" tabindex="-1"></a><span class="co"># add noisyx as regressor to lmodp</span></span>
<span id="cb110-5"><a href="linear-model-estimation.html#cb110-5" tabindex="-1"></a>lmod_silly <span class="ot">&lt;-</span> <span class="fu">update</span>(lmodp, . <span class="sc">~</span> . <span class="sc">+</span> noisyx)</span>
<span id="cb110-6"><a href="linear-model-estimation.html#cb110-6" tabindex="-1"></a><span class="co"># extract R^2 from fitted model</span></span>
<span id="cb110-7"><a href="linear-model-estimation.html#cb110-7" tabindex="-1"></a><span class="fu">summary</span>(lmod_silly)<span class="sc">$</span>r.squared</span>
<span id="cb110-8"><a href="linear-model-estimation.html#cb110-8" tabindex="-1"></a><span class="do">## [1] 0.8087789</span></span></code></pre></div>
<p>The <span class="math inline">\(R^2\)</span> value increased from 0.8080 to 0.8088! So clearly, choosing the model with the largest <span class="math inline">\(R^2\)</span> can be a mistake, as it will tend to favor models with more regressors.</p>
<p>Regarding point 2, <span class="math inline">\(R^2\)</span> can mislead us into thinking an inappropriate model fits better than it actually does. <span class="citation">Anscombe (<a href="#ref-anscombe1973graphs">1973</a>)</span> provided a canonical data set known as “Anscombe’s quartet” that illustrates this point. The data set is comprised of 4 different data sets. When a simple linear regression model is fit to each data set, we find that <span class="math inline">\(\hat{\beta}_0=3\)</span>, <span class="math inline">\(\hat{\beta}_1=0.5\)</span>, and that <span class="math inline">\(R^2=0.67\)</span>. However, as we will see, not all models describe the data particularly well!</p>
<p>Anscombe’s quartet is available as the <code>anscombe</code> data set in the <strong>datasets</strong> package. The data set includes 11 observations of
8 variables. The variables are:</p>
<ul>
<li><code>x1</code>, <code>x2</code>, <code>x3</code>, <code>x4</code>: the regressor variable for each individual data set.</li>
<li><code>y1</code>, <code>y2</code>, <code>y3</code>, <code>y4</code>: the response variable for each individual data set.</li>
</ul>
<p>We fit the simple linear regression model to the four data sets in the code below, then extract the coefficients and <span class="math inline">\(R^2\)</span> to verify the information provided above.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="linear-model-estimation.html#cb111-1" tabindex="-1"></a><span class="co"># fit model to first data set</span></span>
<span id="cb111-2"><a href="linear-model-estimation.html#cb111-2" tabindex="-1"></a>lmod_a1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1, <span class="at">data =</span> anscombe)</span>
<span id="cb111-3"><a href="linear-model-estimation.html#cb111-3" tabindex="-1"></a><span class="co"># extract coefficients from fitted model</span></span>
<span id="cb111-4"><a href="linear-model-estimation.html#cb111-4" tabindex="-1"></a><span class="fu">coef</span>(lmod_a1)</span>
<span id="cb111-5"><a href="linear-model-estimation.html#cb111-5" tabindex="-1"></a><span class="do">## (Intercept)          x1 </span></span>
<span id="cb111-6"><a href="linear-model-estimation.html#cb111-6" tabindex="-1"></a><span class="do">##   3.0000909   0.5000909</span></span>
<span id="cb111-7"><a href="linear-model-estimation.html#cb111-7" tabindex="-1"></a><span class="co"># extract R^2 from fitted model</span></span>
<span id="cb111-8"><a href="linear-model-estimation.html#cb111-8" tabindex="-1"></a><span class="fu">summary</span>(lmod_a1)<span class="sc">$</span>r.squared</span>
<span id="cb111-9"><a href="linear-model-estimation.html#cb111-9" tabindex="-1"></a><span class="do">## [1] 0.6665425</span></span>
<span id="cb111-10"><a href="linear-model-estimation.html#cb111-10" tabindex="-1"></a><span class="co"># fit model to second data set</span></span>
<span id="cb111-11"><a href="linear-model-estimation.html#cb111-11" tabindex="-1"></a>lmod_a2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> x2, <span class="at">data =</span> anscombe)</span>
<span id="cb111-12"><a href="linear-model-estimation.html#cb111-12" tabindex="-1"></a><span class="fu">coef</span>(lmod_a2)</span>
<span id="cb111-13"><a href="linear-model-estimation.html#cb111-13" tabindex="-1"></a><span class="do">## (Intercept)          x2 </span></span>
<span id="cb111-14"><a href="linear-model-estimation.html#cb111-14" tabindex="-1"></a><span class="do">##    3.000909    0.500000</span></span>
<span id="cb111-15"><a href="linear-model-estimation.html#cb111-15" tabindex="-1"></a><span class="fu">summary</span>(lmod_a2)<span class="sc">$</span>r.squared</span>
<span id="cb111-16"><a href="linear-model-estimation.html#cb111-16" tabindex="-1"></a><span class="do">## [1] 0.666242</span></span>
<span id="cb111-17"><a href="linear-model-estimation.html#cb111-17" tabindex="-1"></a><span class="co"># fit model to third data set</span></span>
<span id="cb111-18"><a href="linear-model-estimation.html#cb111-18" tabindex="-1"></a>lmod_a3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y3 <span class="sc">~</span> x3, <span class="at">data =</span> anscombe)</span>
<span id="cb111-19"><a href="linear-model-estimation.html#cb111-19" tabindex="-1"></a><span class="fu">coef</span>(lmod_a3)</span>
<span id="cb111-20"><a href="linear-model-estimation.html#cb111-20" tabindex="-1"></a><span class="do">## (Intercept)          x3 </span></span>
<span id="cb111-21"><a href="linear-model-estimation.html#cb111-21" tabindex="-1"></a><span class="do">##   3.0024545   0.4997273</span></span>
<span id="cb111-22"><a href="linear-model-estimation.html#cb111-22" tabindex="-1"></a><span class="fu">summary</span>(lmod_a3)<span class="sc">$</span>r.squared</span>
<span id="cb111-23"><a href="linear-model-estimation.html#cb111-23" tabindex="-1"></a><span class="do">## [1] 0.666324</span></span>
<span id="cb111-24"><a href="linear-model-estimation.html#cb111-24" tabindex="-1"></a><span class="co"># fit model to fourth data set</span></span>
<span id="cb111-25"><a href="linear-model-estimation.html#cb111-25" tabindex="-1"></a>lmod_a4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y4 <span class="sc">~</span> x4, <span class="at">data =</span> anscombe)</span>
<span id="cb111-26"><a href="linear-model-estimation.html#cb111-26" tabindex="-1"></a><span class="fu">coef</span>(lmod_a4)</span>
<span id="cb111-27"><a href="linear-model-estimation.html#cb111-27" tabindex="-1"></a><span class="do">## (Intercept)          x4 </span></span>
<span id="cb111-28"><a href="linear-model-estimation.html#cb111-28" tabindex="-1"></a><span class="do">##   3.0017273   0.4999091</span></span>
<span id="cb111-29"><a href="linear-model-estimation.html#cb111-29" tabindex="-1"></a><span class="fu">summary</span>(lmod_a4)<span class="sc">$</span>r.squared</span>
<span id="cb111-30"><a href="linear-model-estimation.html#cb111-30" tabindex="-1"></a><span class="do">## [1] 0.6667073</span></span></code></pre></div>
<p>Figure <a href="linear-model-estimation.html#fig:anscombe-plots">3.10</a> provides a scatter plot each data set and overlays their fitted models.</p>
<div class="figure"><span style="display:block;" id="fig:anscombe-plots"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/anscombe-plots-1.png" alt="Scatter plots of the four Anscombe data sets along with their line of best fit." width="672" />
<p class="caption">
Figure 3.10: Scatter plots of the four Anscombe data sets along with their line of best fit.
</p>
</div>
<p>While the fitted model and <span class="math inline">\(R^2\)</span> value is essentially the same for each model, the fitted model is only appropriate for data set 1. The fitted model for the second data set fails to model the curve of the data. The third fitted model doesn’t handle the outlier in the data. Lastly, the fourth data set has a single point on the far right side driving the model fit, so the fitted model is highly questionable.</p>
<p>To address the problem with <span class="math inline">\(R^2\)</span> that it cannot decrease as regressors are added to a model, <span class="citation">Ezekiel (<a href="#ref-ezekiel1930methods">1930</a>)</span> proposed the adjusted R-squared statistic for measuring model fit. The adjusted <span class="math inline">\(R^2\)</span> statistic is defined as
<span class="math display">\[
R^2_a=1-(1-R^2)\frac{n-1}{n-p}=1-\frac{RSS/(n-p)}{TSS/(n-1)}.
\]</span>
Practically speaking, <span class="math inline">\(R^2_a\)</span> will only increase when a regressors substantively improves the fit of the model to the observed data. We favor models with larger values of <span class="math inline">\(R^2_a\)</span>. To extract the adjusted R-squared from a fitted model, we can use the syntax <code>summary(lmod)$adj.R.squared</code>, where <code>lmod</code> is the fitted model.</p>
<p>Using the code below, we extract the <span class="math inline">\(R^2_a\)</span> for the 4 models we previously fit to the <code>penguins</code> data. Specifically, we extract <span class="math inline">\(R_a^2\)</span> for the simple linear regression model fit in Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>, the multiple linear regression model in Section <a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>, and the parallel and separate lines models fit in Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a>.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="linear-model-estimation.html#cb112-1" tabindex="-1"></a><span class="co"># simple linear regression model</span></span>
<span id="cb112-2"><a href="linear-model-estimation.html#cb112-2" tabindex="-1"></a><span class="fu">summary</span>(lmod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb112-3"><a href="linear-model-estimation.html#cb112-3" tabindex="-1"></a><span class="do">## [1] 0.3522562</span></span>
<span id="cb112-4"><a href="linear-model-estimation.html#cb112-4" tabindex="-1"></a><span class="co"># multiple linear regression model</span></span>
<span id="cb112-5"><a href="linear-model-estimation.html#cb112-5" tabindex="-1"></a><span class="fu">summary</span>(mlmod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb112-6"><a href="linear-model-estimation.html#cb112-6" tabindex="-1"></a><span class="do">## [1] 0.4295084</span></span>
<span id="cb112-7"><a href="linear-model-estimation.html#cb112-7" tabindex="-1"></a><span class="co"># parallel lines model</span></span>
<span id="cb112-8"><a href="linear-model-estimation.html#cb112-8" tabindex="-1"></a><span class="fu">summary</span>(lmodp)<span class="sc">$</span>adj.r.squared</span>
<span id="cb112-9"><a href="linear-model-estimation.html#cb112-9" tabindex="-1"></a><span class="do">## [1] 0.8062521</span></span>
<span id="cb112-10"><a href="linear-model-estimation.html#cb112-10" tabindex="-1"></a><span class="co"># separate lines model</span></span>
<span id="cb112-11"><a href="linear-model-estimation.html#cb112-11" tabindex="-1"></a><span class="fu">summary</span>(lmods)<span class="sc">$</span>adj.r.squared</span>
<span id="cb112-12"><a href="linear-model-estimation.html#cb112-12" tabindex="-1"></a><span class="do">## [1] 0.8069556</span></span></code></pre></div>
<p>With an <span class="math inline">\(R_a^2\)</span> of 0.8070, the separate lines regression model appears to be slightly favored over the other 3 models fit to the <code>penguins</code> data. To confirm that this statistic is meaningful (i.e., that the model provides a reasonable fit to the data), we use the code below to create a scatter plot of the response versus fitted values shown in Figure <a href="linear-model-estimation.html#fig:y-vs-yhat-penguins">3.11</a>. The points in Figure <a href="linear-model-estimation.html#fig:y-vs-yhat-penguins">3.11</a> follow a linear pattern, so the separate lines model seems to be a reasonable model for the <code>penguins</code> data.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="linear-model-estimation.html#cb113-1" tabindex="-1"></a><span class="fu">plot</span>(penguins<span class="sc">$</span>bill_length_mm <span class="sc">~</span> <span class="fu">fitted</span>(lmods),</span>
<span id="cb113-2"><a href="linear-model-estimation.html#cb113-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:y-vs-yhat-penguins"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/y-vs-yhat-penguins-1.png" alt="A scatter plot of the observed bill length versus the fitted values of the separate lines model for the `penguins` data." width="672" />
<p class="caption">
Figure 3.11: A scatter plot of the observed bill length versus the fitted values of the separate lines model for the <code>penguins</code> data.
</p>
</div>
</div>
<div id="summary" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Summary<a href="linear-model-estimation.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned:</p>
<ul>
<li>What a linear model is.</li>
<li>What various objects are, such as coefficients, residuals, fitted values, etc.</li>
<li>How to estimate the coefficients of a linear model using ordinary least squares estimation.</li>
<li>How to fit a linear model using R.</li>
<li>How to include a categorical predictor in a linear model.</li>
<li>How to evaluate the fit of a model.</li>
</ul>
<div id="ss:term-summary" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> Summary of terms<a href="linear-model-estimation.html#ss:term-summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have introduced many terms to define a linear model. It can be difficult to keep track of their notation, their purpose, whether they are observable, and whether they are treated as random variables or vectors. We discuss various terms below, and then summarize the discussion in Table <a href="linear-model-estimation.html#tab:term-df">3.2</a>.</p>
<p>We’ve already talked about observing the response variable and the predictor/regressor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. One way to distinguish observable versus non-observable variables is that observable variables are denoted using Phoenician letters (e.g., <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>) while non-observable variables are denoted using Greek letters (e.g., <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(\sigma^2\)</span>).</p>
<p>We treat the response variable as a random variable. Perhaps surprisingly, we treat the predictor and regressor variables as fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the regressor variables and the regression coefficients are non-random, the only way for the responses in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.14)</a> to be random variables is for the errors to be random.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:term-df">Table 3.2: </span>An overview of terms used to define a linear model.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Term
</th>
<th style="text-align:left;">
Description
</th>
<th style="text-align:left;">
Observable?
</th>
<th style="text-align:left;">
Random?
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(Y\)</span>
</td>
<td style="text-align:left;width: 2in; ">
response variable
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(Y_i\)</span>
</td>
<td style="text-align:left;width: 2in; ">
response value for the <span class="math inline">\(i\)</span>th observation
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathbf{y}\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(n\times 1\)</span> column vector of response values
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X\)</span>
</td>
<td style="text-align:left;width: 2in; ">
regressor variable
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(X_j\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(j\)</span>th regressor variable
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(x_{i,j}\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the value of the <span class="math inline">\(j\)</span>th regressor variable for the <span class="math inline">\(i\)</span>th observation
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathbf{X}\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(n\times p\)</span> matrix of regressor values
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathbf{x}_i\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(p\times 1\)</span> vector of regressor values for the <span class="math inline">\(i\)</span>th observation
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\beta_j\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the coefficient associated with the <span class="math inline">\(j\)</span>th regressor variable
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\boldsymbol{\beta}\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(p\times 1\)</span> column vector of regression coefficients
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
No
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\epsilon\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the model error
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\epsilon_i\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the error for the <span class="math inline">\(i\)</span>th observation
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\boldsymbol{\epsilon}\)</span>
</td>
<td style="text-align:left;width: 2in; ">
the <span class="math inline">\(n\times 1\)</span> column vector of errors
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
</tbody>
</table>
</div>
<div id="summary-of-functions" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> Summary of functions<a href="linear-model-estimation.html#summary-of-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have used many functions in this Chapter. We summarize some of the most important ones in Table <a href="linear-model-estimation.html#tab:function-df">3.3</a>.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:function-df">Table 3.3: </span>An overview of important functions discussed in this chapter.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Function
</th>
<th style="text-align:left;">
Purpose
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>lm</code>
</td>
<td style="text-align:left;width: 3in; ">
Fits a linear model based on a provided <code>formula</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>summary</code>
</td>
<td style="text-align:left;width: 3in; ">
Provides summary information about the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>coef</code>
</td>
<td style="text-align:left;width: 3in; ">
Extracts the vector of estimated regression coefficients from the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>residuals</code>
</td>
<td style="text-align:left;width: 3in; ">
Extracts the vector of residuals from the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>fitted</code>
</td>
<td style="text-align:left;width: 3in; ">
Extracts the vector of fitted values from the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>predict</code>
</td>
<td style="text-align:left;width: 3in; ">
Computes the fitted values (or arbitrary predictions) based on a fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>deviance</code>
</td>
<td style="text-align:left;width: 3in; ">
Extracts the RSS of a fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>sigma</code>
</td>
<td style="text-align:left;width: 3in; ">
Extracts <span class="math inline">\(\hat{\sigma}\)</span> from the fitted model
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>update</code>
</td>
<td style="text-align:left;width: 3in; ">
Updates a fitted model to remove or add regressors
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="going-deeper" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> Going Deeper<a href="linear-model-estimation.html#going-deeper" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="degrees-of-freedom" class="section level3 hasAnchor" number="3.12.1">
<h3><span class="header-section-number">3.12.1</span> Degrees of freedom<a href="linear-model-estimation.html#degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The degrees of freedom of a statistics refers to the number of independent pieces of information that go into its calculation.</p>
<p>Consider the sample mean
<span class="math display">\[\bar{x}=\sum_{i=1}^n x_i.\]</span></p>
<p>The calculation uses <span class="math inline">\(n\)</span> pieces of information to compute, but the statistic only has <span class="math inline">\(n-1\)</span> degrees of freedom. Once we know the sample mean, only <span class="math inline">\(n-1\)</span> values are independent, while the last is constrained to be a certain value.</p>
<p>Let’s say <span class="math inline">\(n=3\)</span> and <span class="math inline">\(\bar{x} = 10\)</span>. Then <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> can be any numbers, but the last value MUST equal <span class="math inline">\(30 - x_1 - x_2\)</span> so that <span class="math inline">\(x_1 + x_2 + x_3 = 30\)</span> (otherwise the sample mean won’t equal 10). To be more specific, if <span class="math inline">\(x_1 = 5\)</span> and <span class="math inline">\(x_2 = 25\)</span>, then <span class="math inline">\(x_3\)</span> must be 0, otherwise the sample mean won’t be 10.</p>
</div>
<div id="slr-derivation" class="section level3 hasAnchor" number="3.12.2">
<h3><span class="header-section-number">3.12.2</span> Derivation of the OLS estimators of the simple linear regression model coefficients<a href="linear-model-estimation.html#slr-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume a simple linear regression model with <span class="math inline">\(n\)</span> observations. The residual sum of squares for the simple linear regression model is
<span class="math display">\[
RSS(\hat\beta_0, \hat\beta_1) = \sum_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1x_i)^2.
\]</span></p>
<p><strong>OLS estimator of <span class="math inline">\(\beta_0\)</span></strong></p>
<p>First, we take the partial derivative of the RSS with respect to <span class="math inline">\(\hat\beta_0\)</span> and simplify:
<span class="math display">\[
\begin{aligned}
\frac{\partial RSS(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_0} &amp;= \frac{\partial}{\partial \hat\beta_0}\sum_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1x_i)^2 &amp; \tiny\text{ (substituting the formula for the RSS)} \\
&amp;= \sum_{i=1}^n \frac{\partial}{\partial \hat\beta_0}(Y_i - \hat\beta_0 - \hat\beta_1x_i)^2  &amp; \tiny\text{ (by the linearity property of derivatives)} \\
&amp;= -2\sum_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1x_i). &amp; \tiny\text{ (chain rule, factoring out -2)}
\end{aligned}
\]</span></p>
<p>Next, we set the partial derivative equal to zero and rearrange the terms to solve for <span class="math inline">\(\hat{\beta}_0\)</span>:
<span class="math display">\[
\begin{aligned}
0 &amp;= \frac{\partial RSS(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_0}  &amp;  \\
0 &amp;= -2\sum_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1x_i) &amp; \tiny\text{ (substitute partial deriviative)}\\
0 &amp;= \sum_{i=1}^n(Y_i - \hat\beta_0 - \hat\beta_1x_i) &amp; \tiny\text{ (divide both sides by -2)} \\
0 &amp;= \sum_{i=1}^n Y_i - \sum_{i=1}^n\hat\beta_0 - \sum_{i=1}^n\hat\beta_1x_i &amp;\tiny\text{ (by linearity of sum)} \\
0 &amp;= \sum_{i=1}^n Y_i - n\hat\beta_0 - \sum_{i=1}^n\hat\beta_1x_i &amp; \tiny(\text{sum }\hat\beta_0\ n\text{ times equals }n\hat\beta_0) \\
n\hat\beta_0 &amp;= \sum_{i=1}^n Y_i-\hat{\beta}_1\sum_{i=1}^nx_i. &amp;\tiny\text{ (algebra rearrange, factor }\hat{\beta}_1\text{)} \\
\end{aligned}
\]</span></p>
<p>Finally, we divide both sides by <span class="math inline">\(n\)</span> to get the OLS estimator for <span class="math inline">\(\hat\beta_0\)</span> in terms of <span class="math inline">\(\hat\beta_1\)</span>:
<span class="math display">\[
\hat\beta_0 = \bar Y-\hat\beta_1\bar x
\]</span></p>
<p><strong>OLS Estimator of <span class="math inline">\(\beta_1\)</span></strong></p>
<p>Similar to the previous derivation, we differentiate the RSS with respect to the parameter estimate of interest, set the derivative equal to zero, and solve for the parameter.</p>
<p>We start by taking the partial derivative of the RSS with respect to <span class="math inline">\(\hat{\beta}_1\)</span> and simplify.
<span class="math display">\[
\begin{aligned}
\frac{\partial RSS(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_1} &amp;= \frac{\partial}{\partial \hat\beta_1}\sum_{i=1}^n (Y_i - \hat\beta_0 - \hat\beta_1x_i)^2 &amp; \tiny\text{ (substitute formula for RSS)} \\
&amp;= \sum_{i=1}^n \frac{\partial}{\partial \hat\beta_1}(Y_i - \hat\beta_0 - \hat\beta_1x_i)^2 &amp; \tiny\text{ (linearity property of derivatives)} \\
&amp;= -2\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1x_i)x_i &amp; \tiny\text{ (chain rule, factor out -2)}
\end{aligned}
\]</span></p>
<p>We now set this derivative equal to 0 and rearrange the terms to solve for <span class="math inline">\(\hat{\beta}_1\)</span>:
<span class="math display">\[
\begin{aligned}
0 &amp;= \frac{\partial RSS(\hat\beta_0, \hat\beta_1)}{\partial \hat\beta_1} &amp; \\
0 &amp;= -2\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1x_i)x_i &amp;\tiny\text{(substitute partial derivative})\\
0 &amp;= \sum_{i=1}^n(Y_i-(\bar Y -\hat \beta_1\bar x)-\hat\beta_1x_i)x_i &amp;\tiny\text{(substitute OLS estimator of }\hat\beta_0, \text{ divide both sides by -2}) \\
0 &amp;= \sum_{i=1}^n x_iY_i -\sum_{i=1}^n x_i\bar Y+\hat\beta_1\bar x\sum_{i=1}^n x_i-\hat\beta_1\sum_{i=1}^n x_i^2. &amp;\tiny\text{(expand sum, use linearity of sum)} \\
\end{aligned}
\]</span></p>
<p>Continuing from the previous line, we move the terms involving <span class="math inline">\(\hat{\beta}_1\)</span> to the other side of the equality to get
<span class="math display">\[
\begin{aligned}
\hat\beta_1\sum_{i=1}^n x_i^2-\hat\beta_1\bar x\sum_{i=1}^n x_i &amp;=\sum_{i=1}^n x_iY_i -\sum_{i=1}^n x_i\bar Y &amp; \tiny\text{(move estimator to other side)}\\
\hat\beta_1\sum_{i=1}^n x_i^2-\hat\beta_1\frac{1}{n}\sum_{i=1}^n  x_i\sum_{i=1}^n x_i&amp;=\sum_{i=1}^n x_iY_i -\sum_{i=1}^n x_i\frac{1}{n}\sum_{i=1}^n  Y_i  &amp;\tiny\text{(rewrite using definition of sample means)} \\
\hat\beta_1\sum_{i=1}^n x_i^2-\hat\beta_1\frac{1}{n}\left(\sum_{i=1}^n  x_i\right)^2 &amp;=\sum_{i=1}^n x_iY_i -\frac{1}{n}\sum_{i=1}^n x_i\sum_{i=1}^n  Y_i  &amp; \tiny\text{(reorder and simplify)} \\
\hat\beta_1\left(\sum_{i=1}^n x_i^2-\frac{1}{n}\left(\sum_{i=1}^n  x_i\right)^2\right)&amp;=\sum_{i=1}^n x_iY_i -\frac{1}{n}\sum_{i=1}^n x_i\sum_{i=1}^n  Y_i, &amp; \tiny\text{(factoring)}\\
\end{aligned}
\]</span>
which allows us to obtain
<span class="math display">\[
\hat\beta_1=\frac{\sum_{i=1}^n x_iY_i -\frac{1}{n}\sum_{i=1}^n x_i\sum_{i=1}^n  Y_i}{\sum_{i=1}^n x_i^2-\frac{1}{n}\left(\sum_{i=1}^n  x_i\right)^2}.
\]</span></p>
<p>Thus, we have the OLS estimators of the simple linear regression coefficients are
<span class="math display">\[
\begin{aligned}
\hat\beta_0 &amp;= \bar Y-\hat\beta_1\bar x, \\
\hat\beta_1 &amp; =\frac{\sum_{i=1}^n x_iY_i -\frac{1}{n}\sum_{i=1}^n x_i\sum_{i=1}^n Y_i}{\sum_{i=1}^n x_i^2-\frac{1}{n}\left(\sum_{i=1}^n x_i\right)^2}.
\end{aligned}
\]</span></p>
</div>
<div id="unbiasedness-of-ols-estimators" class="section level3 hasAnchor" number="3.12.3">
<h3><span class="header-section-number">3.12.3</span> Unbiasedness of OLS estimators<a href="linear-model-estimation.html#unbiasedness-of-ols-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now show that the OLS estimators we derived in Section <a href="linear-model-estimation.html#slr-derivation">3.12.2</a> are unbiased. An estimator is unbiased if the expected value is equal to the parameter it is estimating.</p>
<p>The OLS estimator assumes we know the value of the regressor variables for all observations. Thus, we must condition our expectation on knowing the regressor matrix <span class="math inline">\(\mathbf{X}\)</span>. Thus, we want to show that
<span class="math display">\[
E(\hat{\beta}_0\mid \mathbf{X})=\beta_0,
\]</span>
where “<span class="math inline">\(\mid \mathbf{X}\)</span>” is convenient notation to indicate that we are conditioning our expectation on knowing the regressor values for every observation.</p>
<p>In Section <a href="linear-model-estimation.html#s-slr-estimation">3.2</a>, we noted that we assume <span class="math inline">\(E(\epsilon \mid X)=0\)</span>, which is true for every error in our model, i.e. <span class="math inline">\(E(\epsilon_i \mid X = x_i) = 0\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>. Thus,</p>
<p><span class="math display">\[
\begin{aligned}
E(Y_i\mid X=x_i) &amp;= E(\beta_0 + \beta_1 x_i +\epsilon_i\mid X = x_i) &amp; \tiny\text{(substiute definition of $Y_i$)} \\
&amp;= E(\beta_0\mid X=x_i) + E(\beta_1 x_i \mid X = X_i) +E(\epsilon_i | X=x_i) &amp; \tiny\text{(linearity property of expectation)} \\
&amp;= \beta_0+\beta_1x_i +E(\epsilon_i | X=x_i) &amp; \tiny\text{(the $\beta$s and $x_i$ are non-random values)} \\
&amp;= \beta_0+\beta_1x_i + 0 &amp; \tiny\text{(assumption about errors)} \\
&amp;= \beta_0+\beta_1x_i. &amp;
\end{aligned}
\]</span></p>
<p>In the derivations below, every sum is over all values of <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\(\sum \equiv \sum_{i=1}^n\)</span>. We drop the index for simplicity.</p>
<p>Next, we note:
<span class="math display">\[
\begin{aligned}
E\left(\sum x_iY_i \biggm| \mathbf{X} \right) &amp;= \sum E(x_iY_i \mid \mathbf{X}) &amp;\tiny\text{ (by the linearity of the expectation operator)}\\
&amp;=\sum x_iE(Y_i\mid \mathbf{X})&amp;\tiny(x_i\text{ is a fixed value, so it can be brought out})\\
&amp;=\sum x_i(\beta_0+\beta_1 x_i)&amp;\tiny\text{(substitute expected value of }Y_i)\\
&amp;=\sum x_i\beta_0+\sum x_i\beta_1 x_i&amp;\tiny\text{(distribute sum)}\\
&amp;=\beta_0\sum x_i+\beta_1\sum x_i^2.&amp;\tiny\text{(factor out constants)}
\end{aligned}
\]</span></p>
<p>Also,
<span class="math display">\[
\begin{aligned}
E(\bar Y\mid \mathbf{X})
&amp;= E\left(\frac{1}{n}\sum Y_i\Biggm|\mathbf{X} \right)&amp;\tiny\text{(definition of sample mean)}\\
&amp;= \frac{1}{n}E\left(\sum Y_i \Bigm| \mathbf{X}\right)&amp;\tiny\text{(factor out constant)}\\
&amp;= \frac{1}{n}\sum E\left(Y_i \mid \mathbf{X}\right)&amp;\tiny\text{(linearity of expectation)}\\
&amp;= \frac{1}{n}\sum(\beta_0+\beta_1 x_i)&amp;\tiny\text{(substitute expected value of }Y_i)\\
&amp;= \frac{1}{n}\left(\sum\beta_0+\sum\beta_1 x_i\right)&amp;\tiny\text{(distribute sum)}\\
&amp;= \frac{1}{n}\left(n\beta_0+\beta_1\sum x_i\right)&amp;\tiny\text{(simplify, factor out constant)}\\
&amp;= \beta_0+\beta_1\bar x. &amp;\tiny\text{(simplify)}
\end{aligned}
\]</span></p>
<p>To simplify our derivation below, define
<span class="math display">\[
SSX = \sum x_i^2-\frac{1}{n}\left(\sum  x_i\right)^2.
\]</span>
Thus,</p>
<p><span class="math display">\[
\begin{aligned}
&amp;E(\hat\beta_1 \mid \mathbf{X}) &amp;\\
&amp;= E\left(\frac{\sum x_iY_i -\frac{1}{n}\sum x_i\sum Y_i}{\sum x_i^2-\frac{1}{n}\left(\sum  x_i\right)^2} \Biggm| \mathbf{X} \right) &amp;\tiny\text{(substitute OLS estimator)} \\
&amp;= \frac{1}{SSX}E\left(\sum x_iY_i-\frac{1}{n}\sum x_i\sum Y_i \biggm| \mathbf{X}\right)&amp;\tiny\text{(factor out constant denominator, substitute }SSX\text{)} \\
&amp;= \frac{1}{SSX}\left[E\left(\sum x_iY_i\Bigm|\mathbf{X}\right)-E\left(\frac{1}{n}\sum x_i\sum Y_i\biggm|\mathbf{X}\right)\right]&amp;\tiny\text{(linearity of expectation)}\\
&amp;= \frac{1}{SSX}\left[E\left(\sum x_iY_i\Bigm|\mathbf{X}\right)-\left(\sum x_i\right)E\left(\bar Y\mid \mathbf{X}\right)\right]&amp;\tiny\text{(factor out constant }\sum x_i\text{, use definition of}\bar{Y})\\
&amp;= \frac{1}{SSX}\left[\left(\beta_0\sum x_i + \beta_1\sum x_i^2\right)-\left(\sum x_i\right)(\beta_0+\beta_1\bar x)\right]&amp;\tiny\text{(substitute previous derivations
)}\\
&amp;= \frac{1}{SSX}\left[\beta_0\sum x_i+\beta_1\sum x_i^2-\beta_0\sum x_i-\beta_1\bar x\sum x_i\right]&amp;\tiny\text{(expand product and reorder)} \\
&amp;= \frac{1}{SSX}\left[\beta_1\sum x_i^2-\beta_1\bar x\sum x_i\right]&amp;\tiny\text{(cancel terms)}\\
&amp;= \frac{1}{SSX}\left[\beta_1\sum x_i^2-\beta_1\frac{1}{n}\sum x_i\sum x_i\right]&amp;\tiny\text{(using definition of sample mean)}\\
&amp;= \frac{1}{SSX}\beta_1\left[\sum x_i^2-\frac{1}{n}\left(\sum x_i\right)^2\right]&amp;\tiny\text{(factor out }\beta_1\text{, simplify})\\
&amp;= \frac{1}{SSX}\beta_1[SSX]&amp;\tiny\text{(substitute }SSX\text{)} \\
&amp;=\beta_1. &amp;\tiny\text{(simplify)}
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math inline">\(\hat\beta_1\)</span> is an unbiased estimator of <span class="math inline">\(\beta_1\)</span>.</p>
<p>Next, we show that <span class="math inline">\(\hat\beta_0\)</span> is unbiased:
<span class="math display">\[
\begin{aligned}
E(\hat\beta_0\mid \mathbf{X}) &amp;= E(\bar Y - \hat{\beta}_1\bar x\mid \mathbf{X}) &amp;\tiny\text{(OLS estimator of }\beta_0) \\
&amp;= E(\bar{Y}\mid \mathbf{X}) - E(\hat\beta_1\bar{x}\mid \mathbf{X}) &amp;\tiny\text{(linearity of expectation})\\
&amp;= E(\bar{Y}\mid \mathbf{X}) - \bar{x}E(\hat\beta_1\mid \mathbf{X}) &amp;\tiny\text{(factor out constant})\\
&amp;= \beta_0 +\beta_1\bar x-\bar x\beta_1 &amp;\tiny\text{(substitute previous derivations})\\
&amp;= \beta_0. &amp;\tiny\text{(cancel terms})\\
\end{aligned}
\]</span></p>
<p>Therefore, <span class="math inline">\(\hat\beta_0\)</span> is an unbiased estimator of <span class="math inline">\(\beta_0\)</span>.</p>
<!-- Next, we derive the variance of the OLS estimators conditional on the known regressor values, i.e., $\mathrm{var}(\hat\beta_0 \mid \mathbf{X})$ and $\mathrm{var}(\hat\beta_1 \mid \mathbf{X})$. -->
<!-- First, we derive that -->
<!-- \[ -->
<!-- \begin{align} -->
<!-- \mathrm{var}(Y_i\mid X = x_i) &= \mathrm{var}(\beta_0+\beta_1x_i+\epsilon_i\mid X = x_i)&\tiny\text{(substitute model definition)} \\ -->
<!-- &= \mathrm{var}(\epsilon_i\mid X = x_i)&\tiny(\beta_0, \beta_1, x_i\text{ are fixed, so zero variance)} \\ -->
<!-- &= \sigma^2. &\tiny\text{(by assumption)} -->
<!-- \end{align} -->
<!-- \] -->
<!-- Second, we derive that -->
<!-- \[ -->
<!-- \begin{align} -->
<!-- \text{cov}(Y_i, Y_j\mid \mathbf{X}) &= \text{cov}(\beta_0+\beta_1x_i+\epsilon_i, \beta_0+\beta_1x_j+\epsilon_j\mid \mathbf{X})&\tiny\text{(substitute model definition)} \\ -->
<!-- &= \text{cov}(\epsilon_i,\epsilon_j\mid \mathbf{X})&\tiny\text{(other values are fixed)} \\ -->
<!-- &= 0.&\tiny\text{(errors are uncorrelated)} -->
<!-- \end{align} -->
<!-- \] -->
<!-- Next, to simplify the derivation, we use a different form of $\hat\beta_1$ from Equation \@ref(eq:slr-beta1hat), namely, -->
<!-- \[ -->
<!-- \begin{align} -->
<!-- \mathrm{var}(\hat\beta_1\mid \mathbf{X}) &=\mathrm{var}\left(\frac{\sum(x_i-\bar x)Y_i}{\sum(x_i-\bar x)^2}\mid \mathbf{X}\right)&\tiny\text{(expression for }\hat\beta_1)\\ -->
<!-- &=\frac{1}{\left[\sum(x_i-\bar x)^2\right]^2}\mathrm{var}\left(\sum(x_i-\bar x)Y_i\Bigm| \mathbf{X}\right)&\tiny\text{(factor out constant denominator)}\\ -->
<!-- &=\frac{1}{\left[\sum(x_i-\bar x)^2\right]^2}\biggl[\sum\mathrm{var}((x_i-\bar x)Y_i\mid \mathbf{X})&\\ -->
<!-- &\qquad+\sum_{i=1}^{n}\sum_{i\neq j}\text{cov}((x_i-\bar x)Y_i, (x_j-\bar x)Y_j\mid \mathbf{X})\biggr]&\tiny\text{(variance of a sum formula)}\\ -->
<!-- &=\frac{1}{\left[\sum(x_i-\bar x)^2\right]^2}\biggl[\sum(x_i-\bar x)^2\mathrm{var}(Y_i\mid \mathbf{X}) & \\ -->
<!-- &\qquad +\sum_{i=1}^{n}\sum_{i\neq j}(x_i-\bar x)(x_j-\bar x)\text{cov}(Y_i,Y_j\mid \mathbf{X})\biggr]&\tiny\text{(factor out constants)}\\ -->
<!-- &=\frac{1}{\left[\sum(x_i-\bar x)^2\right]^2}\left[\sum(x_i-\bar x)^2\mathrm{var}(Y_i\mid \mathbf{X})\right]&\tiny\text{(simplify using }\text{cov}(Y_i, Y_j\mid \mathbf{X})=0 \text{ for } i\neq j)\\ -->
<!-- &=\frac{1}{\left[\sum(x_i-\bar x)^2\right]^2}\left[\sigma^2\sum(x_i-\bar x)^2\right]&\tiny\text{(substitute known variance, factor out }\sigma^2\text{)}\\ -->
<!-- &=\frac{\sigma^2}{\sum(x_i-\bar x)^2}.&\tiny\text{(cancel out numerator and denominator)}\ -->
<!-- \end{align} -->
<!-- \] -->
</div>
<div id="manual-calculation-penguins-simple-linear-regression-example" class="section level3 hasAnchor" number="3.12.4">
<h3><span class="header-section-number">3.12.4</span> Manual calculation Penguins simple linear regression example<a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we manually produce (i.e., without the <code>lm</code> function) the <code>penguins</code> simple linear regression example in Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>.</p>
<p>First, we will manually fit a simple linear regression model that regresses <code>bill_length_mm</code> on <code>body_mass_g</code>.</p>
<p>Using the <code>summary</code> function on the <code>penguins</code> data frame, we see that both <code>bill_length_mm</code> and <code>body_mass_g</code> have <code>NA</code> values.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="linear-model-estimation.html#cb114-1" tabindex="-1"></a><span class="fu">summary</span>(penguins)</span>
<span id="cb114-2"><a href="linear-model-estimation.html#cb114-2" tabindex="-1"></a><span class="do">##       species          island    bill_length_mm </span></span>
<span id="cb114-3"><a href="linear-model-estimation.html#cb114-3" tabindex="-1"></a><span class="do">##  Adelie   :152   Biscoe   :168   Min.   :32.10  </span></span>
<span id="cb114-4"><a href="linear-model-estimation.html#cb114-4" tabindex="-1"></a><span class="do">##  Chinstrap: 68   Dream    :124   1st Qu.:39.23  </span></span>
<span id="cb114-5"><a href="linear-model-estimation.html#cb114-5" tabindex="-1"></a><span class="do">##  Gentoo   :124   Torgersen: 52   Median :44.45  </span></span>
<span id="cb114-6"><a href="linear-model-estimation.html#cb114-6" tabindex="-1"></a><span class="do">##                                  Mean   :43.92  </span></span>
<span id="cb114-7"><a href="linear-model-estimation.html#cb114-7" tabindex="-1"></a><span class="do">##                                  3rd Qu.:48.50  </span></span>
<span id="cb114-8"><a href="linear-model-estimation.html#cb114-8" tabindex="-1"></a><span class="do">##                                  Max.   :59.60  </span></span>
<span id="cb114-9"><a href="linear-model-estimation.html#cb114-9" tabindex="-1"></a><span class="do">##                                  NA&#39;s   :2      </span></span>
<span id="cb114-10"><a href="linear-model-estimation.html#cb114-10" tabindex="-1"></a><span class="do">##  bill_depth_mm   flipper_length_mm  body_mass_g  </span></span>
<span id="cb114-11"><a href="linear-model-estimation.html#cb114-11" tabindex="-1"></a><span class="do">##  Min.   :13.10   Min.   :172.0     Min.   :2700  </span></span>
<span id="cb114-12"><a href="linear-model-estimation.html#cb114-12" tabindex="-1"></a><span class="do">##  1st Qu.:15.60   1st Qu.:190.0     1st Qu.:3550  </span></span>
<span id="cb114-13"><a href="linear-model-estimation.html#cb114-13" tabindex="-1"></a><span class="do">##  Median :17.30   Median :197.0     Median :4050  </span></span>
<span id="cb114-14"><a href="linear-model-estimation.html#cb114-14" tabindex="-1"></a><span class="do">##  Mean   :17.15   Mean   :200.9     Mean   :4202  </span></span>
<span id="cb114-15"><a href="linear-model-estimation.html#cb114-15" tabindex="-1"></a><span class="do">##  3rd Qu.:18.70   3rd Qu.:213.0     3rd Qu.:4750  </span></span>
<span id="cb114-16"><a href="linear-model-estimation.html#cb114-16" tabindex="-1"></a><span class="do">##  Max.   :21.50   Max.   :231.0     Max.   :6300  </span></span>
<span id="cb114-17"><a href="linear-model-estimation.html#cb114-17" tabindex="-1"></a><span class="do">##  NA&#39;s   :2       NA&#39;s   :2         NA&#39;s   :2     </span></span>
<span id="cb114-18"><a href="linear-model-estimation.html#cb114-18" tabindex="-1"></a><span class="do">##      sex           year        pl_fitted    </span></span>
<span id="cb114-19"><a href="linear-model-estimation.html#cb114-19" tabindex="-1"></a><span class="do">##  female:165   Min.   :2007   Min.   :35.60  </span></span>
<span id="cb114-20"><a href="linear-model-estimation.html#cb114-20" tabindex="-1"></a><span class="do">##  male  :168   1st Qu.:2007   1st Qu.:38.98  </span></span>
<span id="cb114-21"><a href="linear-model-estimation.html#cb114-21" tabindex="-1"></a><span class="do">##  NA&#39;s  : 11   Median :2008   Median :45.67  </span></span>
<span id="cb114-22"><a href="linear-model-estimation.html#cb114-22" tabindex="-1"></a><span class="do">##               Mean   :2008   Mean   :43.92  </span></span>
<span id="cb114-23"><a href="linear-model-estimation.html#cb114-23" tabindex="-1"></a><span class="do">##               3rd Qu.:2009   3rd Qu.:48.34  </span></span>
<span id="cb114-24"><a href="linear-model-estimation.html#cb114-24" tabindex="-1"></a><span class="do">##               Max.   :2009   Max.   :52.83  </span></span>
<span id="cb114-25"><a href="linear-model-estimation.html#cb114-25" tabindex="-1"></a><span class="do">##                              NA&#39;s   :2      </span></span>
<span id="cb114-26"><a href="linear-model-estimation.html#cb114-26" tabindex="-1"></a><span class="do">##    sl_fitted    </span></span>
<span id="cb114-27"><a href="linear-model-estimation.html#cb114-27" tabindex="-1"></a><span class="do">##  Min.   :36.08  </span></span>
<span id="cb114-28"><a href="linear-model-estimation.html#cb114-28" tabindex="-1"></a><span class="do">##  1st Qu.:38.95  </span></span>
<span id="cb114-29"><a href="linear-model-estimation.html#cb114-29" tabindex="-1"></a><span class="do">##  Median :45.40  </span></span>
<span id="cb114-30"><a href="linear-model-estimation.html#cb114-30" tabindex="-1"></a><span class="do">##  Mean   :43.92  </span></span>
<span id="cb114-31"><a href="linear-model-estimation.html#cb114-31" tabindex="-1"></a><span class="do">##  3rd Qu.:48.42  </span></span>
<span id="cb114-32"><a href="linear-model-estimation.html#cb114-32" tabindex="-1"></a><span class="do">##  Max.   :53.60  </span></span>
<span id="cb114-33"><a href="linear-model-estimation.html#cb114-33" tabindex="-1"></a><span class="do">##  NA&#39;s   :2</span></span></code></pre></div>
<p>This is important to note because the <code>lm</code> function automatically removes any observation with <code>NA</code> values for any of the variables specified in the <code>formula</code> argument. In order to replicate our results, we must remove the same observations.</p>
<p>We want to remove the rows of <code>penguins</code> where either <code>body_mass_g</code> or <code>bill_length_mm</code> have <code>NA</code> values. We do that below using the <code>na.omit</code> function (selecting only the relevant variables) and assign the cleaned
object the name <code>penguins_clean</code>.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="linear-model-estimation.html#cb115-1" tabindex="-1"></a><span class="co"># remove rows of penguins where bill_length_mm or body_mass_g have NA values</span></span>
<span id="cb115-2"><a href="linear-model-estimation.html#cb115-2" tabindex="-1"></a>penguins_clean <span class="ot">&lt;-</span></span>
<span id="cb115-3"><a href="linear-model-estimation.html#cb115-3" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb115-4"><a href="linear-model-estimation.html#cb115-4" tabindex="-1"></a>  <span class="fu">subset</span>(<span class="at">select =</span> <span class="fu">c</span>(<span class="st">&quot;bill_length_mm&quot;</span>, <span class="st">&quot;body_mass_g&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb115-5"><a href="linear-model-estimation.html#cb115-5" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span></code></pre></div>
<p>We extract the <code>bill_length_mm</code> variable from the <code>penguins</code> data frame and assign it the name <code>y</code> since it will be the response variable. We extract the <code>body_mass_g</code> variable from the <code>penguins</code> data frame and
assign it the name <code>x</code> since it will be the regressor variable. We also determine the number of observations and assign that value the name <code>n</code>.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="linear-model-estimation.html#cb116-1" tabindex="-1"></a><span class="co"># extract response and regressor from penguins_clean</span></span>
<span id="cb116-2"><a href="linear-model-estimation.html#cb116-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>bill_length_mm</span>
<span id="cb116-3"><a href="linear-model-estimation.html#cb116-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>body_mass_g</span>
<span id="cb116-4"><a href="linear-model-estimation.html#cb116-4" tabindex="-1"></a><span class="co"># determine number of observations</span></span>
<span id="cb116-5"><a href="linear-model-estimation.html#cb116-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span></code></pre></div>
<p>We now compute <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> using Equations <a href="linear-model-estimation.html#eq:slr-beta1hat">(3.9)</a> and <a href="linear-model-estimation.html#eq:slr-beta0hat">(3.10)</a>. Note that placing <code>()</code> around the assignment operations will both perform the assignment and print the results.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="linear-model-estimation.html#cb117-1" tabindex="-1"></a><span class="co"># compute OLS estimate of beta1</span></span>
<span id="cb117-2"><a href="linear-model-estimation.html#cb117-2" tabindex="-1"></a>(b1 <span class="ot">&lt;-</span> (<span class="fu">sum</span>(x <span class="sc">*</span> y) <span class="sc">-</span> <span class="fu">sum</span>(x) <span class="sc">*</span> <span class="fu">sum</span>(y) <span class="sc">/</span> n)<span class="sc">/</span>(<span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">sum</span>(x)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n))</span>
<span id="cb117-3"><a href="linear-model-estimation.html#cb117-3" tabindex="-1"></a><span class="do">## [1] 0.004051417</span></span>
<span id="cb117-4"><a href="linear-model-estimation.html#cb117-4" tabindex="-1"></a><span class="co"># compute OLS estimate of beta0</span></span>
<span id="cb117-5"><a href="linear-model-estimation.html#cb117-5" tabindex="-1"></a>(b0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> b1 <span class="sc">*</span> <span class="fu">mean</span>(x))</span>
<span id="cb117-6"><a href="linear-model-estimation.html#cb117-6" tabindex="-1"></a><span class="do">## [1] 26.89887</span></span></code></pre></div>
<p>The estimated value of <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(\hat{\beta}_0=26.90\)</span> and the estimated value of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat{\beta}_1=0.004\)</span>.</p>
<p>We can also compute the residuals, the fitted values, the RSS, and the estimated error variance. Using the code below, the RSS for our model is 6564.49 and the estimated error variance if <span class="math inline">\(\hat{\sigma}^2=19.31\)</span>.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="linear-model-estimation.html#cb118-1" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="co"># compute fitted values</span></span>
<span id="cb118-2"><a href="linear-model-estimation.html#cb118-2" tabindex="-1"></a>ehat <span class="ot">&lt;-</span> y <span class="sc">-</span> yhat <span class="co"># compute residuals</span></span>
<span id="cb118-3"><a href="linear-model-estimation.html#cb118-3" tabindex="-1"></a>(rss <span class="ot">&lt;-</span> <span class="fu">sum</span>(ehat<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># sum of the squared residuals</span></span>
<span id="cb118-4"><a href="linear-model-estimation.html#cb118-4" tabindex="-1"></a><span class="do">## [1] 6564.494</span></span>
<span id="cb118-5"><a href="linear-model-estimation.html#cb118-5" tabindex="-1"></a>(sigmasqhat <span class="ot">&lt;-</span> rss<span class="sc">/</span>(n<span class="dv">-2</span>)) <span class="co"># estimated error variance</span></span>
<span id="cb118-6"><a href="linear-model-estimation.html#cb118-6" tabindex="-1"></a><span class="do">## [1] 19.30734</span></span></code></pre></div>
</div>
<div id="mlr-derivation" class="section level3 hasAnchor" number="3.12.5">
<h3><span class="header-section-number">3.12.5</span> Derivation of the OLS estimator for the multiple linear regression model coefficients<a href="linear-model-estimation.html#mlr-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to determine the value of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that will minimize
<span class="math display">\[
\begin{aligned}
RSS(\hat{\boldsymbol{\beta}}) &amp;=\sum_{i=1}^n \hat{\epsilon_i}^2 \\
&amp;= \hat{\boldsymbol{\epsilon}}^T\hat{\boldsymbol{\epsilon}} \\
&amp;= (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
&amp;= \mathbf{y}^T\mathbf{y}-2\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}+\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}},
\end{aligned}
\]</span>
where the second term in the last line comes from the fact that <span class="math inline">\(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}\)</span> is a <span class="math inline">\(1\times 1\)</span> matrix, and is thus symmetric. Consequently, <span class="math inline">\(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}=(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y})^T=\mathbf{y}^T\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>To find the local extrema of <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, we set its derivative with respect to <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> equal to 0, and solve for <span class="math inline">\(\hat{\boldsymbol{\beta}})\)</span>.</p>
<p>Using the results in Appendix <a href="overview-of-matrix-facts.html#matrix-derivatives">A.5</a>,
we see that
<span class="math display">\[
\frac{\partial RSS(\hat{\boldsymbol{\beta}})}{\partial\hat{\boldsymbol{\beta}}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}.
\]</span>
Setting <span class="math inline">\(\partial RSS(\hat{\boldsymbol{\beta}})/\partial\hat{\boldsymbol{\beta}}=0\)</span> and using some simple algebra, we derive the <strong>normal equations</strong>
<span class="math display">\[\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}^T\mathbf{y}.\]</span>
Assuming the <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is invertible, which it will be when <span class="math inline">\(\mathbf{X}\)</span> is full-rank, our solution is
<span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>To show that the OLS estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> minimizes <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, we technically need to show that the Hessian matrix of <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, the matrix of second-order partial derivatives, is positive definite. In our context, the Hessian matrix is
<span class="math display">\[
\begin{aligned}
\frac{\partial^2 RSS(\hat{\boldsymbol{\beta}})}{\partial \hat{\boldsymbol{\beta}}^2} &amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}(-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}) \\
&amp;= 2\mathbf{X}^T\mathbf{X}.
\end{aligned}
\]</span>
The <span class="math inline">\(p\times p\)</span> matrix <span class="math inline">\(2\mathbf{X}^T\mathbf{X}\)</span> is positive definite, but it is beyond the scope of the course to prove this.</p>
<p>Therefore, the OLS estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>,
<span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span>
minimizes the RSS.</p>
</div>
<div id="manual-calculation-of-penguins-multiple-linear-regression-example" class="section level3 hasAnchor" number="3.12.6">
<h3><span class="header-section-number">3.12.6</span> Manual calculation of Penguins multiple linear regression example<a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We manually verify the calculations for the <code>penguins</code> example given in Section @ref{s:penguins-mlr}, where we fit the multiple linear regression model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>. We refit the model below, specifying the argument <code>y = TRUE</code> so we can get the response vector used in the model.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="linear-model-estimation.html#cb119-1" tabindex="-1"></a><span class="co"># fit regression model, retaining the y vector</span></span>
<span id="cb119-2"><a href="linear-model-estimation.html#cb119-2" tabindex="-1"></a>mlmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> flipper_length_mm,</span>
<span id="cb119-3"><a href="linear-model-estimation.html#cb119-3" tabindex="-1"></a>            <span class="at">data =</span> penguins, <span class="at">y =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>We can use <code>model.matrix</code> to extract the <span class="math inline">\(\mathbf{X}\)</span> matrix from our fitted model. And because we specified <code>y = TRUE</code> in our call to <code>lm</code>, we can also extract <code>y</code> from the fitted model using the code below.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="linear-model-estimation.html#cb120-1" tabindex="-1"></a><span class="co"># extract X matrix from fitted model</span></span>
<span id="cb120-2"><a href="linear-model-estimation.html#cb120-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(mlmod)</span>
<span id="cb120-3"><a href="linear-model-estimation.html#cb120-3" tabindex="-1"></a><span class="co"># extract y vector from fitted model</span></span>
<span id="cb120-4"><a href="linear-model-estimation.html#cb120-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> mlmod<span class="sc">$</span>y</span></code></pre></div>
<p>We’ll need to learn a few new commands in R to do the calculations:</p>
<ul>
<li><code>t</code> is the transpose of a matrix.</li>
<li><code>%*%</code> is the multiplication operator for two matrices.</li>
<li><code>solve(A, b)</code> computes <span class="math inline">\(\mathbf{A}^{-1}\mathbf{b}\)</span>.</li>
</ul>
<p>Thus, we compute <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using the code below, which matches the estimate from the <code>lm</code> function.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="linear-model-estimation.html#cb121-1" tabindex="-1"></a><span class="co"># manually calculate betahat</span></span>
<span id="cb121-2"><a href="linear-model-estimation.html#cb121-2" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb121-3"><a href="linear-model-estimation.html#cb121-3" tabindex="-1"></a><span class="do">##                            [,1]</span></span>
<span id="cb121-4"><a href="linear-model-estimation.html#cb121-4" tabindex="-1"></a><span class="do">## (Intercept)       -3.4366939266</span></span>
<span id="cb121-5"><a href="linear-model-estimation.html#cb121-5" tabindex="-1"></a><span class="do">## body_mass_g        0.0006622186</span></span>
<span id="cb121-6"><a href="linear-model-estimation.html#cb121-6" tabindex="-1"></a><span class="do">## flipper_length_mm  0.2218654584</span></span>
<span id="cb121-7"><a href="linear-model-estimation.html#cb121-7" tabindex="-1"></a><span class="co"># betahat from lm function</span></span>
<span id="cb121-8"><a href="linear-model-estimation.html#cb121-8" tabindex="-1"></a><span class="fu">coef</span>(mlmod)</span>
<span id="cb121-9"><a href="linear-model-estimation.html#cb121-9" tabindex="-1"></a><span class="do">##       (Intercept)       body_mass_g flipper_length_mm </span></span>
<span id="cb121-10"><a href="linear-model-estimation.html#cb121-10" tabindex="-1"></a><span class="do">##     -3.4366939266      0.0006622186      0.2218654584</span></span></code></pre></div>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-anscombe1973graphs" class="csl-entry">
Anscombe, Francis J. 1973. <span>“Graphs in Statistical Analysis.”</span> <em>The American Statistician</em> 27 (1): 17–21.
</div>
<div id="ref-ezekiel1930methods" class="csl-entry">
Ezekiel, Mordecai. 1930. <span>“Methods of Correlation Analysis.”</span>
</div>
<div id="ref-GormanEtAl2014" class="csl-entry">
Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. <span>“Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).”</span> <em>PLOS ONE</em> 9 (3): 1–14. <a href="https://doi.org/10.1371/journal.pone.0090081">https://doi.org/10.1371/journal.pone.0090081</a>.
</div>
<div id="ref-R-palmerpenguins" class="csl-entry">
Horst, Allison, Alison Hill, and Kristen Gorman. 2022. <em>Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data</em>. <a href="https://allisonhorst.github.io/palmerpenguins/">https://allisonhorst.github.io/palmerpenguins/</a>.
</div>
<div id="ref-alr4" class="csl-entry">
Weisberg, Sanford. 2014. <em>Applied Linear Regression</em>. Fourth. Hoboken <span>NJ</span>: Wiley. <a href="http://z.umn.edu/alr4ed">http://z.umn.edu/alr4ed</a>.
</div>
<div id="ref-wilkinsonrogers1973" class="csl-entry">
Wilkinson, GN, and CE Rogers. 1973. <span>“Symbolic Description of Factorial Models for Analysis of Variance.”</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 22 (3): 392–99.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-cleaning-and-exploration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interp-chapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["A-Progessive-Introduction-to-Linear-Models.pdf", "A-Progessive-Introduction-to-Linear-Models.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
