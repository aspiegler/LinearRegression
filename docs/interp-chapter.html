<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Interpreting a fitted linear model | A Progressive Introduction to Linear Models</title>
  <meta name="description" content="A collection of material that progressively introduces how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Interpreting a fitted linear model | A Progressive Introduction to Linear Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of material that progressively introduces how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Interpreting a fitted linear model | A Progressive Introduction to Linear Models" />
  
  <meta name="twitter:description" content="A collection of material that progressively introduces how to fit and use linear models." />
  

<meta name="author" content="Joshua French" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-model-estimation.html"/>
<link rel="next" href="linear-model-theory.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Progressive Introduction to Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminaries</a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#setting-up-r-and-rstudio-desktop"><i class="fa fa-check"></i><b>1.1</b> Setting up R and RStudio Desktop</a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.2</b> Running code, scripts, and comments</a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.3</b> Assignment</a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#functions"><i class="fa fa-check"></i><b>1.4</b> Functions</a></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages</a></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help</a></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types</a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types</a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.8</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.8.1</b> Creation</a></li>
<li class="chapter" data-level="1.8.2" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.8.2</b> Categorical vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.8.3</b> Extracting parts of a vector</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.9</b> Helpful functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.9.1</b> General functions</a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.9.2</b> Functions related to statistical distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.10</b> Data Frames</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#direct-creation"><i class="fa fa-check"></i><b>1.10.1</b> Direct creation</a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.10.2</b> Importing Data</a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.10.3</b> Extracting parts of a data frame</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#using-the-pipe-operator"><i class="fa fa-check"></i><b>1.11</b> Using the pipe operator</a></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#dealing-with-common-problems"><i class="fa fa-check"></i><b>1.12</b> Dealing with common problems</a></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.13</b> Ecosystem debate</a></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#additional-information"><i class="fa fa-check"></i><b>1.14</b> Additional information</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="r-foundations.html"><a href="r-foundations.html#comparing-assignment-operators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing assignment operators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html"><i class="fa fa-check"></i><b>2</b> Data cleaning and exploration</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#raw-palmer-penguins-data"><i class="fa fa-check"></i><b>2.1</b> Raw Palmer penguins data</a></li>
<li class="chapter" data-level="2.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#initial-data-cleaning"><i class="fa fa-check"></i><b>2.2</b> Initial data cleaning</a></li>
<li class="chapter" data-level="2.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numerical-summarization-of-data"><i class="fa fa-check"></i><b>2.3</b> Numerical summarization of data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numeric-data"><i class="fa fa-check"></i><b>2.3.1</b> Numeric data</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#categorical-data"><i class="fa fa-check"></i><b>2.3.2</b> Categorical data</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-summary-function"><i class="fa fa-check"></i><b>2.3.3</b> The <code>summary</code> function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.4</b> Visual summaries of data</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-ggplot-recipe"><i class="fa fa-check"></i><b>2.4.1</b> The ggplot recipe</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#univariate-plots"><i class="fa fa-check"></i><b>2.4.2</b> Univariate plots</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#bivariate-plots"><i class="fa fa-check"></i><b>2.4.3</b> Bivariate plots</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#multivariate-plots"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate plots</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#a-plan-for-data-cleaning-and-exploration"><i class="fa fa-check"></i><b>2.5</b> A plan for data cleaning and exploration</a></li>
<li class="chapter" data-level="2.6" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#final-notes-on-missing-or-erroneous-data"><i class="fa fa-check"></i><b>2.6</b> Final notes on missing or erroneous data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>3</b> Linear model estimation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>3.1</b> A simple motivating example</a></li>
<li class="chapter" data-level="3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s-slr-estimation"><i class="fa fa-check"></i><b>3.2</b> Estimation of the simple linear regression model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>3.2.1</b> Model definition, fitted values, residuals, and RSS</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>3.2.2</b> OLS estimators of the simple linear regression parameters</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>3.3</b> Penguins simple linear regression example</a></li>
<li class="chapter" data-level="3.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>3.4</b> Defining a linear model</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>3.4.1</b> Necessary components and notation</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>3.4.2</b> Standard definition of linear model</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5</b> Estimation of the multiple linear regression model</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>3.5.1</b> Using matrix notation to represent a linear model</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>3.5.2</b> Residuals, fitted values, and RSS for multiple linear regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.5.3</b> OLS estimator of the regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr"><i class="fa fa-check"></i><b>3.6</b> Penguins multiple linear regression example</a></li>
<li class="chapter" data-level="3.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#model-types"><i class="fa fa-check"></i><b>3.7</b> Types of linear models</a></li>
<li class="chapter" data-level="3.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#categorical-predictors"><i class="fa fa-check"></i><b>3.8</b> Categorical predictors</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#indicator-variables"><i class="fa fa-check"></i><b>3.8.1</b> Indicator variables</a></li>
<li class="chapter" data-level="3.8.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#parallel-and-separate-lines-models"><i class="fa fa-check"></i><b>3.8.2</b> Parallel and separate lines models</a></li>
<li class="chapter" data-level="3.8.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#extensions"><i class="fa fa-check"></i><b>3.8.3</b> Extensions</a></li>
<li class="chapter" data-level="3.8.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#avoiding-an-easy-mistake"><i class="fa fa-check"></i><b>3.8.4</b> Avoiding an easy mistake</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr2"><i class="fa fa-check"></i><b>3.9</b> Penguins example with categorical predictor</a></li>
<li class="chapter" data-level="3.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#evaluating-model-fit"><i class="fa fa-check"></i><b>3.10</b> Evaluating model fit</a></li>
<li class="chapter" data-level="3.11" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary"><i class="fa fa-check"></i><b>3.11</b> Summary</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:term-summary"><i class="fa fa-check"></i><b>3.11.1</b> Summary of terms</a></li>
<li class="chapter" data-level="3.11.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-functions"><i class="fa fa-check"></i><b>3.11.2</b> Summary of functions</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#going-deeper"><i class="fa fa-check"></i><b>3.12</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#degrees-of-freedom"><i class="fa fa-check"></i><b>3.12.1</b> Degrees of freedom</a></li>
<li class="chapter" data-level="3.12.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#slr-derivation"><i class="fa fa-check"></i><b>3.12.2</b> Derivation of the OLS estimators of the simple linear regression model coefficients</a></li>
<li class="chapter" data-level="3.12.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#unbiasedness-of-ols-estimators"><i class="fa fa-check"></i><b>3.12.3</b> Unbiasedness of OLS estimators</a></li>
<li class="chapter" data-level="3.12.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>3.12.4</b> Manual calculation Penguins simple linear regression example</a></li>
<li class="chapter" data-level="3.12.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#mlr-derivation"><i class="fa fa-check"></i><b>3.12.5</b> Derivation of the OLS estimator for the multiple linear regression model coefficients</a></li>
<li class="chapter" data-level="3.12.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example"><i class="fa fa-check"></i><b>3.12.6</b> Manual calculation of Penguins multiple linear regression example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interp-chapter.html"><a href="interp-chapter.html"><i class="fa fa-check"></i><b>4</b> Interpreting a fitted linear model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="interp-chapter.html"><a href="interp-chapter.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>4.1</b> Interpretation of coefficients</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="interp-chapter.html"><a href="interp-chapter.html#interpretation-for-simple-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation for simple linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="interp-chapter.html"><a href="interp-chapter.html#interp-1st-order-ml"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation for first-order multiple linear regression models</a></li>
<li class="chapter" data-level="4.1.3" data-path="interp-chapter.html"><a href="interp-chapter.html#regressor-roles"><i class="fa fa-check"></i><b>4.1.3</b> Roles of regressor variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots"><i class="fa fa-check"></i><b>4.2</b> Effect plots</a></li>
<li class="chapter" data-level="4.3" data-path="interp-chapter.html"><a href="interp-chapter.html#interp-cat-predictor"><i class="fa fa-check"></i><b>4.3</b> Interpretation for categorical predictors</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="interp-chapter.html"><a href="interp-chapter.html#pl-interp"><i class="fa fa-check"></i><b>4.3.1</b> Coefficient interpretation for parallel lines models</a></li>
<li class="chapter" data-level="4.3.2" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots-for-fitted-models-with-non-interacting-categorical-predictors"><i class="fa fa-check"></i><b>4.3.2</b> Effect plots for fitted models with non-interacting categorical predictors</a></li>
<li class="chapter" data-level="4.3.3" data-path="interp-chapter.html"><a href="interp-chapter.html#sl-interp"><i class="fa fa-check"></i><b>4.3.3</b> Coefficient interpretation for separate lines models</a></li>
<li class="chapter" data-level="4.3.4" data-path="interp-chapter.html"><a href="interp-chapter.html#effect-plots-for-interacting-categorical-predictors"><i class="fa fa-check"></i><b>4.3.4</b> Effect plots for interacting categorical predictors</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="interp-chapter.html"><a href="interp-chapter.html#added-variable-and-leverage-plots"><i class="fa fa-check"></i><b>4.4</b> Added-variable and leverage plots</a></li>
<li class="chapter" data-level="4.5" data-path="interp-chapter.html"><a href="interp-chapter.html#going-deeper-1"><i class="fa fa-check"></i><b>4.5</b> Going deeper</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="interp-chapter.html"><a href="interp-chapter.html#orthogonality"><i class="fa fa-check"></i><b>4.5.1</b> Orthogonality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-model-theory.html"><a href="linear-model-theory.html"><i class="fa fa-check"></i><b>5</b> Basic theoretical results for linear models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-model-theory.html"><a href="linear-model-theory.html#standard-assumptions"><i class="fa fa-check"></i><b>5.1</b> Standard assumptions</a></li>
<li class="chapter" data-level="5.2" data-path="linear-model-theory.html"><a href="linear-model-theory.html#summary-of-results"><i class="fa fa-check"></i><b>5.2</b> Summary of results</a></li>
<li class="chapter" data-level="5.3" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-mathbfy"><i class="fa fa-check"></i><b>5.3</b> Results for <span class="math inline">\(\mathbf{y}\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-hatboldsymbolbeta"><i class="fa fa-check"></i><b>5.4</b> Results for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="linear-model-theory.html"><a href="linear-model-theory.html#results-for-the-residuals"><i class="fa fa-check"></i><b>5.5</b> Results for the residuals</a></li>
<li class="chapter" data-level="5.6" data-path="linear-model-theory.html"><a href="linear-model-theory.html#the-gauss-markov-theorem"><i class="fa fa-check"></i><b>5.6</b> The Gauss-Markov Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6</b> Linear model inference and prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html#overview-of-inference-and-prediction"><i class="fa fa-check"></i><b>6.1</b> Overview of inference and prediction</a></li>
<li class="chapter" data-level="6.2" data-path="inference.html"><a href="inference.html#some-relevant-distributions"><i class="fa fa-check"></i><b>6.2</b> Some relevant distributions</a></li>
<li class="chapter" data-level="6.3" data-path="inference.html"><a href="inference.html#properties-betahat"><i class="fa fa-check"></i><b>6.3</b> Assumptions and properties of the OLS estimator</a></li>
<li class="chapter" data-level="6.4" data-path="inference.html"><a href="inference.html#parametric-confidence-intervals-for-regression-coefficients"><i class="fa fa-check"></i><b>6.4</b> Parametric confidence intervals for regression coefficients</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inference.html"><a href="inference.html#tci"><i class="fa fa-check"></i><b>6.4.1</b> Standard <span class="math inline">\(t\)</span>-based confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="inference.html"><a href="inference.html#mcp"><i class="fa fa-check"></i><b>6.5</b> The multiple comparisons problem</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="inference.html"><a href="inference.html#adjusted-cis-betas"><i class="fa fa-check"></i><b>6.5.1</b> Adjusted confidence intervals for regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference.html"><a href="inference.html#prediction-mean-response-versus-new-response"><i class="fa fa-check"></i><b>6.6</b> Prediction: mean response versus new response</a></li>
<li class="chapter" data-level="6.7" data-path="inference.html"><a href="inference.html#parametric-ci-mean-response"><i class="fa fa-check"></i><b>6.7</b> Confidence interval for the mean response</a></li>
<li class="chapter" data-level="6.8" data-path="inference.html"><a href="inference.html#pi-new-response"><i class="fa fa-check"></i><b>6.8</b> Prediction interval for a new response</a></li>
<li class="chapter" data-level="6.9" data-path="inference.html"><a href="inference.html#hypothesis-tests-for-a-single-regression-coefficient"><i class="fa fa-check"></i><b>6.9</b> Hypothesis tests for a single regression coefficient</a></li>
<li class="chapter" data-level="6.10" data-path="inference.html"><a href="inference.html#hypothesis-tests-for-multiple-regression-coefficients"><i class="fa fa-check"></i><b>6.10</b> Hypothesis tests for multiple regression coefficients</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="inference.html"><a href="inference.html#test-for-a-regression-relationship"><i class="fa fa-check"></i><b>6.10.1</b> Test for a regression relationship</a></li>
<li class="chapter" data-level="6.10.2" data-path="inference.html"><a href="inference.html#a-more-general-f-test"><i class="fa fa-check"></i><b>6.10.2</b> A more general F test</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="inference.html"><a href="inference.html#going-deeper-2"><i class="fa fa-check"></i><b>6.11</b> Going deeper</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="inference.html"><a href="inference.html#manual-t-cis"><i class="fa fa-check"></i><b>6.11.1</b> Manual calculation of the standard <span class="math inline">\(t\)</span>-based confidence interval for a regression coefficient</a></li>
<li class="chapter" data-level="6.11.2" data-path="inference.html"><a href="inference.html#mean-response-calculations"><i class="fa fa-check"></i><b>6.11.2</b> Details about estimation of the mean response</a></li>
<li class="chapter" data-level="6.11.3" data-path="inference.html"><a href="inference.html#manual-calc-ci-mean-response"><i class="fa fa-check"></i><b>6.11.3</b> Manual calculation of confidence intervals for the mean response</a></li>
<li class="chapter" data-level="6.11.4" data-path="inference.html"><a href="inference.html#new-response-pi-calculations"><i class="fa fa-check"></i><b>6.11.4</b> Details about prediction interval for a new response</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html"><i class="fa fa-check"></i><b>A</b> Overview of matrix facts</a>
<ul>
<li class="chapter" data-level="A.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#notation"><i class="fa fa-check"></i><b>A.1</b> Notation</a></li>
<li class="chapter" data-level="A.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>A.2</b> Basic mathematical operations</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>A.2.1</b> Addition and subtraction</a></li>
<li class="chapter" data-level="A.2.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>A.2.2</b> Scalar multiplication</a></li>
<li class="chapter" data-level="A.2.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>A.2.3</b> Matrix multiplication</a></li>
<li class="chapter" data-level="A.2.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose"><i class="fa fa-check"></i><b>A.2.4</b> Transpose</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>A.3</b> Basic mathematical properties</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>A.3.1</b> Associative property</a></li>
<li class="chapter" data-level="A.3.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>A.3.2</b> Distributive property</a></li>
<li class="chapter" data-level="A.3.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>A.3.3</b> No commutative property</a></li>
<li class="chapter" data-level="A.3.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose-related-properties"><i class="fa fa-check"></i><b>A.3.4</b> Transpose-related properties</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>A.4</b> Special matrices</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>A.4.1</b> Square matrices</a></li>
<li class="chapter" data-level="A.4.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>A.4.2</b> Identity matrix</a></li>
<li class="chapter" data-level="A.4.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#diagonal-matrices"><i class="fa fa-check"></i><b>A.4.3</b> Diagonal matrices</a></li>
<li class="chapter" data-level="A.4.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#symmetric-matrices"><i class="fa fa-check"></i><b>A.4.4</b> Symmetric matrices</a></li>
<li class="chapter" data-level="A.4.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#idempotent-matrices"><i class="fa fa-check"></i><b>A.4.5</b> Idempotent matrices</a></li>
<li class="chapter" data-level="A.4.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.4.6</b> Positive definite matrices</a></li>
<li class="chapter" data-level="A.4.7" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#inverse-matrix"><i class="fa fa-check"></i><b>A.4.7</b> Inverse matrix</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>A.5</b> Matrix derivatives</a></li>
<li class="chapter" data-level="A.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#additional-topics"><i class="fa fa-check"></i><b>A.6</b> Additional topics</a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#determinant"><i class="fa fa-check"></i><b>A.6.1</b> Determinant</a></li>
<li class="chapter" data-level="A.6.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#linearly-independent-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Linearly independent vectors</a></li>
<li class="chapter" data-level="A.6.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#rank"><i class="fa fa-check"></i><b>A.6.3</b> Rank</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="prob-review.html"><a href="prob-review.html"><i class="fa fa-check"></i><b>B</b> Overview of probability, random variables, and random vectors</a>
<ul>
<li class="chapter" data-level="B.1" data-path="prob-review.html"><a href="prob-review.html#probability-basics"><i class="fa fa-check"></i><b>B.1</b> Probability Basics</a></li>
<li class="chapter" data-level="B.2" data-path="prob-review.html"><a href="prob-review.html#random-variables"><i class="fa fa-check"></i><b>B.2</b> Random Variables</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="prob-review.html"><a href="prob-review.html#discrete-random-variables"><i class="fa fa-check"></i><b>B.2.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="B.2.2" data-path="prob-review.html"><a href="prob-review.html#continuous-random-variables"><i class="fa fa-check"></i><b>B.2.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="B.2.3" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-random-variables"><i class="fa fa-check"></i><b>B.2.3</b> Useful facts for transformations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="prob-review.html"><a href="prob-review.html#multivariate-distributions"><i class="fa fa-check"></i><b>B.3</b> Multivariate distributions</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="prob-review.html"><a href="prob-review.html#basic-properties"><i class="fa fa-check"></i><b>B.3.1</b> Basic properties</a></li>
<li class="chapter" data-level="B.3.2" data-path="prob-review.html"><a href="prob-review.html#marginal-distributions"><i class="fa fa-check"></i><b>B.3.2</b> Marginal distributions</a></li>
<li class="chapter" data-level="B.3.3" data-path="prob-review.html"><a href="prob-review.html#independence-of-random-variables"><i class="fa fa-check"></i><b>B.3.3</b> Independence of random variables</a></li>
<li class="chapter" data-level="B.3.4" data-path="prob-review.html"><a href="prob-review.html#conditional-distributions"><i class="fa fa-check"></i><b>B.3.4</b> Conditional distributions</a></li>
<li class="chapter" data-level="B.3.5" data-path="prob-review.html"><a href="prob-review.html#covariance"><i class="fa fa-check"></i><b>B.3.5</b> Covariance</a></li>
<li class="chapter" data-level="B.3.6" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>B.3.6</b> Useful facts for transformations of multiple random variables</a></li>
<li class="chapter" data-level="B.3.7" data-path="prob-review.html"><a href="prob-review.html#example-binomial"><i class="fa fa-check"></i><b>B.3.7</b> Example (Binomial)</a></li>
<li class="chapter" data-level="B.3.8" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example"><i class="fa fa-check"></i><b>B.3.8</b> Example (Continuous bivariate distribution)</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="prob-review.html"><a href="prob-review.html#random-vectors"><i class="fa fa-check"></i><b>B.4</b> Random vectors</a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="prob-review.html"><a href="prob-review.html#definition"><i class="fa fa-check"></i><b>B.4.1</b> Definition</a></li>
<li class="chapter" data-level="B.4.2" data-path="prob-review.html"><a href="prob-review.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>B.4.2</b> Mean, variance, and covariance</a></li>
<li class="chapter" data-level="B.4.3" data-path="prob-review.html"><a href="prob-review.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Properties of transformations of random vectors</a></li>
<li class="chapter" data-level="B.4.4" data-path="prob-review.html"><a href="prob-review.html#example-continuous-bivariate-distribution-continued"><i class="fa fa-check"></i><b>B.4.4</b> Example (Continuous bivariate distribution continued)</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="prob-review.html"><a href="prob-review.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>B.5</b> Multivariate normal (Gaussian) distribution</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="prob-review.html"><a href="prob-review.html#definition-1"><i class="fa fa-check"></i><b>B.5.1</b> Definition</a></li>
<li class="chapter" data-level="B.5.2" data-path="prob-review.html"><a href="prob-review.html#linear-functions-of-a-multivariate-normal-random-vector"><i class="fa fa-check"></i><b>B.5.2</b> Linear functions of a multivariate normal random vector</a></li>
<li class="chapter" data-level="B.5.3" data-path="prob-review.html"><a href="prob-review.html#example-ols-matrix-form"><i class="fa fa-check"></i><b>B.5.3</b> Example (OLS matrix form)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="est-infer-review.html"><a href="est-infer-review.html"><i class="fa fa-check"></i><b>C</b> Review of Estimation and Inference</a>
<ul>
<li class="chapter" data-level="C.1" data-path="est-infer-review.html"><a href="est-infer-review.html#estimation"><i class="fa fa-check"></i><b>C.1</b> Estimation</a></li>
<li class="chapter" data-level="C.2" data-path="est-infer-review.html"><a href="est-infer-review.html#hypothesis-testing"><i class="fa fa-check"></i><b>C.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="C.3" data-path="est-infer-review.html"><a href="est-infer-review.html#confidence-intervals"><i class="fa fa-check"></i><b>C.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="C.4" data-path="est-infer-review.html"><a href="est-infer-review.html#linking-hypothesis-tests-and-confidence-intervals"><i class="fa fa-check"></i><b>C.4</b> Linking Hypothesis Tests and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Progressive Introduction to Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interp-chapter" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Interpreting a fitted linear model<a href="interp-chapter.html#interp-chapter" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Interpreting a fitted model is a critical part of a regression analysis
and aids us in determining the role and impact that each variable plays
in describing the behavior of the response variable.</p>
<div id="interpretation-of-coefficients" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Interpretation of coefficients<a href="interp-chapter.html#interpretation-of-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard approach to interpreting the coefficients of a fitted
linear model is to consider the expected change in the response in
relation to changes in the regressors in the model.</p>
<p>Consider the typical multiple linear regression model of the response
<span class="math display" id="eq:mlr-equation-ch4">\[
Y=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon.\tag{4.1}
\]</span></p>
<p>As discussed in Chapter <a href="linear-model-estimation.html#linear-model-estimation">3</a>, we treat the
values of our regressor variables as being fixed, known values. The
error term is treated as a random variable, and consequently, the
response variable is also a random variable. Additionally, we assume
that the errors all have mean 0, conditional on the values of the
regressor variables. More formally, we write this assumption as</p>
<p><span class="math display" id="eq:mean-error-assumption">\[
E(\epsilon \mid X_1, X_2, \ldots, X_{p-1})=0.\tag{4.2}
\]</span></p>
<p>Recall that we use the notation <span class="math inline">\(\mathbb{X} = \{X_1,\ldots,X_{p-1}\}\)</span> to
denote the set of all regressors, which will help us simplify the
derivations below. Thus, the assumption in Equation
<a href="interp-chapter.html#eq:mean-error-assumption">(4.2)</a> can be expressed as
<span class="math inline">\(E(\epsilon \mid \mathbb{X})=0\)</span>. Using the assumption in Equation
<a href="interp-chapter.html#eq:mean-error-assumption">(4.2)</a> and applying it to the model in Equation
<a href="interp-chapter.html#eq:mlr-equation-ch4">(4.1)</a>, we see that</p>
<p><span class="math display">\[
\begin{aligned}
&amp; E(Y\mid X_1, X_2, \ldots, X_{p-1}) \\
&amp;= E(Y \mid \mathbb{X}) \\
&amp;= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon \mid \mathbb{X}) \\
&amp;= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}\mid \mathbb{X}) + E(\epsilon \mid \mathbb{X}) \\
&amp;=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}
\end{aligned}
\]</span></p>
<p>since all terms in the first summand of line 4 are fixed, non-random
values conditional on <span class="math inline">\(\mathbb{X}\)</span> and the second summand is 0 by
assumption. If you are rusty with properties of random variables,
consider reviewing the material in Appendix <a href="prob-review.html#prob-review">B</a>.</p>
<p>Using the facts above, we discuss interpretation of simple linear
regression models, multiple linear regression models with basic numeric
predictors as regressors, and interpretation for parallel and separate
lines regression model.</p>
<div id="interpretation-for-simple-linear-regression" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Interpretation for simple linear regression<a href="interp-chapter.html#interpretation-for-simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a simple linear regression model, so that
<span class="math display" id="eq:slr-equation">\[
E(Y\mid X)=\beta_0 + \beta_1 X. \tag{4.3}
\]</span>
The interpretations of the coefficients are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when the regressor is 0, i.e.,
<span class="math inline">\(\beta_0=E(Y\mid X=0)\)</span>.</li>
<li><span class="math inline">\(\beta_1\)</span> is the expected change in the response when the regressor
increases 1 unit, i.e., <span class="math inline">\(\beta_1=E(Y\mid X=x^*+1)-E(Y\mid X=x^*)\)</span>,
where <span class="math inline">\(x^*\)</span> is a fixed, real number.</li>
</ul>
<p>Regarding the interpretation of <span class="math inline">\(\beta_0\)</span>, from the regression model in
Equation <a href="interp-chapter.html#eq:slr-equation">(4.3)</a>, notice that
<span class="math display">\[
\begin{aligned}
E(Y\mid X = 0) &amp;= \beta_0 + \beta_1 \cdot 0 \\
&amp;= \beta_0.
\end{aligned}
\]</span> This is why <span class="math inline">\(\beta_0\)</span> is the expected value of the response variable
when the regressor is zero.</p>
<p>Similarly, for <span class="math inline">\(\beta_1\)</span>, we notice that
<span class="math display">\[
\begin{aligned}
E(Y\mid X=x^*+1)-E(Y\mid X=x^*) &amp;= [\beta_0 + \beta_1 (x^* + 1)] - [\beta_0 + \beta_1 x^*] \\
&amp;= \beta_1.
\end{aligned}
\]</span>
Thus, <span class="math inline">\(\beta_1\)</span> literally equals the change in the expected response
when the regressor increases by 1 unit.</p>
<p>The regressors we use in our regression analysis are often observational
in nature, meaning that we do not control them. And even if we could
control them, they may be difficult to change. E.g., if one of our
regressors was the size of an island, how would we realistically go
about increasing the size of the island? thus, in the context of
observational data, it may not make sense to say “we increase <span class="math inline">\(X\)</span> by 1
unit” or “when <span class="math inline">\(X\)</span> increases by 1 unit”. An alternative approach,
alluded to by <span class="citation">Faraway (<a href="#ref-lmwr2">2014</a>)</span>, is to consider the expected response difference
between observations that are identical with respect to all regressors
we include in our model except the regressor under consideration, which
varies by only a single unit. While mathematically, the result is the
same, the interpretation is more philosophically palatable. Regardless,
interpretation is a bit of an art. There can be many correct ways to
interpret a coefficient or the impact of a variable. Always double-check
that the mathematics of our model supports our conclusions.</p>
<p>To illustrate the interpretations given above, we interpret the simple
linear regression model fit to the <code>penguins</code> data in Section
<a href="linear-model-estimation.html#s:penguins-slr">3.3</a>. From Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>, the fitted
simple linear regression model of <code>bill_length_mm</code> regressed on
<code>body_mass_g</code> is
<span class="math display" id="eq:slr-penguin-orig">\[
\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g})=26.9+0.004 \tag{4.4} \,\mathtt{body\_mass\_g}.
\]</span>
Some basic interpretations of the coefficients are:</p>
<ul>
<li>Intercept: The expected bill length of a penguin with a body mass of
0 grams is 26.9 mm. We discussed the absurdity of this
interpretation in Section <a href="linear-model-estimation.html#s:penguins-slr">3.3</a>.</li>
<li><code>body_mass_g</code>: A penguin 1 gram heavier than another penguin is
expected to have a bill length 0.004 mm longer than the smaller
penguin.</li>
</ul>
<p>The scale of the latter interpretation is difficult to comprehend. A
weight difference of 1 gram is negligible in the context of penguin
weights, and a bill length change of 0.004 mm is unlikely to be noticed
by the naked eye. This suggests that rescaling our <code>body_mass_g</code>
predictor could result in a more natural interpretation of the
associated coefficient. In the code below, we divide the <code>body_mass_g</code>
variable by 1000 to convert the variable from grams to kilograms, and
consequently, save the rescaled variable as <code>body_mass_kg</code>. We then fit
the model regressing <code>bill_length_mm</code> on <code>body_mass_kg</code> and extract the
estimated coefficients.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="interp-chapter.html#cb122-1" tabindex="-1"></a><span class="co"># load penguins data</span></span>
<span id="cb122-2"><a href="interp-chapter.html#cb122-2" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span>
<span id="cb122-3"><a href="interp-chapter.html#cb122-3" tabindex="-1"></a><span class="co"># transform body mass variable from g to kg</span></span>
<span id="cb122-4"><a href="interp-chapter.html#cb122-4" tabindex="-1"></a>penguins <span class="ot">&lt;-</span> penguins <span class="sc">|&gt;</span> <span class="fu">transform</span>(<span class="at">body_mass_kg =</span> body_mass_g<span class="sc">/</span><span class="dv">1000</span>)</span>
<span id="cb122-5"><a href="interp-chapter.html#cb122-5" tabindex="-1"></a><span class="co"># fit model with body_mass_kg</span></span>
<span id="cb122-6"><a href="interp-chapter.html#cb122-6" tabindex="-1"></a>slmod_scaled <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_kg, <span class="at">data =</span> penguins)</span>
<span id="cb122-7"><a href="interp-chapter.html#cb122-7" tabindex="-1"></a><span class="co"># extract coefficients</span></span>
<span id="cb122-8"><a href="interp-chapter.html#cb122-8" tabindex="-1"></a><span class="fu">coefficients</span>(slmod_scaled)</span>
<span id="cb122-9"><a href="interp-chapter.html#cb122-9" tabindex="-1"></a><span class="do">##  (Intercept) body_mass_kg </span></span>
<span id="cb122-10"><a href="interp-chapter.html#cb122-10" tabindex="-1"></a><span class="do">##    26.898872     4.051417</span></span></code></pre></div>
<p>The fitted model from above is
<span class="math display" id="eq:slr-penguin-rescaled">\[
\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_kg})=26.9+4.05 \,\mathtt{body\_mass\_kg}.\tag{4.5}
\]</span>
Thus, we can interpret the estimated coefficient for <code>body_mass_kg</code>
as something like, “A penguin that is 1 kg larger than another penguin
is expected to have a bill length 4 mm longer than the smaller penguin”.</p>
<p>Comparing the estimated coefficients from Equations
<a href="interp-chapter.html#eq:slr-penguin-orig">(4.4)</a> and <a href="interp-chapter.html#eq:slr-penguin-rescaled">(4.5)</a>, we see
that dividing <code>body_mass_g</code> by 1000 resulted in the estimated
coefficient changing by a factor of 1000. More generally, if
<span class="math inline">\(\hat{\beta}_j\)</span> is the estimated coefficient for <span class="math inline">\(X_j\)</span>, then the
regressor <span class="math inline">\((X_j + a)/c\)</span> will have an estimated coefficient of
<span class="math inline">\(c\hat{\beta}_j\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span> are fixed, real numbers and assuming
nothing else in the fitted model changes.</p>
</div>
<div id="interp-1st-order-ml" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Interpretation for first-order multiple linear regression models<a href="interp-chapter.html#interp-1st-order-ml" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a multiple linear regression model with <span class="math inline">\(p-1\)</span> first-order regressors, so that
<span class="math display" id="eq:mlr-equation">\[
E(Y\mid X_1,\ldots,X_{p-1})=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1}.\tag{4.6}
\]</span>
Relying on the definition of <span class="math inline">\(\mathbb{X}\)</span>, we denote the set of
regressors without <span class="math inline">\(X_j\)</span> as
<span class="math inline">\(\mathbb{X}_{-j} = \mathbb{X}\setminus\{X_j\}\)</span>.</p>
<p>The interpretations of the coefficients from the model in Equation
<a href="interp-chapter.html#eq:mlr-equation">(4.6)</a> are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when all regressors are 0, i.e.,
<span class="math inline">\(\beta_0=E(Y\mid X_1=0,\ldots,X_{p-1}=0)\)</span>.</li>
<li><span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j = 1,\ldots,p-1\)</span>, represents the expected change in the
response when regressor <span class="math inline">\(j\)</span> increases 1 unit and the other
regressors stay the same, i.e.,
<span class="math inline">\(\beta_j=E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*)\)</span>
where
<span class="math inline">\(\mathbf{x}_{-j}^*=[x^*_1,\ldots,x_{j-1}^*,x_{j+1}^*,\ldots,x_{p-1}^*]\in \mathbb{R}^{p-2}\)</span>
is a vector with <span class="math inline">\(p-2\)</span> fixed values (the number of regressors
excluding <span class="math inline">\(X_j\)</span>) and <span class="math inline">\(x_j^*\)</span> is a fixed real number. The
non-intercept coefficients of a multiple linear regression model are
known as <strong>partial slopes</strong>.</li>
</ul>
<p>Regarding the interpretation of <span class="math inline">\(\beta_0\)</span>, from the regression model in
Equation <a href="interp-chapter.html#eq:mlr-equation">(4.6)</a>, notice that
<span class="math display">\[
\begin{aligned}
E(Y\mid X_1=0,\ldots,X_{p-1}=0) &amp;= \beta_0 + \beta_1 \cdot 0 + \cdots + \beta_{p-1} \cdot 0\\
&amp;= \beta_0.
\end{aligned}
\]</span></p>
<p>It is quite common for the mathematical interpretation of the intercept
to be nonsensical because we are extrapolating outside the range of the
observed data.</p>
<p>For <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j = 1,\ldots, p-1\)</span>, we notice that <span class="math display">\[
\begin{aligned}
&amp; E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_j = x_{j}^*+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_j = x_{j}^*)\\
&amp;=  \biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_j(x^*_j+1) + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl] \\
&amp;\quad -\biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_jx^*_j + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl]\\
&amp;= \beta_j.
\end{aligned}
\]</span></p>
<p>A notable problem with the standard mathematical interpretation of
multiple regression models is that a single predictor can be used more
than once in the model. E.g., in the 2nd-degree polynomial regression
model
<span class="math display">\[
E(Y\mid X) = \beta_0 + \beta_1 X + \beta_2 X^2,
\]</span>
<span class="math inline">\(X\)</span> is used in
both the second and third terms. So it is not possible to increase <span class="math inline">\(X\)</span>
while keeping <span class="math inline">\(X^2\)</span> fixed. The mathematical interpretation given in this
section is applicable to first-order linear regression models (cf.
Section <a href="linear-model-estimation.html#model-types">3.7</a>), where a <em>first-order linear regression
model</em> is a multiple linear regression model in which no regressor is a
function of any other regressor.</p>
<p>Additionally, the interpretation given above for the partial slope
coefficients applies to the coefficients of numeric predictors that can
increase by 1 unit; that interpretation doesn’t apply to the
coefficients for categorical predictors, which are discussed in Section
<a href="interp-chapter.html#interp-cat-predictor">4.3</a>.</p>
<p>To illustrate the interpretations given above, we interpret the
first-order multiple linear regression model fit to the <code>penguins</code> data
in Section <a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>. The fitted multiple linear regression
model is
<span class="math display" id="eq:mlr-penguins-interp">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm})\\
&amp;=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\end{aligned}
\tag{4.7}
\]</span></p>
<p>Some basic interpretations of the coefficients are:</p>
<ul>
<li>Intercept: We expect a penguin with a body mass of 0 grams and a
flipper length of 0 mm to have a bill length of -3.44 mm.</li>
<li><code>body_mass_g</code>: For two penguins that have the same flipper length but one penguin has a body mass 1 gram larger, we expect the heavier penguin to have a bill length 0.0007 mm longer than the other penguin.</li>
<li><code>flipper_length_mm</code>: For two penguins with the same body mass but whose flipper lengths differ by 1 mm, we expect the penguin with longer flippers to have a bill length 0.22 mm longer than the other penguin.</li>
</ul>
</div>
<div id="regressor-roles" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Roles of regressor variables<a href="interp-chapter.html#regressor-roles" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Did you notice that the estimated coefficients for the intercept and the
<code>body_mass_g</code> regressor changed between the fitted simple linear model
and the fitted multiple linear regression model for the <code>penguins</code>
example in the sections above (cf. Equations <a href="interp-chapter.html#eq:slr-penguin-orig">(4.4)</a>
and <a href="interp-chapter.html#eq:mlr-penguins-interp">(4.7)</a>)? Why does this happen? What is going
on?</p>
<p>The role a regressor plays in a regression model depends on what other
regressors are in the model. Recall a team setting you’ve been in where
you had to work with others to accomplish something; it could be related
to school, work, sports, etc. Depending on the skills and knowledge your
team members have, you will try to find a role in which you can
effectively help the team. Something similar happens in regression
models. Generally, we can’t provide a definitive interpretation of a
regressor’s role in a fitted model without knowing what other regressors
are in the model. Thus, when interpreting a regressor, it is common to
include something like <em>after accounting for the other variables in
the model</em>. We do this because we are giving the interpretation of that
regressor’s role when the other variables are also in the model. If our
model had different variables, then our interpretation would be
different.</p>
<p>There is also a technical reason why the estimated coefficients change
as we add or remove regressors from a model. If a regressor is
correlated with other regressors in a model, then adding or removing
that regressor will impact the estimated coefficients in the new model.
The more correlated the regressors are, the more they tend to affect
each others’ estimated coefficients. More formally, a regressor will
impact the estimated coefficients of the other regressors in a model
unless it is <strong>orthogonal</strong> to the other regressors. Orthogonality is
related to correlation, but there are important differences. See Section
<a href="interp-chapter.html#orthogonality">4.5.1</a>), which provides an extensive discussion of
orthogonality and how it affects estimation.</p>
</div>
</div>
<div id="effect-plots" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Effect plots<a href="interp-chapter.html#effect-plots" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An effect plot is a visual display that aids in helping us intuitively
interpret the impact of a <em>predictor</em> in a model. As stated by
<span class="citation">Fox and Weisberg (<a href="#ref-fox2020predictor">2020</a>)</span>:</p>
<blockquote>
<p>Summarization of the effects of predictors using tables of coefficient
estimates is often incomplete. Effects, and particularly plots of
effects, can in many instances reveal the relationship of the response
to the predictors more clearly. This conclusion is especially true for
models with linear predictors that include interactions and
multiple-coefficient terms such as regression splines and polynomials
….</p>
</blockquote>
<p>More formally, an <strong>effect plot</strong> is a plot of the estimated mean
response as a function of a <em>focal predictor</em> with the other
<em>predictors</em> being held at “typical values”. The distinction between
predictor and regressor is important when discussing effect plots,
because we can create effect plots for each predictor but not
necessarily each regressor. Recall from Section
<a href="linear-model-estimation.html#a-simple-motivating-example">3.1</a> that a predictor variable is a
variable available to model the response variable, while a regressor
variable is a variable used in our regression model, whether that is an
unmodified predictor variable, some transformation of a predictor, some
combination of predictors, etc.
<!-- Recalling the regression models fit in Chapter <a href="linear-model-estimation.html#linear-model-estimation">3</a>, `body_mass_g`, `flipper_length_mm`, and `species` were predictor variables. Additionally, based on how we used them, `body_mass_g` and `flipper_length_mm` were also regressors in certain models. Additional regressors were `speciesChinstrap` and `speciesGentoo` (the indicator variables for Chinstrap and Gentoo penguins based on the variable `species`, respectively) and `body_mass_g:speciesChinstrap` and `body_mass_g:speciesGentoo`, the regressors created by multiplying the indicator variables with the `body_mass_g` predictor. --></p>
<p>We first discuss how to create an effect plot for a first-order linear
regression models with numeric regressors since these are the simplest
to construct. In a first-order linear model, none of the regressors
interact, i.e., none of the regressors are functions of each other.
<span class="citation">Fox and Weisberg (<a href="#ref-fox2020predictor">2020</a>)</span> use the terminology <strong>fixed group</strong> to refer to the
group of predictors that do not interact with the focal predictor. For a
first-order regression model, all of the non-focal predictors are part
of the fixed group. To create our effect plot, we must first find the
equation for the estimated mean response as a function of a focal
predictor while holding the other predictors at their “typical” values.
We set numeric fixed group predictors equal to their sample means when
finding this function.</p>
<p>We now construct effect plots for the estimated regression model of the
<code>penguins</code> data that regressed <code>bill_length_mm</code> on <code>body_mass_g</code> and
<code>flipper_length_mm</code> (previously considered in Section
<a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>). We refit that model using the code below and
assign it the name <code>mlmod</code>. The fitted model is
<span class="math display" id="eq:mlr-effect-equation">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm})\\
&amp;=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\end{aligned}
\tag{4.8}
\]</span></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="interp-chapter.html#cb123-1" tabindex="-1"></a><span class="co"># load penguins data since it hasn&#39;t been loaded in this chapter</span></span>
<span id="cb123-2"><a href="interp-chapter.html#cb123-2" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span>
<span id="cb123-3"><a href="interp-chapter.html#cb123-3" tabindex="-1"></a><span class="co"># refit multiple linear regression model</span></span>
<span id="cb123-4"><a href="interp-chapter.html#cb123-4" tabindex="-1"></a>mlmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> flipper_length_mm, <span class="at">data =</span> penguins)</span>
<span id="cb123-5"><a href="interp-chapter.html#cb123-5" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb123-6"><a href="interp-chapter.html#cb123-6" tabindex="-1"></a><span class="fu">coef</span>(mlmod)</span>
<span id="cb123-7"><a href="interp-chapter.html#cb123-7" tabindex="-1"></a><span class="do">##       (Intercept)       body_mass_g flipper_length_mm </span></span>
<span id="cb123-8"><a href="interp-chapter.html#cb123-8" tabindex="-1"></a><span class="do">##     -3.4366939266      0.0006622186      0.2218654584</span></span></code></pre></div>
<p>There are two predictors in the model in Equation
<a href="interp-chapter.html#eq:mlr-effect-equation">(4.8)</a>, <code>body_mass_g</code> and <code>flipper_length_mm</code>, so
we can create effect plots for both variables. Since each predictor is
numeric and doesn’t interact with other predictors, the “typical” value
used to determine the estimated mean response will be the sample mean of
the observed predictor values. When fitting a linear model in R, R will
automatically drop any observations that have <code>NA</code> values for any
variables used in our model. Thus, the sample means used in our effect
plot should correspond to the sample mean of the values actually used to
fit the model. If our fitted model was assigned the name <code>lmod</code>, the
response and predictor values used to fit the model may be extracted
using <code>lmod$model</code>. Since the response variable and predictor variables
used to fit <code>mlmod</code> are all numeric, we can use the <code>colMeans</code> function
to get the sample mean of each variable. The sample means of the
<code>body_mass_g</code> and <code>flipper_length_mm</code> values used to fit <code>mlmod</code> are
<span class="math inline">\(\overline{\mathtt{body\_mass\_g}}=4201.75\)</span> and
<span class="math inline">\(\overline{\mathtt{flipper\_length\_mm}}=200.92\)</span>, respectively, as shown
in the code below.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="interp-chapter.html#cb124-1" tabindex="-1"></a><span class="fu">colMeans</span>(mlmod<span class="sc">$</span>model)</span>
<span id="cb124-2"><a href="interp-chapter.html#cb124-2" tabindex="-1"></a><span class="do">##    bill_length_mm       body_mass_g flipper_length_mm </span></span>
<span id="cb124-3"><a href="interp-chapter.html#cb124-3" tabindex="-1"></a><span class="do">##          43.92193        4201.75439         200.91520</span></span></code></pre></div>
<p>The effect plot for <span class="math inline">\(\mathtt{body\_mass\_g}\)</span> (on the response
<span class="math inline">\(\mathtt{bill\_length\_mm}\)</span>) is a plot of
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm} = 200.92)\\
&amp;=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\cdot 200.92 \\
&amp;=41.14+0.0007 \,\mathtt{body\_mass\_g}
\end{aligned}
\]</span>
as a function of <span class="math inline">\(\mathtt{body\_mass\_g}\)</span>. Note: we used exact values
in the calculation above. The intercept will be 40.76 instead of 41.14
if we use the rounded values. Similarly, the effect plot for
<span class="math inline">\(\mathtt{flipper\_length\_mm}\)</span> is a plot of
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g} = 4201.75,\mathtt{flipper\_length\_mm}) \\
&amp;=-3.44+0.0007 \cdot 4201.75+0.22\,\mathtt{flipper\_length\_mm} \\
&amp;=-0.65 + 0.22\,\mathtt{flipper\_length\_mm}
\end{aligned}
\]</span>
as a function of <span class="math inline">\(\mathtt{flipper\_length\_mm}\)</span>.</p>
<p>The <strong><code>effects</code></strong> package <span class="citation">(<a href="#ref-R-effects">Fox et al. 2022</a>)</span> can be used to generate effect
plots for the predictors of a fitted linear model. We start by using the
<code>effects::predictorEffect</code> function to compute the information needed to
draw the plot, then the <code>plot</code> function to display the information.</p>
<p>The <code>predictorEffect</code> function computes the estimated mean response for
different values of the focal predictor while holding the other
predictors at their typical values. The main arguments of
<code>predictorEffect</code> are:</p>
<ul>
<li><code>predictor</code>: the name of the predictor we want to plot. This is the
“focal predictor”.</li>
<li><code>mod</code>: the fitted model. The function works with <code>lm</code> objects and many other types of fitted models.</li>
</ul>
<p>The <code>plot</code> function will take the output of the <code>predictorEffect</code>
function and produce the desired effect plot.</p>
<p>In the code below, we load the <strong><code>effects</code></strong> package (so that we can use
the <code>predictorEffect</code> function) and then combine calls to the <code>plot</code> and
<code>predictorEffect</code> functions to create an effect plot for <code>body_mass_g</code>,
which is shown in Figure <a href="interp-chapter.html#fig:effect-plot-body-mass">4.1</a>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="interp-chapter.html#cb125-1" tabindex="-1"></a><span class="co"># load effects package</span></span>
<span id="cb125-2"><a href="interp-chapter.html#cb125-2" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb125-3"><a href="interp-chapter.html#cb125-3" tabindex="-1"></a><span class="do">## Loading required package: carData</span></span>
<span id="cb125-4"><a href="interp-chapter.html#cb125-4" tabindex="-1"></a><span class="do">## lattice theme set by effectsTheme()</span></span>
<span id="cb125-5"><a href="interp-chapter.html#cb125-5" tabindex="-1"></a><span class="do">## See ?effectsTheme for details.</span></span>
<span id="cb125-6"><a href="interp-chapter.html#cb125-6" tabindex="-1"></a><span class="co"># draw effect plot for body_mass_g</span></span>
<span id="cb125-7"><a href="interp-chapter.html#cb125-7" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;body_mass_g&quot;</span>, mlmod))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:effect-plot-body-mass"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/effect-plot-body-mass-1.png" alt="Effect plot for body mass based on the fitted model in Equation \@ref(eq:mlr-effect-equation)." width="672" />
<p class="caption">
Figure 4.1: Effect plot for body mass based on the fitted model in Equation <a href="interp-chapter.html#eq:mlr-effect-equation">(4.8)</a>.
</p>
</div>
<p>We see from Figure <a href="interp-chapter.html#fig:effect-plot-body-mass">4.1</a> that there is a
clear positive association between <code>body_mass_g</code> and <code>bill_length_mm</code>
after accounting for the <code>flipper_length_mm</code> variable. The shaded area
indicates the 95% confidence interval bands for the estimated mean
response. We do not discuss confidence interval bands here, except to
say that they provide a visual picture of the uncertainty of our
estimated mean (wider bands indicate greater uncertainty). Chapter
<a href="inference.html#inference">6</a> discusses confidence intervals for linear models in
some detail. The many tick marks along the the x-axis of the effect plot
indicate observed values of the x-axis variable.</p>
<p>We next create an effect plot for <code>flipper_length_mm</code>, which is shown in
Figure <a href="interp-chapter.html#fig:effect-plot-flipper-length">4.2</a>, using the code below.
There is a clear positive association between <code>flipper_length_mm</code> and
<code>bill_length_mm</code> after accounting for <code>body_mass_g</code>.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="interp-chapter.html#cb126-1" tabindex="-1"></a><span class="co"># draw effect plot for flipper_length_mm</span></span>
<span id="cb126-2"><a href="interp-chapter.html#cb126-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;flipper_length_mm&quot;</span>, mlmod))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:effect-plot-flipper-length"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/effect-plot-flipper-length-1.png" alt="Effect plot for flipper length based on the fitted model in Equation \@ref(eq:mlr-effect-equation)." width="672" />
<p class="caption">
Figure 4.2: Effect plot for flipper length based on the fitted model in Equation <a href="interp-chapter.html#eq:mlr-effect-equation">(4.8)</a>.
</p>
</div>
<p>Alternatively, we could use <code>effects::allEffects</code> to compute the
necessary effect plot information for all predictors simultaneously,
then use <code>plot</code> to create a display of the effect plots for all
predictors in one graphic. This approach is quicker, but the individual
effect plots can sometimes be too small for practical use. We
demonstrate this faster approach in the code below, which produces
Figure <a href="interp-chapter.html#fig:mlmod-effect-plots-all">4.3</a>.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="interp-chapter.html#cb127-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(mlmod))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:mlmod-effect-plots-all"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/mlmod-effect-plots-all-1.png" alt="All effect plots for predictors of the fitted model in Equation \@ref(eq:mlr-effect-equation)." width="672" />
<p class="caption">
Figure 4.3: All effect plots for predictors of the fitted model in Equation <a href="interp-chapter.html#eq:mlr-effect-equation">(4.8)</a>.
</p>
</div>
</div>
<div id="interp-cat-predictor" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Interpretation for categorical predictors<a href="interp-chapter.html#interp-cat-predictor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now discuss the interpretation of regression coefficients in the
context of a parallel lines and separate lines models.</p>
<div id="pl-interp" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Coefficient interpretation for parallel lines models<a href="interp-chapter.html#pl-interp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a parallel lines model with numeric regressor <span class="math inline">\(X\)</span> and
categorical predictor <span class="math inline">\(C\)</span> with levels <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span>, and <span class="math inline">\(L_3\)</span>. Following
the discussion in Section <a href="linear-model-estimation.html#categorical-predictors">3.8</a>, predictor <span class="math inline">\(C\)</span>
must be transformed into two indicator variables, <span class="math inline">\(D_2\)</span> and <span class="math inline">\(D_3\)</span>, for
category levels <span class="math inline">\(L_2\)</span> and <span class="math inline">\(L_3\)</span>, to be included in our linear model.
<span class="math inline">\(L_1\)</span> is the reference level. The parallel lines model is formulated as
<span class="math display" id="eq:pl-def-interp">\[
E(Y \mid X, C) = \beta_{int} + \beta_{X} X + \beta_{L_2} D_2 +  \beta_{L_3} D_3, \tag{4.9}
\]</span>
where we replace the usual <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and
<span class="math inline">\(\beta_3\)</span> with notation the indicates the regressor each coefficient is
associated with.</p>
<p>When an observation has level <span class="math inline">\(L_1\)</span> and <span class="math inline">\(X=0\)</span>, then the expected
response is
<span class="math display">\[
\begin{aligned}
E(Y|X = 0, C=L_1) &amp;= \beta_{int} + \beta_X \cdot 0 + \beta_{L_2} \cdot 0 + \beta_{L_3} \cdot 0 \\
&amp;= \beta_{int}.
\end{aligned}
\]</span>
Thus, <span class="math inline">\(\beta_{int}\)</span> is the expected response for an observation with
level <span class="math inline">\(L_1\)</span> when <span class="math inline">\(X=0\)</span>.</p>
<p>When an observation has a fixed level <span class="math inline">\(L_j\)</span> (it doesn’t matter which
level) and <span class="math inline">\(X\)</span> increases from <span class="math inline">\(x^*\)</span> to <span class="math inline">\(x^*+1\)</span>, then the change in the
expected response is
<span class="math display">\[
E(Y|X = x^* + 1, C=L_j) - E(Y|X = x^*, C=L_j)= \beta_X.
\]</span>
Thus, <span class="math inline">\(\beta_X\)</span> is the expected change in the response for an
observation with fixed level <span class="math inline">\(L_j\)</span> when <span class="math inline">\(X\)</span> increases by 1 unit.</p>
<p>When an observation has level <span class="math inline">\(L_2\)</span>, the expected response is
<span class="math display">\[
\begin{aligned}
E(Y\mid X = x^*, C=L_2) &amp;= \beta_{int} + \beta_X x^* + \beta_{L_2} \cdot 1 + \beta_{L_3} \cdot 0 \\
&amp;= \beta_{int} + \beta_X x^* + \beta_{L_2}.
\end{aligned}
\]</span>
Thus,
<span class="math display">\[
\begin{aligned}
&amp; E(Y\mid X=x^*, C=L_2) - E(Y\mid X=x^*, C=L_1) \\
&amp;= (\beta_{int} + \beta_X x^* + \beta_{L_2}) - (\beta_{int} + \beta_X x^*) \\
&amp;=  \beta_{L_2}.
\end{aligned}
\]</span>
Thus, <span class="math inline">\(\beta_{L_2}\)</span> is the expected change in the response for a
fixed value of <span class="math inline">\(X\)</span> when comparing on observation having level <span class="math inline">\(L_1\)</span> to
level <span class="math inline">\(L_2\)</span> of predictor <span class="math inline">\(C\)</span>. More specifically, <span class="math inline">\(\beta_{L_2}\)</span> indicates
the distance between the estimated regression lines for observations
having levels <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span>. A similar interpretation holds for
<span class="math inline">\(\beta_{L_3}\)</span> when comparing observations having level <span class="math inline">\(L_3\)</span> to
observations having level <span class="math inline">\(L_1\)</span>. <span class="math inline">\(L_1\)</span> is known as as the reference
level of <span class="math inline">\(C\)</span> because we must refer to it to interpret our model with
respect to other levels of <span class="math inline">\(C\)</span>.</p>
<p>To summarize the interpretation of the coefficients in parallel lines
models like Equation <a href="interp-chapter.html#eq:pl-def-interp">(4.9)</a>, assuming categorical
predictor <span class="math inline">\(C\)</span> has <span class="math inline">\(K\)</span> levels instead of 3:</p>
<ul>
<li><span class="math inline">\(\beta_{int}\)</span> represents the expected response for observations
having the reference level when the numeric regressor <span class="math inline">\(X = 0\)</span>.</li>
<li><span class="math inline">\(\beta_X\)</span> is the expected change in the response when <span class="math inline">\(X\)</span> increases
by 1 unit for a fixed level of <span class="math inline">\(C\)</span>.</li>
<li><span class="math inline">\(\beta_{L_j}\)</span>, for <span class="math inline">\(j=2,\ldots,K\)</span>, represents the expected change in
the response when comparing observations having level <span class="math inline">\(L_1\)</span> and
<span class="math inline">\(L_j\)</span> with <span class="math inline">\(X\)</span> fixed at the same value.</li>
</ul>
<p>In Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a>, we fit a parallel lines model to the
<code>penguins</code> data that used both <code>body_mass_g</code> and <code>species</code> to explain
the behavior of <code>bill_length_mm</code>. Letting <span class="math inline">\(D_C\)</span> denote the indicator
variable for the <code>Chinstrap</code> level and <span class="math inline">\(D_G\)</span> denote the indicator
variable for the <code>Gentoo</code> level, the fitted parallel lines model was
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species})\\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G.
\end{aligned}
\]</span>
In the context of this model:</p>
<ul>
<li>The expected bill length for an Adelie penguin with a body mass of 0
grams is 24.92 mm.</li>
<li>If two penguins are of the same species, but one penguin has a body
mass 1 gram larger, then the larger penguin is expected to have a
bill length 0.004 mm longer than the smaller penguin.</li>
<li>A Chinstrap penguin is expected to have a bill length 9.92 mm longer
than an Adelie penguin, assuming their body mass is the same.</li>
<li>A Gentoo penguin is expected to have a bill length 3.56 mm longer
than an Adelie penguin, assuming their body mass is the same.</li>
</ul>
</div>
<div id="effect-plots-for-fitted-models-with-non-interacting-categorical-predictors" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Effect plots for fitted models with non-interacting categorical predictors<a href="interp-chapter.html#effect-plots-for-fitted-models-with-non-interacting-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How do we create an effect plot for a numeric focal predictor when a
non-interacting categorical predictor is in the model (such as for the
parallel lines model we have been discussing)? In short, we determine
the fitted model equation as a function of the focal predictor for each level of
the categorical predictor, and then compute the weighted average of the
equation with the weights being proportional to the number of
observations in each group.</p>
<p>Let’s construct an effect plot for the <code>body_mass_g</code> predictor in the
context of the <code>penguins</code> parallel lines model discussed in the previous
section. In Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a>, we determined that the
fitted parallel lines model simplified to the following equations
depending on the level of <code>species</code>:
<span class="math display" id="eq:pl-equations">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g} \\
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;= 34.84 + 0.004 \mathtt{body\_mass\_g} \\
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Gentoo}) \\
&amp;= 28.48 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\tag{4.10}
\]</span>
We recreate the fitted model producing these equations in R using the
code below.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="interp-chapter.html#cb128-1" tabindex="-1"></a><span class="co"># refit the parallel lines model</span></span>
<span id="cb128-2"><a href="interp-chapter.html#cb128-2" tabindex="-1"></a>lmodp <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species, <span class="at">data =</span> penguins)</span>
<span id="cb128-3"><a href="interp-chapter.html#cb128-3" tabindex="-1"></a><span class="co"># double-check coefficients</span></span>
<span id="cb128-4"><a href="interp-chapter.html#cb128-4" tabindex="-1"></a><span class="fu">coef</span>(lmodp)</span>
<span id="cb128-5"><a href="interp-chapter.html#cb128-5" tabindex="-1"></a><span class="do">##      (Intercept)      body_mass_g speciesChinstrap </span></span>
<span id="cb128-6"><a href="interp-chapter.html#cb128-6" tabindex="-1"></a><span class="do">##     24.919470977      0.003748497      9.920884113 </span></span>
<span id="cb128-7"><a href="interp-chapter.html#cb128-7" tabindex="-1"></a><span class="do">##    speciesGentoo </span></span>
<span id="cb128-8"><a href="interp-chapter.html#cb128-8" tabindex="-1"></a><span class="do">##      3.557977539</span></span></code></pre></div>
<p>The code below determines the number of observations with each level of
<code>species</code> for the data used in the fitted model <code>lmodp</code>. We see that 151
Adelie, 68 Chinstrap, and 123 Gentoo penguins (342 total penguins) were
used to fit the model stored in <code>lmodp</code>.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="interp-chapter.html#cb129-1" tabindex="-1"></a><span class="fu">table</span>(lmodp<span class="sc">$</span>model<span class="sc">$</span>species)</span>
<span id="cb129-2"><a href="interp-chapter.html#cb129-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb129-3"><a href="interp-chapter.html#cb129-3" tabindex="-1"></a><span class="do">##    Adelie Chinstrap    Gentoo </span></span>
<span id="cb129-4"><a href="interp-chapter.html#cb129-4" tabindex="-1"></a><span class="do">##       151        68       123</span></span></code></pre></div>
<p>The equation used to create the effect plot of <code>body_mass_g</code> is the
weighted average of the equations in Equation <a href="interp-chapter.html#eq:pl-equations">(4.10)</a>,
with weights proportional to the number of observational having each
level of the categorical predictor. Specifically,
<span class="math display">\[
\begin{aligned}
&amp; \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{typical}) \\
&amp;= \frac{151}{342}(24.92 + 0.004 \mathtt{body\_mass\_g})\\
&amp;\quad + \frac{68}{342}(34.84 + 0.004 \mathtt{body\_mass\_g})\\
&amp;\quad + \frac{123}{342}(28.48 + 0.004 \mathtt{body\_mass\_g}) \\
&amp;=28.17 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\]</span>
The effect plot for <code>body_mass_g</code> for the fitted parallel lines model
is shown in Figure <a href="interp-chapter.html#fig:effect-plot-plmod-body-mass">4.4</a>. The
association between <code>bill_length_mm</code> and <code>body_mass_g</code> is positive after
accounting for <code>species</code>.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="interp-chapter.html#cb130-1" tabindex="-1"></a><span class="co"># draw effect plot for body_mass_g</span></span>
<span id="cb130-2"><a href="interp-chapter.html#cb130-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;body_mass_g&quot;</span>, lmodp))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:effect-plot-plmod-body-mass"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/effect-plot-plmod-body-mass-1.png" alt="The effect plot of `body_mass_g` for the fitted parallel lines model." width="672" />
<p class="caption">
Figure 4.4: The effect plot of <code>body_mass_g</code> for the fitted parallel lines model.
</p>
</div>
<!-- Alternatively, if we want to see the effect plot for `body_mass_g` for each level of the categorical predictors, then we can use the `multiline` argument of the `plot` function to get the desired result. We use the code below to produce Figure <a href="#fig:eff-plot-pl-multiline"><strong>??</strong></a>.  -->
<!-- ```{r eff-plot-pl-multiline, fig.cap="An effect plot of `body_mass_g` that distinguishes the effect for each level of `species`."} -->
<!-- plot(predictorEffect("body_mass_g", lmodp),  lines=TRUE) -->
<!-- ``` -->
<p>An effect plot for a categorical predictor, assuming all other
predictors in the model are non-interacting numerical predictors (i.e.,
fixed group predictors), is a plot of the estimated mean response for
each level of the categorical variable when the fixed group group
predictors are held at their sample mean. The sample mean of the
<code>body_mass_g</code> values used to fit <code>lmodp</code> is 4201.75, as shown in the
code below.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="interp-chapter.html#cb131-1" tabindex="-1"></a><span class="co"># sample mean of body_mass_g variable used to fit lmodp</span></span>
<span id="cb131-2"><a href="interp-chapter.html#cb131-2" tabindex="-1"></a><span class="fu">mean</span>(lmodp<span class="sc">$</span>model<span class="sc">$</span>body_mass_g)</span>
<span id="cb131-3"><a href="interp-chapter.html#cb131-3" tabindex="-1"></a><span class="do">## [1] 4201.754</span></span></code></pre></div>
<p>Using the first equation in Equation <a href="interp-chapter.html#eq:pl-equations">(4.10)</a>, the
estimated mean for the Adelie species when <code>body_mass_g</code> is fixed at
4201.75 is
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g} = 4201.75, \mathtt{species}=\mathtt{Adelie}) \\
&amp;= 24.92 + 0.004 \cdot 4201.75 \\
&amp;= 40.67.
\end{aligned}
\]</span>
Similarly,
<span class="math inline">\(\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g} = 4201.75, \mathtt{species}=\mathtt{Chinstrap}) = 50.59\)</span>
and
<span class="math inline">\(\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g} = 4201.75, \mathtt{species}=\mathtt{Gentoo}) = 44.23\)</span>.</p>
<p>The code below produces the effect plot for <code>species</code>, which is shown in
Figure <a href="interp-chapter.html#fig:eff-plot-species-plmod">4.5</a>. We see that after accounting
for <code>body_mass_g</code>, the <code>bill_length_mm</code> tends to be largest for
Chinstrap penguins, second largest for Gentoo penguins, and smallest for
Adelie penguins. The confidence bands for the estimated mean response
are shown by the vertical bars.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="interp-chapter.html#cb132-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;species&quot;</span>, lmodp))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:eff-plot-species-plmod"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/eff-plot-species-plmod-1.png" alt="An effect plot for `species` after accounting for `body_mass_g`." width="672" />
<p class="caption">
Figure 4.5: An effect plot for <code>species</code> after accounting for <code>body_mass_g</code>.
</p>
</div>
</div>
<div id="sl-interp" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Coefficient interpretation for separate lines models<a href="interp-chapter.html#sl-interp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a separate lines model with numeric regressor <span class="math inline">\(X\)</span> and
categorical predictor <span class="math inline">\(C\)</span> with levels <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span>, and <span class="math inline">\(L_3\)</span>. The
predictor <span class="math inline">\(C\)</span> will be transformed into two indicator variables, <span class="math inline">\(D_2\)</span>
and <span class="math inline">\(D_3\)</span>, for category levels <span class="math inline">\(L_2\)</span> and <span class="math inline">\(L_3\)</span>, with <span class="math inline">\(L_1\)</span> being the
reference level. The separate lines model is formulated as
<span class="math display" id="eq:sl-def-interp">\[
E(Y \mid X, C) = \beta_{int} + \beta_{X} X + \beta_{L_2} D_2 +  \beta_{L_3} D_3 + \beta_{XL_2} XD_2+\beta_{XL_3}XD_3. \tag{4.11}
\]</span></p>
<p>When an observation has level <span class="math inline">\(L_1\)</span> and <span class="math inline">\(X=x^*\)</span>, then the expected
response is
<span class="math display" id="eq:slr-mean-L1">\[
\begin{aligned}
&amp;E(Y\mid X = x^*, C=L_1) \\
&amp;= \beta_{int} + \beta_X \cdot x^* + \beta_{L_2} \cdot 0 + \beta_{L_3} \cdot 0  + \beta_{X L_2} \cdot x^* \cdot 0 + \beta_{X L_3}\cdot x^* \cdot 0 \\
&amp;= \beta_{int} + \beta_X x^*.
\end{aligned}
\tag{4.12}
\]</span></p>
<p>Using Equation <a href="interp-chapter.html#eq:slr-mean-L1">(4.12)</a>, we can verify that:</p>
<ul>
<li><span class="math inline">\(\beta_{int} = E(Y\mid X = 0, C=L_1)\)</span>.</li>
<li><span class="math inline">\(\beta_{X} = E(Y\mid X = x^* + 1, C=L_1) - E(Y\mid X = x^*, C=L_1)\)</span>.</li>
</ul>
<p>Similarly, when <span class="math inline">\(C=L_2\)</span>,
<span class="math display" id="eq:slr-mean-L2">\[
\begin{aligned}
&amp; E(Y|X = x^*, C=L_2) \\
&amp;= \beta_{int} + \beta_X \cdot x^* + \beta_{L_2} \cdot 1 + \beta_{L_3} \cdot 0  + \beta_{X L_2} \cdot x^* \cdot 1 + \beta_{X L_3}\cdot x^* \cdot 0 \\
&amp;= \beta_{int} + \beta_X x^* + \beta_{L_2} + \beta_{XL_2}x^*\\
&amp;= (\beta_{int} + \beta_{L_2}) + (\beta_X + \beta_{XL_2})x^*.
\end{aligned}
\tag{4.13}
\]</span>
Following this same pattern, when <span class="math inline">\(C=L_3\)</span> we have
<span class="math display" id="eq:slr-mean-L3">\[
E(Y|X = x^*, C=L_3) = (\beta_{int} + \beta_{L_3}) + (\beta_X + \beta_{XL_3})x^*. \tag{4.14}
\]</span></p>
<p>Using Equations <a href="interp-chapter.html#eq:slr-mean-L1">(4.12)</a>, <a href="interp-chapter.html#eq:slr-mean-L2">(4.13)</a>, and
<a href="interp-chapter.html#eq:slr-mean-L3">(4.14)</a>, we can verify that for <span class="math inline">\(j=2,3\)</span>,
<span class="math display">\[
\beta_{L_j}= E(Y\mid X = 0, C=L_j) - E(Y\mid X = 0, C=L_1),
\]</span>
and
<span class="math display">\[
\begin{aligned}
&amp; \beta_{XL_j} \\
&amp;= [E(Y\mid X = x^*+1, C=L_j) - E(Y\mid X = x^*, C=L_j)]\\
&amp;\quad-[E(Y\mid X = x^*+1, C=L_1) - E(Y\mid X = x^*, C=L_1)].
\end{aligned}
\]</span></p>
<p>To summarize the interpretation of the coefficients in separate lines
models like Equation <a href="interp-chapter.html#eq:sl-def-interp">(4.11)</a>, assuming categorical
predictor <span class="math inline">\(C\)</span> has <span class="math inline">\(K\)</span> levels instead of 3:</p>
<ul>
<li><span class="math inline">\(\beta_{int}\)</span> represents the expected response for observations
having the reference level when the numeric regressor <span class="math inline">\(X = 0\)</span>.</li>
<li><span class="math inline">\(\beta_{L_j}\)</span>, for <span class="math inline">\(j=2,\ldots,K\)</span>, represents the expected change in
the response when comparing observations having level <span class="math inline">\(L_1\)</span> and
<span class="math inline">\(L_j\)</span> with <span class="math inline">\(X=0\)</span>.</li>
<li><span class="math inline">\(\beta_X\)</span> represents the expected change in the response when <span class="math inline">\(X\)</span>
increases by 1 unit for observations having the reference level.</li>
<li><span class="math inline">\(\beta_{X L_j}\)</span>, for <span class="math inline">\(j=2,\ldots,K\)</span>, represents the difference in the
expected response between observations having the reference level in
comparison to level <span class="math inline">\(L_j\)</span> when <span class="math inline">\(X\)</span> increases by 1 unit. More simply,
these terms represent the difference in the rate of change for
observations having level <span class="math inline">\(L_j\)</span> compared to the reference level.</li>
</ul>
<p>We illustrate these interpretation using the separate lines model to the
<code>penguins</code> data in Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a>. The fitted
separate lines model was
<span class="math display" id="eq:sl-refit-interp">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&amp;\quad + 0.001 D_C \mathtt{body\_mass\_g} + 0.0009 D_G \mathtt{body\_mass\_g}.
\end{aligned}
\tag{4.15}
\]</span>
In the context of this model:</p>
<ul>
<li>The expected bill length for an Adelie penguin with a body mass of 0
grams is 26.99 mm.</li>
<li>If an Adelie penguin has a body mass 1 gram larger than another
Adelie penguin, then the larger penguin is expected to have a bill
length 0.003 mm longer than the smaller penguin.</li>
<li>A Chinstrap penguin is expected to have a bill length 5.18 mm longer
than an Adelie penguin when both have a body mass of 0 grams.</li>
<li>A Gentoo penguin is expected to have a bill length 0.25 mm shorter
than an Adelie penguin when both have a body mass of 0 grams.</li>
<li>For each 1 gram increase in body mass, we expect the change in bill
length by Chinstrap penguins to be 0.001 mm larger than the
corresponding change in bill length by Adelie penguins.</li>
<li>For each 1 gram increase in body mass, we expect the change in bill
length by Gentoo penguins to be 0.0009 mm larger than the
corresponding change in bill length by Adelie penguins.</li>
</ul>
</div>
<div id="effect-plots-for-interacting-categorical-predictors" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Effect plots for interacting categorical predictors<a href="interp-chapter.html#effect-plots-for-interacting-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now discuss construction of effect plots for a separate lines model,
which has an interaction between a categorical and numeric predictor. In
additional to the focal and fixed group predictors we have previously
discussed, <span class="citation">Fox and Weisberg (<a href="#ref-fox2020predictor">2020</a>)</span> also discuss predictors in the
<strong>conditioning group</strong>, which is the set of predictors that interact
with the focal predictor.</p>
<p>When some predictors interact with the focal predictor, the effect plot
of the focal predictor is a plot of the estimated mean response when the
fixed group predictors are held at their typical values and the
conditioning group predictors vary over different combinations of
discrete values. The process of determining the estimated mean response
as function of the focal predictor is similar to before, but there are
more predictor values on which to condition. By default, to compute the
estimated mean response as a function of the focal predictor, we:</p>
<ul>
<li>Hold the numeric fixed group predictors at their sample means.</li>
<li>Average the estimated mean response equation across the different
levels of a fixed group categorical predictor, with weights equal to
the number of observations with each level.</li>
<li>Compute the estimated mean response function for 5 discrete
values of numeric predictors in the conditioning group.</li>
<li>Compute the estimated mean response function for different levels
of a categorical predictor in the conditioning group.</li>
</ul>
<p>We provide examples of the effect plots for the <code>body_mass_g</code> and
<code>species</code> predictors for the separate lines model fit to the <code>penguins</code>
data and given in Equation <a href="interp-chapter.html#eq:sl-refit-interp">(4.15)</a>. We first run the
code below to fit the separate lines model previously fit in Section
<a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a>.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="interp-chapter.html#cb133-1" tabindex="-1"></a><span class="co"># fit separate lines model</span></span>
<span id="cb133-2"><a href="interp-chapter.html#cb133-2" tabindex="-1"></a>lmods <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species <span class="sc">+</span> body_mass_g<span class="sc">:</span>species,</span>
<span id="cb133-3"><a href="interp-chapter.html#cb133-3" tabindex="-1"></a>            <span class="at">data =</span> penguins)</span>
<span id="cb133-4"><a href="interp-chapter.html#cb133-4" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb133-5"><a href="interp-chapter.html#cb133-5" tabindex="-1"></a><span class="fu">coef</span>(lmods)</span>
<span id="cb133-6"><a href="interp-chapter.html#cb133-6" tabindex="-1"></a><span class="do">##                  (Intercept)                  body_mass_g </span></span>
<span id="cb133-7"><a href="interp-chapter.html#cb133-7" tabindex="-1"></a><span class="do">##                26.9941391367                 0.0031878758 </span></span>
<span id="cb133-8"><a href="interp-chapter.html#cb133-8" tabindex="-1"></a><span class="do">##             speciesChinstrap                speciesGentoo </span></span>
<span id="cb133-9"><a href="interp-chapter.html#cb133-9" tabindex="-1"></a><span class="do">##                 5.1800537287                -0.2545906615 </span></span>
<span id="cb133-10"><a href="interp-chapter.html#cb133-10" tabindex="-1"></a><span class="do">## body_mass_g:speciesChinstrap    body_mass_g:speciesGentoo </span></span>
<span id="cb133-11"><a href="interp-chapter.html#cb133-11" tabindex="-1"></a><span class="do">##                 0.0012748183                 0.0009029956</span></span></code></pre></div>
<p>We previously determined in Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.9</a> that the
model simplifies depending on the level of species. Specifically, we
have that
<span class="math display" id="eq:separate-lines-equations-effects-plot">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g},\\
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;= 31.17 + 0.004 \mathtt{body\_mass\_g},\\
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Gentoo}) \\
&amp;= 26.74 + 0.004 \mathtt{body\_mass\_g}.
\end{aligned}
\tag{4.16}
\]</span></p>
<p>The effect plot of <code>body_mass_g</code> for the separate lines model will be a
plot of each equation given in Equation
<a href="interp-chapter.html#eq:separate-lines-equations-effects-plot">(4.16)</a>. Figure
<a href="interp-chapter.html#fig:effect-plot-body-mass-lmods">4.6</a> displays this effect plot, which
was created using the code below. We use the <code>axes</code> argument to rotate
the x-axis labels (otherwise the text overlaps) and the <code>lines</code>
argument to display all three lines in one graphic instead of a
separate panel for each level of <code>species</code>. We notice Chinstrap penguins
tend to have the largest bill lengths for a given value of body mass and
the bill lengths increase more quickly as a function of body mass then
for the Adelie and Gentoo penguins. Similarly, the Adelie penguins tend
to have the smallest bill length for a fixed value of body mass and the
bill length tends to increase more slowly as body mass increases
compared to the other two types of penguins.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="interp-chapter.html#cb134-1" tabindex="-1"></a><span class="co"># effect plot of body mass for separate lines model</span></span>
<span id="cb134-2"><a href="interp-chapter.html#cb134-2" tabindex="-1"></a><span class="co"># axes ... rotates the x-axis labels 90 degrees</span></span>
<span id="cb134-3"><a href="interp-chapter.html#cb134-3" tabindex="-1"></a><span class="co"># lines ... plots the effect of body mass in one graphic</span></span>
<span id="cb134-4"><a href="interp-chapter.html#cb134-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;body_mass_g&quot;</span>, lmods),</span>
<span id="cb134-5"><a href="interp-chapter.html#cb134-5" tabindex="-1"></a>     <span class="at">axes =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fu">list</span>(<span class="at">rotate =</span> <span class="dv">90</span>)),</span>
<span id="cb134-6"><a href="interp-chapter.html#cb134-6" tabindex="-1"></a>     <span class="at">lines =</span> <span class="fu">list</span>(<span class="at">multiline =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:effect-plot-body-mass-lmods"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/effect-plot-body-mass-lmods-1.png" alt="Effect plot for body mass based on the equations in Equation \@ref(eq:separate-lines-equations-effects-plot)." width="672" />
<p class="caption">
Figure 4.6: Effect plot for body mass based on the equations in Equation <a href="interp-chapter.html#eq:separate-lines-equations-effects-plot">(4.16)</a>.
</p>
</div>
<p>The effect plot of <code>species</code> for the separate lines model will be a plot
of the estimated mean response in Equation
<a href="interp-chapter.html#eq:separate-lines-equations-effects-plot">(4.16)</a> for each level of
<code>species</code> when varying <code>body_mass_g</code> over 5 discrete values. Figure
<a href="interp-chapter.html#fig:effect-plot-species-lmods">4.7</a> displays this effect plot, which
was created using the code below. By specifying
<code>lines = list(multiline = TRUE)</code>, the estimated mean responses for each
level of <code>species</code> are connected for each discrete value of
<code>body_mass_g</code>. Figure <a href="interp-chapter.html#fig:effect-plot-species-lmods">4.7</a> allows us to
determine the effect of <code>species</code> on <code>bill_length_mm</code> when we vary
<code>body_mass_g</code> over 5 discrete values. When varying <code>body_mass_g</code> across
the values 2700, 3600, 4500, 5300, and 6300 g, we see greater changes in
the estimated mean of <code>bill_lengh_mm</code> for Chinstrap penguins in
comparison to Adelie and Gentoo penguins.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="interp-chapter.html#cb135-1" tabindex="-1"></a><span class="co"># effect plot of body mass for separate lines model</span></span>
<span id="cb135-2"><a href="interp-chapter.html#cb135-2" tabindex="-1"></a><span class="co"># axes ... rotates the x-axis labels 90 degrees</span></span>
<span id="cb135-3"><a href="interp-chapter.html#cb135-3" tabindex="-1"></a><span class="co"># lines ... plots the effect of species in one graphic</span></span>
<span id="cb135-4"><a href="interp-chapter.html#cb135-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predictorEffect</span>(<span class="st">&quot;species&quot;</span>, lmods),</span>
<span id="cb135-5"><a href="interp-chapter.html#cb135-5" tabindex="-1"></a>     <span class="at">axes =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fu">list</span>(<span class="at">rotate =</span> <span class="dv">90</span>)),</span>
<span id="cb135-6"><a href="interp-chapter.html#cb135-6" tabindex="-1"></a>     <span class="at">lines=</span><span class="fu">list</span>(<span class="at">multiline =</span> <span class="cn">FALSE</span>))</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:effect-plot-species-lmods"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/effect-plot-species-lmods-1.png" alt="Effect plot for species based on the equations in Equation \@ref(eq:separate-lines-equations-effects-plot)." width="672" />
<p class="caption">
Figure 4.7: Effect plot for species based on the equations in Equation <a href="interp-chapter.html#eq:separate-lines-equations-effects-plot">(4.16)</a>.
</p>
</div>
<p>We refer the reader to the “Predictor effects gallery” vignette in the
<strong><code>effects</code></strong> package (run
<code>vignette("predictor-effects-gallery", package = "effects")</code> in the
Console) for more details about how to construct effect plots in
different settings.</p>
</div>
</div>
<div id="added-variable-and-leverage-plots" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Added-variable and leverage plots<a href="interp-chapter.html#added-variable-and-leverage-plots" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we will use visual displays to assess the impact of a predictor after accounting for the impact of other preditors already in the model.</p>
<p>An <strong>added-variable plot</strong> or <strong>partial regression plot</strong> displays the
marginal effect of a regressor on the response after accounting for the
other regressors in the model <span class="citation">(<a href="#ref-mt1977">Mosteller and Tukey 1977</a>)</span>. While an effect plot is a plot
of the estimated mean relationship between the response and a focal
predictor while holding the model’s predictors at typical values, the
added-variable plot is a plot of two sets of residuals against one
other.</p>
<p>We create an added-variable plot for regressor <span class="math inline">\(X_j\)</span> in the following
way:</p>
<ol style="list-style-type: decimal">
<li>Compute the residuals of the model regressing the response <span class="math inline">\(Y\)</span> on
all regressors except <span class="math inline">\(X_j\)</span>. We denote these residuals
<span class="math inline">\(\hat{\boldsymbol{\epsilon}}(Y\mid \mathbb{X}_{-j})\)</span>. These
residuals represent the part of the response variable not explained
by the regressors in <span class="math inline">\(\mathbb{X}_{-j}\)</span>.</li>
<li>Compute the residuals of the model regressing the regressor <span class="math inline">\(X_j\)</span> on
all regressors except <span class="math inline">\(X_j\)</span>. We denote these residuals
<span class="math inline">\(\hat{\boldsymbol{\epsilon}}(X_j \mid \mathbb{X}_{-j})\)</span>. These
residuals represent the part of the <span class="math inline">\(X_j\)</span> not explained by the
regressors in <span class="math inline">\(\mathbb{X}_{-j}\)</span>. Alternatively, these residuals
represent the amount of additional information <span class="math inline">\(X_j\)</span> provides after
accounting for the regressors in <span class="math inline">\(\mathbb{X}_{-j}\)</span>.</li>
<li>The added-variable plot for <span class="math inline">\(X_j\)</span> is a plot of
<span class="math inline">\(\hat{\boldsymbol{\epsilon}}(Y\mid \mathbb{X}_{-j})\)</span> on the y-axis
and <span class="math inline">\(\hat{\boldsymbol{\epsilon}}(X_j \mid \mathbb{X}_{-j})\)</span> on the
x-axis.</li>
</ol>
<p>Added-variable plots allow us to visualize the impact a regressor has
when added to an existing regression model. We can use the
added-variable plot for <span class="math inline">\(X_j\)</span> to visually estimate the partial slope
<span class="math inline">\(\hat{\beta}_{j}\)</span> <span class="citation">(<a href="#ref-sheather2009modern">Sheather 2009</a>)</span>. In fact, the simple linear
regression line that minimizes the RSS for the added-variable plot of
<span class="math inline">\(X_j\)</span> will have slope <span class="math inline">\(\hat{\beta}_j\)</span>.</p>
<p>We can use an added-variable plot in several ways:</p>
<ol style="list-style-type: decimal">
<li>To assess the marginal relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span> after
accounting for all of the other variables in the model.</li>
<li>To assess the strength of this marginal relationship.</li>
<li>To identify deficiencies in our fitted model.</li>
<li>To identify outliers and observations influential in determining the estimated partial slope.</li>
</ol>
<p>We focus on points 1 and 2 above, as they are directly related to
interpreting our fitted model. We discuss points 3 and 4 in the context of diagnostics for assessing the appropriateness of our model.</p>
<p>In regards to point 1 and 2:</p>
<ul>
<li>If the added-variable plot for <span class="math inline">\(X_j\)</span> is essentially a scatter of points with slope zero, then <span class="math inline">\(X_j\)</span> can do little to explain <span class="math inline">\(Y\)</span> after accounting for the other regressors. There is little to gain
in adding <span class="math inline">\(X_j\)</span> as an additional regressor to the model regressing
<span class="math inline">\(Y\)</span> on <span class="math inline">\(\mathbb{X}_{-j}\)</span>. Figure <a href="interp-chapter.html#fig:avplot-examples">4.8</a> (a)
provides an example of this situation.</li>
<li>If the points in an added-variable plot for <span class="math inline">\(X_j\)</span> have a linear relationship, then adding <span class="math inline">\(X_j\)</span> to the model regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(\mathbb{X}_{-j}\)</span> is expected to improve our model’s ability to predict the behavior of <span class="math inline">\(Y\)</span>. The stronger the linear relationship of the points in the added-variable plot, the more important this
variable tends to be in our model. Figure <a href="interp-chapter.html#fig:avplot-examples">4.8</a> (b) demonstrates this scenario.</li>
</ul>
<p>In regard to point 3, if the points in an added-variable plot for <span class="math inline">\(X_j\)</span> are curved or non-linear, it indicates that that there is a deficiency in the fitted model (likely because we need to include one or more additional regressors to the model). Figure <a href="interp-chapter.html#fig:avplot-examples">4.8</a> (c) provides an example of this situation.</p>
<p>In relation to point 4, if certain points in the added-variable plot seem to “pull” the fitted line toward themselves so that the line doesn’t fit the bulk of the data, that indicates the presence of influential observations that are substantially influencing the fit of the model to the data. Figure <a href="interp-chapter.html#fig:avplot-examples">4.8</a> (d) provides an example of this situation.</p>
<div class="figure"><span style="display:block;" id="fig:avplot-examples"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/avplot-examples-1.png" alt="Examples of added-variable plots" width="672" />
<p class="caption">
Figure 4.8: Examples of added-variable plots
</p>
</div>
<p>The <strong><code>car</code></strong> package <span class="citation">(<a href="#ref-R-car">Fox, Weisberg, and Price 2024</a>)</span> provides the <code>avPlot</code> and <code>avPlots</code>
functions for creating added-variable plots. The <code>avPlot</code> function will
produce an added-variable plot for a single regressor while the
<code>avPlots</code> function will produce added-variable plots for one or more
regressors.</p>
<p>The main arguments to the <code>avPlot</code> function are:</p>
<ul>
<li><p><code>model</code>: the fitted <code>lm</code> (or <code>glm</code>) object.</p></li>
<li><p><code>variable</code>: the regressor for which to create an added-variable
plot.</p></li>
<li><p><code>id</code>: a logical value indicating whether unusual observations should
be identified. By default, the value is <code>TRUE</code>, which means the 2
points with the largest residuals and the 2 points with the largest
partial leverage are identified, though this can be customized.</p></li>
</ul>
<p>The <code>avPlots</code> function replaces the <code>variable</code> argument with the <code>terms</code>
argument. The <code>terms</code> argument should be a one-sided formula to indicate
the regressors for which we want to construct added-variable plots (one
plot for each term). By default, an added-variable plot is created for
each regressor. Run <code>car::avPlot</code> in the Console for information about
about the arguments and details of the <code>avPlot</code> and <code>avPlots</code> functions.</p>
<p>We now create and interpret added-variable plots for the model
regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>,
which was previously assigned the name <code>mlmod</code>. We first load the
<strong><code>car</code></strong> package and then use the <code>avPlots</code> function to construct
added-variable plots for <code>body_mass_g</code> and <code>flipper_length_mm</code>. Figure
<a href="interp-chapter.html#fig:avplots-mlmod">4.9</a> displays the added-variable plots for
<code>body_mass_g</code> and <code>flipper_length_mm</code>. The blue line is the simple
linear regression model that minimizes the RSS of the points. The
added-variable plot for <code>body_mass_g</code> exhibits a weak positive linear
relationship between the points. After using the <code>flipper_length_mm</code>
variable to explain the behavior of <code>bill_length_mm</code>, <code>body_mass_g</code>
likely has some additional explanatory power, but it doesn’t explain a
lot of additional response variation. The added-variable plot for
<code>flipper_length_mm</code> exhibits a slightly stronger positive linear
relationship. The <code>flipper_length_mm</code> variable seems to have some
additional explanatory power when added to the model regressing
<code>bill_length_mm</code> on <code>body_mass_g</code>.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="interp-chapter.html#cb136-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb136-2"><a href="interp-chapter.html#cb136-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb136-3"><a href="interp-chapter.html#cb136-3" tabindex="-1"></a><span class="do">## Attaching package: &#39;car&#39;</span></span>
<span id="cb136-4"><a href="interp-chapter.html#cb136-4" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:dplyr&#39;:</span></span>
<span id="cb136-5"><a href="interp-chapter.html#cb136-5" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb136-6"><a href="interp-chapter.html#cb136-6" tabindex="-1"></a><span class="do">##     recode</span></span>
<span id="cb136-7"><a href="interp-chapter.html#cb136-7" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:purrr&#39;:</span></span>
<span id="cb136-8"><a href="interp-chapter.html#cb136-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb136-9"><a href="interp-chapter.html#cb136-9" tabindex="-1"></a><span class="do">##     some</span></span>
<span id="cb136-10"><a href="interp-chapter.html#cb136-10" tabindex="-1"></a><span class="co"># create added-variable plots for all regressors in mlmod</span></span>
<span id="cb136-11"><a href="interp-chapter.html#cb136-11" tabindex="-1"></a><span class="fu">avPlots</span>(mlmod)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:avplots-mlmod"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/avplots-mlmod-1.png" alt="The added-variable plots of all regressors for the model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm`." width="672" />
<p class="caption">
Figure 4.9: The added-variable plots of all regressors for the model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>.
</p>
</div>
<p>The added-variable plots of fitted models with categorical predictors
often show “clusters” of points related to the categorical predictors.
These clusters aren’t anything to worry about unless the overall pattern
of the points is non-linear. We use the code below to create the
added-variable plots for all regressors in the parallel lines model
previously fit to the <code>penguins</code> data. The fitted model was
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species})\\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G,
\end{aligned}
\]</span>
where <span class="math inline">\(D_C\)</span> and <span class="math inline">\(D_G\)</span> are indicator variables for the Chinstrap and
Gentoo penguin species (Adelie penguins are the reference species).
Figure <a href="interp-chapter.html#fig:avplots-lmodp">4.10</a> displays the added-variable plots for
the <code>body_mass_g</code> regressor and the indicator variables for Chinstrap
and Gentoo penguins. The added-variable plot for <code>body_mass_g</code> has a
moderately strong linear relationship, so adding <code>body_mass_g</code> to the
model regressing <code>bill_length_mm</code> on <code>species</code> seems to be beneficial.
The other two variable plots also show a linear relationship. Clustering
patterns are apparent in the added-variable plot for the Chinstrap
penguins indicator variable (<code>speciesChinstrap</code>) but not for the Gentoo
penguins indicator variable.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="interp-chapter.html#cb137-1" tabindex="-1"></a><span class="fu">avPlots</span>(lmodp)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:avplots-lmodp"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/avplots-lmodp-1.png" alt="The added-variable plots for all regressors in the parallel lines model fit to the `penguins` data." width="672" />
<p class="caption">
Figure 4.10: The added-variable plots for all regressors in the parallel lines model fit to the <code>penguins</code> data.
</p>
</div>
<p>A challenge in interpreting the added-variable plots of indicator
variable regressors is that it often doesn’t make sense to talk about
the effect of adding a single regressor when all of the other
regressors are in the model. Specifically, we either add the categorical
<em>predictor</em> to our regression model or we do not. When we add a
categorical predictor to our model, we simultaneously add <span class="math inline">\(K-1\)</span>
indicator variables as regressors; we do not add the indicator variables
one-at-a-time. In general, we refer to regressors with this behavior as
“multiple degrees-of-freedom terms”. A categorical variable with 3 or
more levels is the most basic of multiple degrees-of-freedom term, but
we can also consider regressors related to the interaction between
two or more predictors, polynomial regressors, etc.</p>
<p>A <strong>leverage plot</strong> is an extension of the added-variable plot that
allows us to visualize the impact of multiple degrees-of-freedom terms.
<span class="citation">Sall (<a href="#ref-sall1990leverage">1990</a>)</span> originally proposed leverage plots to visualize
hypothesis tests of linear hypotheses. The interpretation of leverage
plots is similar to the interpretation of added-variable plots, though
we refer to “predictors” or “terms” instead regressors (which may be
combined into one plot). The <code>leveragePlot</code> and <code>leveragePlots</code>
functions in the <strong><code>car</code></strong> package produce single or multiple leverage
plots, respectively, with arguments similar to the <code>avPlot</code> and
<code>avPlots</code> functions.</p>
<p>We use the code below to create Figure <a href="interp-chapter.html#fig:leverageplot-lmodp">4.11</a>,
which displays leverage plots for the <code>body_mass_g</code> and <code>species</code>
predictors of the parallel lines model previously fit to the <code>penguins</code>
data. The leverage plot for <code>body_mass_g</code> has a moderate linear
relationship, so we expect <code>body_mass_g</code> to have moderate value in
explaining the behavior of <code>bill_length_mm</code> after accounting for
<code>species</code>. Similarly, the points in the leverage plot for <code>species</code> have
a moderately strong linear relationship, so we expect <code>species</code> to have
moderate value in explaining the behavior of <code>bill_length_mm</code> after
accounting for <code>body_mass_g</code>.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="interp-chapter.html#cb138-1" tabindex="-1"></a><span class="fu">leveragePlots</span>(lmodp)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:leverageplot-lmodp"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/leverageplot-lmodp-1.png" alt="Leverage plots for the predictors in the parallel lines model fit to the `penguins` data." width="672" />
<p class="caption">
Figure 4.11: Leverage plots for the predictors in the parallel lines model fit to the <code>penguins</code> data.
</p>
</div>
<p>We next examine the leverage plot for the separate lines model fit to
the <code>penguins</code> data. The fitted separate lines model is
<span class="math display">\[
\begin{aligned}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&amp;\quad + 0.001 D_C \mathtt{body\_mass\_g} + 0.0009 D_G \mathtt{body\_mass\_g},
\end{aligned}
\]</span>
which has 6 estimated coefficients. However, the fitted model has only
3 non-intercept terms. Recall the formula we fit for the separate lines
model:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="interp-chapter.html#cb139-1" tabindex="-1"></a><span class="co"># function call for separate lines model</span></span>
<span id="cb139-2"><a href="interp-chapter.html#cb139-2" tabindex="-1"></a><span class="fu">lm</span>(<span class="at">formula =</span> bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species <span class="sc">+</span> body_mass_g<span class="sc">:</span>species,</span>
<span id="cb139-3"><a href="interp-chapter.html#cb139-3" tabindex="-1"></a>   <span class="at">data =</span> penguins)</span></code></pre></div>
<p>Thus, we have terms for <code>body_mass_g</code>, <code>species</code>, and the interaction
term <code>body_mass_g:species</code>.</p>
<p>We use the code below to create the leverage plots shown in Figure
<a href="interp-chapter.html#fig:leverageplot-lmods">4.12</a>. The leverage plot for <code>body_mass_g</code> has a
moderate linear relationship, so we expect <code>body_mass_g</code> to have
moderate additional value in explaining the behavior of <code>bill_length_mm</code>
after accounting for <code>species</code> and the interaction term
<code>body_mass_g:species</code>. It is unlikely we would include the
<code>body_mass_g:species</code> term in our model prior to including <code>body_mass_g</code>
, so philosophically, this plot provides little useful information.
Similarly, interpreting the leverage plot for <code>species</code> has limited
utility because the leverage plot includes the influence of the
interaction term <code>body_mass_g:species</code>. We are unlikely to fit a model
that includes the interaction term without also including the <code>species</code>
term directly. Instead it makes more sense to judge the utility of
adding <code>species</code> to the model regressing <code>bill_length_mm</code> on
<code>body_mass_g</code> alone, which we already considered in Figure
<a href="interp-chapter.html#fig:leverageplot-lmodp">4.11</a>. Examining the leverage plot for the
interaction term <code>body_mass_g:species</code> , we see the points have only a
weak linear relationship. Thus, we expect limited utility in adding the
interaction term <code>body_mass_g:species</code> to the parallel lines regression
model that regresses <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>species</code>.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="interp-chapter.html#cb140-1" tabindex="-1"></a><span class="fu">leveragePlots</span>(lmods)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:leverageplot-lmods"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/leverageplot-lmods-1.png" alt="Leverage plots for the terms in the separate lines model fit to the `penguins` data." width="672" />
<p class="caption">
Figure 4.12: Leverage plots for the terms in the separate lines model fit to the <code>penguins</code> data.
</p>
</div>
</div>
<div id="going-deeper-1" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Going deeper<a href="interp-chapter.html#going-deeper-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="orthogonality" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Orthogonality<a href="interp-chapter.html#orthogonality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let
<span class="math display">\[
\mathbf{X}_{[j]}=[x_{1,j},\ldots,x_{n,j}]
\]</span>
denote the <span class="math inline">\(n\times 1\)</span>
column vector of observed values for column <span class="math inline">\(j\)</span> of <span class="math inline">\(\mathbf{X}\)</span>. (We can’t use the
notation <span class="math inline">\(\mathbf{x}_j\)</span> because that is the <span class="math inline">\(p\times 1\)</span> vector of
regressor values for the <span class="math inline">\(j\)</span>th observation).</p>
<p>Regressors
<span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> are <strong>orthogonal</strong> if
<span class="math inline">\(\mathbf{X}_{[j]}^T \mathbf{X}_{[k]}=0\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{1}_{n\times1}\)</span> denote an <span class="math inline">\(n\times 1\)</span> column vector of
1s. The definition of orthogonal vectors above implies that
<span class="math inline">\(\mathbf{X}_{[j]}\)</span> is orthogonal to <span class="math inline">\(\boldsymbol{1}_{n\times1}\)</span> if
<span class="math display">\[
\mathbf{X}_{[j]}^T \boldsymbol{1}_{n\times1} = \sum_{i=1}^n x_{i,j} = 0,
\]</span>
i.e., if the values in <span class="math inline">\(\mathbf{X}_{[j]}\)</span> sum to zero.</p>
<p>Let <span class="math inline">\(\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}\)</span> denote the sample mean
of <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and
<span class="math inline">\(\bar{\mathbf{x}}_j = \bar{x}_j \boldsymbol{1}_{n\times 1}\)</span> denote the
column vector that repeats <span class="math inline">\(\bar{x}_j\)</span> <span class="math inline">\(n\)</span> times.</p>
<p><strong>Centering</strong> <span class="math inline">\(\mathbf{X}_{[j]}\)</span> involves subtracting the sample mean of
<span class="math inline">\(\mathbf{X}_{[j]}\)</span> from <span class="math inline">\(\mathbf{X}_{[j]}\)</span>, i.e.,
<span class="math inline">\(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j\)</span>.</p>
<p>Regressors <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> are
<strong>uncorrelated</strong> if they are orthogonal after being centered, i.e., if
<span class="math display">\[
(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k)=0.
\]</span>
Note that the sample covariance between vectors <span class="math inline">\(\mathbf{X}_{[j]}\)</span>
and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> is
<span class="math display">\[
\begin{aligned}
\widehat{\mathrm{cov}}(\mathbf{X}_{[j]}, \mathbf{X}_{[k]}) &amp;= \frac{1}{n-1}\sum_{i=1}^n (x_{i,j} - \bar{x}_j)(x_{i,k} - \bar{x}_k) \\
&amp;= \frac{1}{n-1}(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k).
\end{aligned}
\]</span>
Thus, two centered regressors are orthogonal if their covariance is
zero.</p>
<p>It is a desirable to have orthogonal regressors in our fitted model
because they simplify estimating the relationship between the regressors
and the response. Specifically:</p>
<p><em>If a regressor is orthogonal to all other regressors (and the column of
1s) in a model, adding or removing the orthogonal regressor from our
model will not impact the estimated regression coefficients of the other
regressors.</em></p>
<p>Since most linear regression models include an intercept, we should
assess whether our regressors are orthogonal to other regressors and the
column of 1s.</p>
<p>We consider a simple example with <span class="math inline">\(n=5\)</span> observations to demonstrate how orthogonality of regressors impacts the estimated regression coefficients.</p>
<p>In the code below:</p>
<ul>
<li><code>y</code> is a vector of response values.</li>
<li><code>X1</code> is a column vector of regressor values.</li>
<li><code>X2</code> is a column vector of regressor values chosen to be orthogonal to <code>X1</code>.</li>
</ul>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="interp-chapter.html#cb141-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">9</span>)       <span class="co"># create an arbitrary response vector</span></span>
<span id="cb141-2"><a href="interp-chapter.html#cb141-2" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">7</span>)      <span class="co"># create regressor 1</span></span>
<span id="cb141-3"><a href="interp-chapter.html#cb141-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">5</span><span class="sc">/</span><span class="dv">7</span>)  <span class="co"># create regressor 2 to be orthogonal to X1</span></span></code></pre></div>
<p>Note that the <code>crossprod</code> function computes the cross product of two vectors or matrices, so that <code>crossprod(A, B)</code> computes <span class="math inline">\(\mathbf{A}^T B\)</span>, where the vectors or matrices must have the correct dimension for the multiplication to be performed.</p>
<p>The regressor vectors <code>X1</code> and <code>X2</code> are orthogonal since their cross product <span class="math inline">\(\mathbf{X}_{[1]}^T \mathbf{X}_{[2]}\)</span> (in R, <code>crossprod(X1, X2)</code>) equals zero, as shown in the code below.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="interp-chapter.html#cb142-1" tabindex="-1"></a><span class="co"># cross product is zero, so X1 and X2 are orthogonal</span></span>
<span id="cb142-2"><a href="interp-chapter.html#cb142-2" tabindex="-1"></a><span class="fu">crossprod</span>(X1, X2)</span>
<span id="cb142-3"><a href="interp-chapter.html#cb142-3" tabindex="-1"></a><span class="do">##      [,1]</span></span>
<span id="cb142-4"><a href="interp-chapter.html#cb142-4" tabindex="-1"></a><span class="do">## [1,]    0</span></span></code></pre></div>
<p>In the code below, we regress <code>y</code> on <code>X1</code> without an intercept (<code>lmod1</code>). The estimated coefficient for <code>X1</code> is <span class="math inline">\(\hat{\beta}_1=0.893\)</span>.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="interp-chapter.html#cb143-1" tabindex="-1"></a><span class="co"># y regressed on X1 without an intercept</span></span>
<span id="cb143-2"><a href="interp-chapter.html#cb143-2" tabindex="-1"></a>lmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb143-3"><a href="interp-chapter.html#cb143-3" tabindex="-1"></a><span class="fu">coef</span>(lmod1)</span>
<span id="cb143-4"><a href="interp-chapter.html#cb143-4" tabindex="-1"></a><span class="do">##       X1 </span></span>
<span id="cb143-5"><a href="interp-chapter.html#cb143-5" tabindex="-1"></a><span class="do">## 0.893401</span></span></code></pre></div>
<p>Next, we then regress <code>y</code> on <code>X1</code> and <code>X2</code> without an intercept (<code>lmod2</code>). The estimated coefficients for <code>X1</code> and <code>X2</code> are <span class="math inline">\(\hat{\beta}_1=0.893\)</span> and <span class="math inline">\(\hat{\beta}_2=0.221\)</span>, respectively. Because <code>X1</code> and <code>X2</code> are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for <code>X1</code> stays the
same in both models.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="interp-chapter.html#cb144-1" tabindex="-1"></a><span class="co"># y regressed on X1 and X2 without an intercept</span></span>
<span id="cb144-2"><a href="interp-chapter.html#cb144-2" tabindex="-1"></a>lmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb144-3"><a href="interp-chapter.html#cb144-3" tabindex="-1"></a><span class="fu">coef</span>(lmod2)</span>
<span id="cb144-4"><a href="interp-chapter.html#cb144-4" tabindex="-1"></a><span class="do">##        X1        X2 </span></span>
<span id="cb144-5"><a href="interp-chapter.html#cb144-5" tabindex="-1"></a><span class="do">## 0.8934010 0.2210526</span></span></code></pre></div>
<p>The previous models (<code>lmod1</code> and <code>lmod2</code>) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our <span class="math inline">\(\mathbf{X}\)</span> matrix, then the coefficients for the other regressors in
the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s.</p>
<p>We create a vector <code>ones</code> that is simply a column of 1s.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="interp-chapter.html#cb145-1" tabindex="-1"></a>ones <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">5</span>)   <span class="co"># column of 1s</span></span></code></pre></div>
<p>Neither <code>X1</code> nor <code>X2</code> is orthogonal with the column of ones. We compute the cross product between <code>ones</code> and the two regressors <code>X1</code> and <code>X2</code>. Since the cross products are not zero, <code>X1</code> and <code>X2</code> are not orthogonal to the
column of ones.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="interp-chapter.html#cb146-1" tabindex="-1"></a><span class="fu">crossprod</span>(ones, X1) <span class="co"># not zero, so not orthogonal</span></span>
<span id="cb146-2"><a href="interp-chapter.html#cb146-2" tabindex="-1"></a><span class="do">##      [,1]</span></span>
<span id="cb146-3"><a href="interp-chapter.html#cb146-3" tabindex="-1"></a><span class="do">## [1,]   31</span></span>
<span id="cb146-4"><a href="interp-chapter.html#cb146-4" tabindex="-1"></a><span class="fu">crossprod</span>(ones, X2) <span class="co"># not zero, so not orthogonal</span></span>
<span id="cb146-5"><a href="interp-chapter.html#cb146-5" tabindex="-1"></a><span class="do">##            [,1]</span></span>
<span id="cb146-6"><a href="interp-chapter.html#cb146-6" tabindex="-1"></a><span class="do">## [1,] -0.2857143</span></span></code></pre></div>
<p>We create <code>lmod3</code> by adding adding a column of ones to <code>lmod2</code> (i.e.,
we include the intercept in the model). The the coefficients for both
<code>X1</code> and <code>X2</code> change when going from <code>lmod2</code> to <code>lmod3</code> because these
regressors are not orthogonal to the column of 1s. Comparing the
coefficients <code>lmod2</code> above and <code>lmod3</code>, <span class="math inline">\(\hat{\beta}_1\)</span> changes from
<span class="math inline">\(0.893\)</span> to <span class="math inline">\(0.397\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> changes from <span class="math inline">\(0.221\)</span> to <span class="math inline">\(0.279\)</span>.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="interp-chapter.html#cb147-1" tabindex="-1"></a><span class="fu">coef</span>(lmod2) <span class="co"># coefficients for lmod2</span></span>
<span id="cb147-2"><a href="interp-chapter.html#cb147-2" tabindex="-1"></a><span class="do">##        X1        X2 </span></span>
<span id="cb147-3"><a href="interp-chapter.html#cb147-3" tabindex="-1"></a><span class="do">## 0.8934010 0.2210526</span></span>
<span id="cb147-4"><a href="interp-chapter.html#cb147-4" tabindex="-1"></a><span class="co"># y regressed on X1 and X2 with an intercept</span></span>
<span id="cb147-5"><a href="interp-chapter.html#cb147-5" tabindex="-1"></a>lmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X2)</span>
<span id="cb147-6"><a href="interp-chapter.html#cb147-6" tabindex="-1"></a><span class="fu">coef</span>(lmod3) <span class="co"># coefficients for lmod3</span></span>
<span id="cb147-7"><a href="interp-chapter.html#cb147-7" tabindex="-1"></a><span class="do">## (Intercept)          X1          X2 </span></span>
<span id="cb147-8"><a href="interp-chapter.html#cb147-8" tabindex="-1"></a><span class="do">##   3.1547101   0.3969746   0.2791657</span></span></code></pre></div>
<p>For orthogonality of our regressors to be most impactful, the model’s
regressors should be orthogonal to each other and the column of 1s. In
that context, adding or removing any of the regressors doesn’t impact
the estimated coefficients of the other regressors. In the code below,
we define centered regressors <code>X3</code> and <code>X4</code> to be uncorrelated, i.e.,
<code>X3</code> and <code>X4</code> have sample mean zero and are orthogonal to each other.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="interp-chapter.html#cb148-1" tabindex="-1"></a>X3 <span class="ot">&lt;-</span>  <span class="fu">c</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>) <span class="co"># sample mean is zero</span></span>
<span id="cb148-2"><a href="interp-chapter.html#cb148-2" tabindex="-1"></a>X4 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)  <span class="co"># sample mean is zero</span></span>
<span id="cb148-3"><a href="interp-chapter.html#cb148-3" tabindex="-1"></a><span class="fu">cov</span>(X3, X4)              <span class="co"># 0, so X3 and X4 are uncorrelated and orthogonal</span></span>
<span id="cb148-4"><a href="interp-chapter.html#cb148-4" tabindex="-1"></a><span class="do">## [1] 0</span></span></code></pre></div>
<p>If we fit linear regression models with any combination of <code>ones</code>, <code>X3</code>,
or <code>X4</code> as regressors, the associated regression coefficients will not
change. To demonstrate this, we consider all possible combinations of
the three variables in the models below. We do not run the code to save
space, but we summarize the results below.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="interp-chapter.html#cb149-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>))           <span class="co"># only column of 1s</span></span>
<span id="cb149-2"><a href="interp-chapter.html#cb149-2" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X3 <span class="sc">-</span> <span class="dv">1</span>))      <span class="co"># only X3</span></span>
<span id="cb149-3"><a href="interp-chapter.html#cb149-3" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X4 <span class="sc">-</span> <span class="dv">1</span>))      <span class="co"># only X4</span></span>
<span id="cb149-4"><a href="interp-chapter.html#cb149-4" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X3))          <span class="co"># 1s and X3</span></span>
<span id="cb149-5"><a href="interp-chapter.html#cb149-5" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X4))          <span class="co"># 1s and X4</span></span>
<span id="cb149-6"><a href="interp-chapter.html#cb149-6" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X3 <span class="sc">+</span> X4 <span class="sc">-</span> <span class="dv">1</span>)) <span class="co"># X3 and X4</span></span>
<span id="cb149-7"><a href="interp-chapter.html#cb149-7" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X3 <span class="sc">+</span> X4))     <span class="co"># 1s, X3, and X4</span></span></code></pre></div>
<p>We simply note that in each of the previous models, because all of the
regressors (and the column of 1s) are orthogonal to each other, adding
or removing any regressor doesn’t impact the estimated coefficients for
the other regressors in the model. Thus, the estimated coefficients were
<span class="math inline">\(\hat{\beta}_{0}=5.6\)</span>, <span class="math inline">\(\hat{\beta}_{3}=1.0\)</span>, <span class="math inline">\(\hat{\beta}_{4}=-0.5\)</span>
when the relevant regressor was included in the model.</p>
<p>The easiest way to determine which vectors are orthogonal to each other
and the intercept is to compute the cross product of the <span class="math inline">\(\mathbf{X}\)</span>
matrix for the largest set of regressors we are considering. Consider
the matrix of cross products for the columns of 1s, <code>X1</code>, <code>X2</code>, <code>X3</code>, and
<code>X4</code>.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="interp-chapter.html#cb150-1" tabindex="-1"></a><span class="fu">crossprod</span>(<span class="fu">model.matrix</span>(<span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3 <span class="sc">+</span> X4))</span>
<span id="cb150-2"><a href="interp-chapter.html#cb150-2" tabindex="-1"></a><span class="do">##             (Intercept)  X1         X2 X3        X4</span></span>
<span id="cb150-3"><a href="interp-chapter.html#cb150-3" tabindex="-1"></a><span class="do">## (Intercept)   5.0000000  31 -0.2857143  0 0.0000000</span></span>
<span id="cb150-4"><a href="interp-chapter.html#cb150-4" tabindex="-1"></a><span class="do">## X1           31.0000000 197  0.0000000  0 0.0000000</span></span>
<span id="cb150-5"><a href="interp-chapter.html#cb150-5" tabindex="-1"></a><span class="do">## X2           -0.2857143   0 15.5102041 -5 0.2857143</span></span>
<span id="cb150-6"><a href="interp-chapter.html#cb150-6" tabindex="-1"></a><span class="do">## X3            0.0000000   0 -5.0000000  2 0.0000000</span></span>
<span id="cb150-7"><a href="interp-chapter.html#cb150-7" tabindex="-1"></a><span class="do">## X4            0.0000000   0  0.2857143  0 2.0000000</span></span></code></pre></div>
<p>Consider the sequence of models below.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="interp-chapter.html#cb151-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>))</span>
<span id="cb151-2"><a href="interp-chapter.html#cb151-2" tabindex="-1"></a><span class="do">## (Intercept) </span></span>
<span id="cb151-3"><a href="interp-chapter.html#cb151-3" tabindex="-1"></a><span class="do">##         5.6</span></span></code></pre></div>
<p>The model with only an intercept has an estimated coefficient of
<span class="math inline">\(\hat{\beta}_{0}=5.6\)</span>. If we add the <code>X1</code> to the model with an
intercept, then the intercept coefficient changes because the column of 1s isn’t orthogonal to <code>X1</code>.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="interp-chapter.html#cb152-1" tabindex="-1"></a>lmod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X1) <span class="co"># model with 1s and X1</span></span>
<span id="cb152-2"><a href="interp-chapter.html#cb152-2" tabindex="-1"></a><span class="fu">coef</span>(lmod4)</span>
<span id="cb152-3"><a href="interp-chapter.html#cb152-3" tabindex="-1"></a><span class="do">## (Intercept)          X1 </span></span>
<span id="cb152-4"><a href="interp-chapter.html#cb152-4" tabindex="-1"></a><span class="do">##         2.5         0.5</span></span></code></pre></div>
<p>If we add <code>X2</code> to <code>lmod4</code>, we might think that only <span class="math inline">\(\hat{\beta}_{0}\)</span>
will change because <code>X1</code> and <code>X2</code> are orthogonal to each other. However,
because <code>X2</code> is not orthogonal to all of the other regressors in the
model (<code>X1</code> and the column of 1s), both <span class="math inline">\(\hat{\beta}_{0}\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span> will change. The easiest way to realize this is to look
at <code>lmod2</code> above with only <code>X1</code> and <code>X2</code>. When we add the column of 1s
to <code>lmod2</code>, both <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> will change because
neither regressor is orthogonal to the column of 1s needed to include
the intercept term.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="interp-chapter.html#cb153-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X2))</span>
<span id="cb153-2"><a href="interp-chapter.html#cb153-2" tabindex="-1"></a><span class="do">## (Intercept)          X1          X2 </span></span>
<span id="cb153-3"><a href="interp-chapter.html#cb153-3" tabindex="-1"></a><span class="do">##   3.1547101   0.3969746   0.2791657</span></span></code></pre></div>
<p>However, note that <code>X3</code> is orthogonal to the column of 1s and <code>X1</code>.
Thus, if we add <code>X3</code> to <code>lmod4</code>, which includes both a column of 1s and
<code>X1</code>, <code>X3</code> will not change the estimated coefficients for the intercept
or <code>X1</code>.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="interp-chapter.html#cb154-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X3))</span>
<span id="cb154-2"><a href="interp-chapter.html#cb154-2" tabindex="-1"></a><span class="do">## (Intercept)          X1          X3 </span></span>
<span id="cb154-3"><a href="interp-chapter.html#cb154-3" tabindex="-1"></a><span class="do">##         2.5         0.5         1.0</span></span></code></pre></div>
<p>Additionally, since <code>X4</code> is orthogonal to the column of 1s, <code>X1</code>, and
<code>X3</code>, adding <code>X4</code> to the previous model will not change the estimated
coefficients for any of the other variables already in the model.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="interp-chapter.html#cb155-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X3 <span class="sc">+</span> X4))</span>
<span id="cb155-2"><a href="interp-chapter.html#cb155-2" tabindex="-1"></a><span class="do">## (Intercept)          X1          X3          X4 </span></span>
<span id="cb155-3"><a href="interp-chapter.html#cb155-3" tabindex="-1"></a><span class="do">##         2.5         0.5         1.0        -0.5</span></span></code></pre></div>
<p>Lastly, if we can partition our <span class="math inline">\(\mathbf{X}\)</span> matrix such that
<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is a block diagonal matrix, then none of the
blocks of variables will affect the estimated coefficients of the other
variables.</p>
<p>Define a new regressor <code>X5</code> below.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="interp-chapter.html#cb156-1" tabindex="-1"></a>X5 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># orthogonal to ones, X1, not X4</span></span></code></pre></div>
<p><code>X5</code> is orthogonal to the column of
1s and <code>X1</code>, but not <code>X4</code>, as evidenced in the code below.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="interp-chapter.html#cb157-1" tabindex="-1"></a><span class="co"># note block of 0s</span></span>
<span id="cb157-2"><a href="interp-chapter.html#cb157-2" tabindex="-1"></a><span class="fu">crossprod</span>(<span class="fu">cbind</span>(ones, X1, X4, X5))</span>
<span id="cb157-3"><a href="interp-chapter.html#cb157-3" tabindex="-1"></a><span class="do">##      ones  X1 X4 X5</span></span>
<span id="cb157-4"><a href="interp-chapter.html#cb157-4" tabindex="-1"></a><span class="do">## ones    5  31  0  0</span></span>
<span id="cb157-5"><a href="interp-chapter.html#cb157-5" tabindex="-1"></a><span class="do">## X1     31 197  0  0</span></span>
<span id="cb157-6"><a href="interp-chapter.html#cb157-6" tabindex="-1"></a><span class="do">## X4      0   0  2 -1</span></span>
<span id="cb157-7"><a href="interp-chapter.html#cb157-7" tabindex="-1"></a><span class="do">## X5      0   0 -1  2</span></span></code></pre></div>
<p>Note the block of zeros in the lower left and upper right corners of the
cross product matrix above. The block containing <code>ones</code> and <code>X1</code> is
orthogonal to the block containing <code>X4</code> and <code>X5</code>. This means that if we
fit the model with only the column of 1s and <code>X1</code>, the model only with
<code>X4</code> and <code>X5</code>, and then fit the model with the column of 1s, <code>X1</code>, <code>X4</code>,
and <code>X5</code>, then the coefficients <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>
are not impacted when <code>X4</code> and <code>X5</code> are added to the model. Similarly,
<span class="math inline">\(\hat{\beta}_{4}\)</span> and <span class="math inline">\(\hat{\beta}_{5}\)</span> are not impacted when the column
of 1s and <code>X1</code> are added to the model with <code>X4</code> and <code>X5</code>. See the output below.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="interp-chapter.html#cb158-1" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> X1)           <span class="co"># model with 1s and X1</span></span>
<span id="cb158-2"><a href="interp-chapter.html#cb158-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-3"><a href="interp-chapter.html#cb158-3" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb158-4"><a href="interp-chapter.html#cb158-4" tabindex="-1"></a><span class="do">## lm(formula = y ~ X1)</span></span>
<span id="cb158-5"><a href="interp-chapter.html#cb158-5" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-6"><a href="interp-chapter.html#cb158-6" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb158-7"><a href="interp-chapter.html#cb158-7" tabindex="-1"></a><span class="do">## (Intercept)           X1  </span></span>
<span id="cb158-8"><a href="interp-chapter.html#cb158-8" tabindex="-1"></a><span class="do">##         2.5          0.5</span></span>
<span id="cb158-9"><a href="interp-chapter.html#cb158-9" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> X4 <span class="sc">+</span> X5 <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># model with X4 and X5 only</span></span>
<span id="cb158-10"><a href="interp-chapter.html#cb158-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-11"><a href="interp-chapter.html#cb158-11" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb158-12"><a href="interp-chapter.html#cb158-12" tabindex="-1"></a><span class="do">## lm(formula = y ~ X4 + X5 - 1)</span></span>
<span id="cb158-13"><a href="interp-chapter.html#cb158-13" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-14"><a href="interp-chapter.html#cb158-14" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb158-15"><a href="interp-chapter.html#cb158-15" tabindex="-1"></a><span class="do">## X4  X5  </span></span>
<span id="cb158-16"><a href="interp-chapter.html#cb158-16" tabindex="-1"></a><span class="do">## -3  -5</span></span>
<span id="cb158-17"><a href="interp-chapter.html#cb158-17" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> X1 <span class="sc">+</span> X4 <span class="sc">+</span> X5) <span class="co"># model with 1s, X1, X4, X5</span></span>
<span id="cb158-18"><a href="interp-chapter.html#cb158-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-19"><a href="interp-chapter.html#cb158-19" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb158-20"><a href="interp-chapter.html#cb158-20" tabindex="-1"></a><span class="do">## lm(formula = y ~ X1 + X4 + X5)</span></span>
<span id="cb158-21"><a href="interp-chapter.html#cb158-21" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-22"><a href="interp-chapter.html#cb158-22" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb158-23"><a href="interp-chapter.html#cb158-23" tabindex="-1"></a><span class="do">## (Intercept)           X1           X4           X5  </span></span>
<span id="cb158-24"><a href="interp-chapter.html#cb158-24" tabindex="-1"></a><span class="do">##         2.5          0.5         -3.0         -5.0</span></span></code></pre></div>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-lmwr2" class="csl-entry">
Faraway, Julian J. 2014. <em>Linear Models with <span>R</span>, 2nd Edition</em>. Chapman; Hall/CRC.
</div>
<div id="ref-fox2020predictor" class="csl-entry">
Fox, John, and Sanford Weisberg. 2020. <span>“Predictor Effects Graphics Gallery.”</span> <a href="https://cran.r-project.org/web/packages/effects/vignettes/predictor-effects-gallery.pdf">https://cran.r-project.org/web/packages/effects/vignettes/predictor-effects-gallery.pdf</a>.
</div>
<div id="ref-R-car" class="csl-entry">
Fox, John, Sanford Weisberg, and Brad Price. 2024. <em>Car: Companion to Applied Regression</em>. <a href="https://r-forge.r-project.org/projects/car/">https://r-forge.r-project.org/projects/car/</a>.
</div>
<div id="ref-R-effects" class="csl-entry">
Fox, John, Sanford Weisberg, Brad Price, Michael Friendly, and Jangman Hong. 2022. <em>Effects: Effect Displays for Linear, Generalized Linear, and Other Models</em>. <a href="https://www.r-project.org">https://www.r-project.org</a>.
</div>
<div id="ref-mt1977" class="csl-entry">
Mosteller, Frederick, and John W Tukey. 1977. <em>Data Analysis and Regression. A Second Course in Statistics</em>. <em>Addison-Wesley, Reading, MA</em>.
</div>
<div id="ref-sall1990leverage" class="csl-entry">
Sall, John. 1990. <span>“Leverage Plots for General Linear Hypotheses.”</span> <em>The American Statistician</em> 44 (4): 308–15.
</div>
<div id="ref-sheather2009modern" class="csl-entry">
Sheather, Simon. 2009. <em>A Modern Approach to Regression with <span>R</span></em>. Springer, New York.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-model-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-model-theory.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["A-Progessive-Introduction-to-Linear-Models.pdf", "A-Progessive-Introduction-to-Linear-Models.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
