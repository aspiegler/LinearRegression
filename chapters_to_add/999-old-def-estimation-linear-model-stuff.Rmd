---
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# Defining and fitting a linear model

In this chapter, we define a linear model and discuss the basic estimation of its parameters. We leave discussion of more theoretical aspects of the model to subsequent chapters.

## Background and terminology

Regression models are used to model the relationship between:

* one or more **response** variables and
* one or more **predictor** variables.

The distinction between these two types variables is their purpose in the model.

* Predictor variables are used to predict the value of the response variable.

Response variables are also known as **outcome**, **output**, or **dependent** variables.

Predictor variables are also known as **explanatory**, **regressor**, **input**, **dependent**, or **feature** variables.

Note:  Because the variables in our model are often interrelated, describing these variables as independent or dependent variables is vague and is best avoided.

A distinction is sometimes made between **regression models** and **classification models**. In that case:

* Regression models attempt to predict a numerical response.
* Classification models attempt to predict the category level a response will have.

## Goals of regression

The basic goals of a regression model are to:

1. *Predict* future or unknown response values based on specified values of the predictors.
    * What will the selling price of a home be?
2. *Describe* relationships (associations) between predictor variables and the response.
    * What is the general relationship between the selling price of a home and the number of bedrooms the home has?

With our regression model, we also hope to be able to:

1. *Generalize* our results from the sample to the a larger population of interest.
    * E.g., we want to extend our results from a small set of college students to all college students.
2. *Infer causality* between our predictors and the response.
    * E.g., if we give a person a vaccine, then this causes the person's risk of catching the disease to decrease.

**A "true model" doesn't exist for real data**. The data-generating process is far more complex than the models we can realistically fit to the data. Thus, finding the true model should not be the goal of a regression analysis. A regression analysis should attempt to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.)

<!-- ## Regression for Pearson's height data -->

<!-- @wachsmuth_et_al2003 compiled child and parent height data from English familes tabulated by @pearson1897 and @pearson_lee1903. The data are available in the `PearsonLee` data set in the **HistData** package [@R-HistData]. The `PearsonLee` data frame includes the variables: -->

<!-- * `child`: child height (inches). -->
<!-- * `parent`: parent height (inches). -->
<!-- * `gp`: a factor with levels `fd` (father/daughter), `fs` (father/son), `md` (mother/daughter), `ms` (mother/son) indicating the parent/child relationship. -->
<!-- * `par`: a factor with levels `Father`, `Mother` indicating the parent measured. -->
<!-- * `chl`: a factor with levels `Daughter`, `Son` indicating the child's relationship to the parent. -->

<!-- It is natural to wonder whether the height of a parent could explain the height of their child. We can consider a regression analysis that regresses child's height (the response variable) on parent's height (the predictor variable). The additional variables `gp`, `par`, and `chl` could also be used as predictor variables in our analysis. We perform an informal (linear) regression analysis visually  using **ggplot2** [@R-ggplot2].  -->

<!-- Consider a plot of child's height versus parent's height. -->

<!-- ```{r} -->
<!-- data(PearsonLee, package = "HistData") # load data -->
<!-- library(ggplot2) # load ggplot2 package -->
<!-- # create ggplot object for repeated use -->
<!-- # we'll be using common aesthetics across multiple geometries -->
<!-- # so we put them in the ggplot function -->
<!-- # also improve the x, y labels -->
<!-- ggheight <- ggplot(data = PearsonLee,  -->
<!--                     mapping = aes(x = parent, y = child)) +  -->
<!--              xlab("parent height (in)") + ylab("child height (in)") -->
<!-- ggheight + geom_point() # scatter plot of child vs parent height -->
<!-- ``` -->

<!-- We see a positive linear association between parent height and child height: as the height of the parent increases, the height of the child also tends to increase. -->

<!-- A simple linear regression model describes the relationship between a response and a predictor variable using the "best fitting" straight line (we'll formalize what best means later). We add the estimated simple linear regression model to our previous plot below using the `geom_smooth` function. The line fits reasonably well. -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add estimated line -->
<!-- ``` -->

<!-- We may also wonder whether the type of parent (father/mother) or child (daughter/son) affects the relationship. We facet our scatter plots based on the `par` and `chl` variables below. While the overall patterns are similar, we notice that Father heights tend to be larger than Mother heights and Son heights tend to be larger than Daughter heights.  -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   facet_grid(par ~ chl) # facet the data by parent/child type -->
<!-- ``` -->

<!-- Having seen the previous graphic, we may wonder whether we can better model the relationship between parent and child height by accounting for which parent and child were measured. An interaction model assumes that the intercept and slope of each combination of parent/child is the different. We fit and plot an interaction model below. -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + facet_grid(par ~ chl) +   -->
<!--        geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add interaction  model to data -->
<!-- ``` -->

<!-- Other questions we could explore are whether the slopes across the different parent/child combinations are the same, whether the variability of the data is constant as parent height changes, predicting heights outside the range of the observed data, the precision of our estimated model, etc. -->

<!-- Regression analysis will generally be much more complex that was is presented above, but this example hopefully gives you an idea of the kinds of questions regression analysis can help you answer.  -->


<!-- ## Scatter plots and linear regression -->

  <!-- Scatter plots are a convenient way to study the potential relationship between a single response and a single predictor variable. -->

  <!-- ### Height inheritability -->

  <!-- Karl Pearson (1857-1936) organized the collection of $n=1375$ heights of mothers in the United Kingdom under the age of 65 and one of their adult daughters over the age of 18.  These data are available in the `Heights` data set in the **alr4** package. We are interested in the inheritance from the mother to the daughter, so the mother's height (`mheight`) is used as the predictor variable and the daughter's height (`dheight`) is used as the response variable. -->

  <!-- Questions of interest: -->

  <!-- * Do taller mothers tend to have taller daughters -->
  <!-- * Do shorter mothers tend to have shorter daughters? -->

  <!-- ```{r} -->
  <!-- data(Heights, package = "alr4") -->
  <!-- str(Heights) -->
  <!-- plot(dheight ~ mheight, data = Heights, -->
              <!--      xlab = "mother's height (in)", -->
              <!--      ylab = "daughter's height (in)", -->
              <!--      xlim = c(55, 75), ylim = c(55, 75)) -->
  <!-- ``` -->
  <!-- There seems to be a clear trend between mother's heights and daughter's heights. The taller the mother, the taller the daughter *tends* to be. -->

  <!-- ### Predicting snowfall -->

  <!-- The `ftcollinssnow` data set in the **alr4** package measures late (September 1st until December 31st) and early (January 1st to June 30th) season snowfall for Fort Collins, CO between Late 1900 and Early 1993. -->

  <!-- Question of interest: Can late season snowfall predict snowfall in the early part of the next year? -->


  <!-- ```{r} -->
  <!-- data("ftcollinssnow", package = "alr4") # load data -->
<!-- str(ftcollinssnow) # examine structure -->
<!-- plot(Late ~ Early, data = ftcollinssnow) # plot data -->
<!-- # add "line of best fit" -->
  <!-- abline(lm(Late ~ Early, data = ftcollinssnow), lty = 2) -->
  <!-- # sample mean line -->
  <!-- abline(mean(ftcollinssnow$Late), 0) -->
  <!-- ``` -->

  <!-- A plot of the snowfall data for the two time periods suggest that this relationship is weak or they may be uncorrelated. -->
  <!-- * The dashed line indicates the "linear of best fit" , while the solid line indicates the average of the Late snowfall. -->

  <!-- ### Turkey growth -->
  <!-- Pens of turkeys were fed the same diet, except that each pen was supplemented with a `Dose` of amino acid methionine as a percentage of the total diet of the birds. The amino acid methionine was provided using three different `Source`s (one standard and two experimental). The `Weight` gain (g) of the turkeys was measured. These data are available in the `turkey` data in the **alr4** package. -->

  <!-- Questions of interest: -->

  <!-- * Is there a relationship between weight gain of the turkeys and the dose amount? If so, is the relationship linear? -->
  <!-- * Does the source of the methionine impact the weight gain of the turkeys? -->

  <!-- Consider a plot of the average `Weight` gain (g) of the turkeys as a function of the `Dose` amount (% of diet), separating the groups by the `Source` of the methionine. -->

  <!-- ```{r} -->
  <!-- data(turkey, package = "alr4") -->
  <!-- str(turkey) -->
  <!-- summary(turkey) # the source factor (S) is not a factor -->
<!-- turkey$S = factor(turkey$S) -->
  <!-- levels(turkey$S) <- c("control", "new source a", "new source b") -->
  <!-- names(turkey) <- c("Dose", "Gain", "Source", "Replications", "SD") # rename variables -->
<!-- # create turkey data ggplot -->
  <!-- library(ggplot2) # load ggplot2 package -->
<!-- gg_turkey <- ggplot(turkey, -->
                           <!--                     mapping = aes(x = Dose, y = Gain, -->
                                                                    <!--                                   color = Source, shape = Source)) -->
  <!-- gg_turkey + geom_point() + geom_line() -->
  <!-- ``` -->

  <!-- Weight gain increases with dose amount, but doesn't appear to be linear. -->

<!-- The amino acid source may slightly affect the growth trajectory of the turkeys. -->

<!-- An alternative version of the previous plot using the **lattice** package -->

<!-- ```{r} -->
<!-- library(lattice) # load lattice package -->
<!-- xyplot(Gain ~ Dose, data = turkey, groups = Source, -->
<!--        auto.key = TRUE, type = "b") -->
<!-- ``` -->

### Visualizing the RSS as a function of the estimated coefficients

As we have attempted to emphasize through its notation,
$RSS(\hat{\beta}_0, \hat{\beta}_1)$ is a function of $\hat{\beta}_0$ and
$\hat{\beta}_1$. OLS estimation for the simple linear regression model
seeks to find the values of the estimated coefficients that minimize the
$RSS(\hat{\beta}_0, \hat{\beta}_1)$. In the example below, we visualize
this three-dimensional surface to see how difficult it would be to
optimize the RSS computationally .

Consider the Pearson and Lee's height data (`PearsonLee` in the
**HistData** package) previously discussed. For that data set, we tried
to model the child's height (`child`) based on the height of the child's
parents (`parent`). Thus, our response variable is `child` and our
predictor variable is `parent`. We seek to estimate the regression
equation
\[
E(\mathtt{child} \mid \mathtt{parent}) = \beta_0 + \beta_1 \mathtt{parent}
\]
with the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the
associated RSS.

We first load the height data, extract the response and predictor and
assign them the names `y` and `x`.

```{r}
# load height data
data(PearsonLee, package = "HistData")
# extract response and predictor variables from data set
y <- PearsonLee$child
x <- PearsonLee$parent
```

We now create a function that computes the RSS as a function of
$\hat{\beta}_0$ and $\hat{\beta}_1$ (called `b0` and `b1`, respectively
in the code below). The function takes the vector `b = c(b0, b1)`,
extracts `b0` and `b1` from this vector, computes the fitted values
(`yhat`) for the provided `b0` and `b1`, computes the corresponding
residuals (`ehat`), and the returns the sum of the squared residuals,
i.e., the RSS.

```{r}
# function to compute the RSS
# b = c(b0, b1)
compute_rss <- function(b) {
  b0 = b[1] # extract b0 from b
  b1 = b[2] # extract b1 from b
  yhat <- b0 + b1 * x # compute fitted values
  ehat <- y - yhat # compute residuals
  return(sum(ehat^2)) # return RSS
}
```

Next, we specify sequences of `b0` and `b1` values to consider for
optimizing the RSS. We create a matrix, `rss_mat` to store the computed
RSS for each combination of `b0` and `b1`. We then use a double `for`
loop to evaluate the RSS for each combination of `b0` and `b1` in our
sequences.

```{r}
# sequences of candidate b0 and b1 values
b0_seq <- seq(41.06, 41.08, len = 101)
b1_seq <- seq(0.383, 0.385, len = 101)
# matrix to store rss values
rss_mat <- matrix(nrow = length(b0_seq), ncol = length(b1_seq))
# use double loop to compute RSS for all combinations of b0_seq and b1_seq
# seq_along(b0_seq) returns the vector 1:length(b0_seq), but is safer 
for (i in seq_along(b0_seq)) {
  for (j in seq_along(b1_seq)) {
    rss_mat[i, j] <- compute_rss(c(b0_seq[i], b1_seq[j]))
  }
}
```

We draw a contour plot of the RSS surface using the `contour` function.

```{r}
# draw a contour plot of the RSS surface
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = "b0", ylab = "b1")
title("RSS surface of Pearson and Lee height data")
```

A contour plot uses contour lines to describe the height of the $z$
dimension of a 3-dimensional $(x, y, z)$ surface. Each line/contour
indicates the height of the surface along that line. Note that in the
graphic above, the contours are basically straight lines. There's no
easily identifiable combinations of `b0` and `b1` the produce the
minimum RSS.

We can approximate the optimal values of `b0` and `b1` that minimize the
RSS through the `optim` function. The optim function takes two main
arguments:

-   `par`: a vector of starting values for the optimization algorithm.
    In our case, this will be the starting values for `b0` and `b1`.
-   `fn`: a function of `par` to minimize.

The `optim` function will return a list with several pieces of
information (see `?stats::optim`) for details. We want the `par`
component of the returned list, which is the `par` vector that
(approximately) minimizes `fn`. We then use the `points` function to
plot the "optimal" values of `b0` and `b1` that minimize the RSS.

```{r}
# use the optim function to find the values of b0 and b1 that minimize the RSS
# par is the vector of initial values
# fn is the function to minimize
# $par extracts the values found by optim to minimize fn
optimal_b <- optim(par = c(41, 0.4), fn = compute_rss)$par
# print the optimal values of b
optimal_b
# plot optimal value as an X on the contour plot
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = "b0", ylab = "b1")
title("RSS surface of Pearson and Lee height data")
points(x = optimal_b[1], y = optimal_b[2], pch = 4)
```

What is our takeaway from this example? It's probably not ideal to
numerically search for the values of $\hat{\beta}_0$ and $\hat{\beta}_1$
that minimize $RSS(\hat{\beta}_0$, $\hat{\beta}_1)$. Instead, we should
seek an exact solution using mathematics.