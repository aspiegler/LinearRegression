{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix A - Matrix Facts\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click or scan the QR code below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/a-matrix-facts.ipynb\"> <img src=\"https://raw.githubusercontent.com/jfrench/LinearRegression/88c43eee5f12de29b6cb6e0f19ae98ce582a30bb/images/qr-matrix-facts.svg\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Overview of matrix definitions, properties, algebra, etc.\n",
        "\n",
        "In this chapter we provide an overview of vectors, matrices, and matrix operations that are useful for data modeling (though their application will not be discussed here).\n",
        "\n",
        "A **matrix** is a two-dimensional array of values, symbols, or other objects (depending on the context). We will assume that our matrices contain numbers or random variables. Context will make it clear which is being represented.\n",
        "\n",
        "## Notation\n",
        "\n",
        "Matrices are commonly denoted by bold capital letters like $\\mathbf{A}$ or $\\mathbf{B}$, but this will sometimes be simplified to capital letters like $A$ or $B$. A matrix $\\mathbf{A}$ with $m$ rows and $n$ columns (an $m\\times n$ matrix) will be denoted as\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \n",
        "\\begin{bmatrix}\n",
        "\\mathbf{A}_{1,1} & \\mathbf{A}_{2,1} & \\cdots & \\mathbf{A}_{1,n} \\\\\n",
        "\\mathbf{A}_{2,1} & \\mathbf{A}_{2,1} & \\cdots & \\mathbf{A}_{2,n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\mathbf{A}_{m,1} & \\mathbf{A}_{m,2} & \\cdots & \\mathbf{A}_{m,n} \\\\\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "where $\\mathbf{A}_{i,j}$ denotes the element in row $i$ and column $j$ of matrix $\\mathbf{A}$.\n",
        "\n",
        "A **column vector** is a matrix with a single column. A **row vector** is a matrix with a single row.\n",
        "\n",
        "-   Vectors are commonly denoted with bold lowercase letters such as $\\mathbf{a}$ or $\\mathbf{b}$, but this may be simplified to lowercase letters such as $a$ or $b$.\n",
        "\n",
        "A $p\\times 1$ column vector $\\mathbf{a}$ may constructed as $$\n",
        "\\mathbf{a} = [a_1, a_2, \\ldots, a_p] = \n",
        "\\begin{bmatrix}\n",
        "a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "A vector is assumed to be a column vector unless otherwise indicated.\n",
        "\n",
        "## Basic mathematical operations\n",
        "\n",
        "### Addition and subtraction\n",
        "\n",
        "Consider matrices $\\mathbf{A}$ and $\\mathbf{B}$ with identical sizes $m\\times n$.\n",
        "\n",
        "We add $\\mathbf{A}$ and $\\mathbf{B}$ by adding the element in position $i,j$ of $\\mathbf{B}$ with the element in position $i,j$ of $A$, i.e.,\n",
        "\n",
        "$$\n",
        "(\\mathbf{A} + \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} + \\mathbf{B}_{i,j}.\n",
        "$$\n",
        "\n",
        "Similarly, if we subtract $\\mathbf{B}$ from matrix $\\mathbf{A}$, then we subtract the element in position $i,j$ of $\\mathbf{B}$ from the element in position $i,j$ of $\\mathbf{A}$, i.e.,\n",
        "\n",
        "$$\n",
        "(\\mathbf{A} - \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} - \\mathbf{B}_{i,j}.\n",
        "$$\n",
        "\n",
        "Example:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "\\end{bmatrix} + \n",
        "\\begin{bmatrix}\n",
        "2 & 9 & 1 \\\\\n",
        "1 & 3 & 1 \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "3 & 11 & 4 \\\\\n",
        "5 & 8 & 7 \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "\\end{bmatrix} - \n",
        "\\begin{bmatrix}\n",
        "2 & 9 & 1 \\\\\n",
        "1 & 3 & 1 \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "-1 & -7 & 2 \\\\\n",
        "3 & 2 & 5 \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Scalar multiplication\n",
        "\n",
        "A matrix multiplied by a scalar value $c\\in\\mathbb{R}$ is the matrix obtained by multiplying each element of the matrix by $c$. If $\\mathbf{A}$ is a matrix and $c\\in \\mathbb{R}$, then $$\n",
        "(c\\mathbf{A})_{i,j} = c\\mathbf{A}_{i,j}.\n",
        "$$ Example: $$\n",
        "3\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "3\\cdot 1 & 3\\cdot 2 & 3\\cdot 3 \\\\\n",
        "3\\cdot 4 & 3\\cdot 5 & 3\\cdot 6 \\\\\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "3 & 6 & 9 \\\\\n",
        "12 & 15 & 18 \\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Matrix multiplication\n",
        "\n",
        "Consider two matrices $\\mathbf{A}$ and $\\mathbf{B}$.\n",
        "\n",
        "The matrix product $\\mathbf{AB}$ is only defined if the number of columns in $\\mathbf{A}$ matches the number of rows in $\\mathbf{B}$.\n",
        "\n",
        "Assume $\\mathbf{A}$ is an $m\\times n$ matrix and $\\mathbf{B}$ is an $n\\times p$ matrix. $\\mathbf{AB}$ will be an $m\\times p$ matrix and\n",
        "\n",
        "$$\n",
        "(\\mathbf{AB})_{i,j} = \\sum_{k=1}^{n} \\mathbf{A}_{i,k}\\mathbf{B}_{k,j}.\n",
        "$$\n",
        "\n",
        "Example: $$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 4\\\\\n",
        "2 & 5\\\\\n",
        "3 & 6\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "1\\cdot 1 +  2 \\cdot 2 + 3 \\cdot 3 & 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6\\\\\n",
        "4\\cdot 1 +  5 \\cdot 2 + 6 \\cdot 3 & 4 \\cdot 4 + 5 \\cdot 5 + 6 \\cdot 6\\\\\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "14 & 32\\\\\n",
        "32 & 77\\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Transpose\n",
        "\n",
        "The **transpose** of a matrix $\\mathbf{A}$, denoted $\\mathbf{A}^T$, exchanges the rows and columns of the matrix. More formally, the $i,j$ element of $\\mathbf{A}^T$ is the $j,i$ element of $\\mathbf{A}$, i.e., $(\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i}$.\n",
        "\n",
        "Example: $$\n",
        "\\begin{bmatrix}\n",
        "2 & 9 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}^T =\n",
        "\\begin{bmatrix}\n",
        "2 & 4\\\\\n",
        "9 & 5\\\\\n",
        "3 & 6\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "## Basic mathematical properties\n",
        "\n",
        "### Associative property\n",
        "\n",
        "Addition and multiplication satisfy the associative property for matrices. Assuming that the matrices $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$ have the sizes required to do the operations below, then\n",
        "\n",
        "$$\n",
        "(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\n",
        "$$ and $$\n",
        "(\\mathbf{AB})\\mathbf{C}=\\mathbf{A}(\\mathbf{BC}).\n",
        "$$\n",
        "\n",
        "### Distributive property\n",
        "\n",
        "Matrix operations satisfy the distributive property. Assuming that the matrices $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$ have the sizes required to do the operations below, then\n",
        "\n",
        "$$\n",
        "\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} + \\mathbf{AC}\\quad\\mathrm{and}\\quad (\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}.\n",
        "$$\n",
        "\n",
        "### No commutative property\n",
        "\n",
        "In general, matrix multiplication does not satisfy the commutative property, i.e., $$\n",
        "\\mathbf{AB} \\neq \\mathbf{BA},\n",
        "$$ even when the matrix sizes allow the operation to be performed.\n",
        "\n",
        "Example:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "5\n",
        "\\end{bmatrix}\n",
        "$$ while $$\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 2\\\\\n",
        "2 & 4\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Transpose-related properties\n",
        "\n",
        "Assume that the matrices $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$ have the sizes required to perform the operations below. Additionally, assume that $c\\in \\mathbb{R}$ is a scalar constant.\n",
        "\n",
        "The following properties are true:\n",
        "\n",
        "-   $c^T = c$.\n",
        "-   $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T$.\n",
        "-   $(\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T$, which can be extended to $(\\mathbf{ABC})^T=\\mathbf{C}^T \\mathbf{B}^T \\mathbf{A}^T$, etc.\n",
        "-   $(\\mathbf{A}^T)^T=\\mathbf{A}$.\n",
        "\n",
        "## Special matrices\n",
        "\n",
        "### Square matrices\n",
        "\n",
        "A matrix is **square** if the number of rows equals the number of columns.\n",
        "\n",
        "The **diagonal elements** of an $n\\times n$ square matrix $\\mathbf{A}$ are the elements $\\mathbf{A}_{i,i}$ for $i = 1, 2, \\ldots, n$. Any non-diagonal elements of $\\mathbf{A}$ are called **off-diagonal** elements.\n",
        "\n",
        "### Identity matrix\n",
        "\n",
        "The $n\\times n$ identity matrix $\\mathbf{I}_{n\\times n}$ is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so $\\mathbf{I}_{n\\times n}$ is often simplified to $\\mathbf{I}$ or $I$.\n",
        "\n",
        "Example:\n",
        "\n",
        "$$\n",
        "\\mathbf{I}_{3\\times 3} = \\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Diagonal matrices\n",
        "\n",
        "A square matrix $\\mathbf{A}$ is **diagonal** if all its off-diagonal elements are zero. A $3\\times 3$ diagonal matrix and will look something like $$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "0 & -1 & 0\\\\\n",
        "0 & 0 & 5\n",
        "\\end{bmatrix},\n",
        "$$ where the non-zero values can be replaced by any real numbers.\n",
        "\n",
        "### Symmetric matrices\n",
        "\n",
        "A matrix $\\mathbf{A}$ is **symmetric** if $\\mathbf{A} = \\mathbf{A}^T$, i.e., $\\mathbf{A}_{i,j} = \\mathbf{A}_{j,i}$ for all potential $i,j$.\n",
        "\n",
        "A symmetric matrix must be square.\n",
        "\n",
        "### Idempotent matrices\n",
        "\n",
        "A matrix $\\mathbf{A}$ is **idempotent** if $\\mathbf{AA} = \\mathbf{A}$.\n",
        "\n",
        "An idempotent matrix must be square.\n",
        "\n",
        "### Positive definite matrices\n",
        "\n",
        "A matrix $\\mathbf{A}$ is positive definite if $$\n",
        "\\mathbf{a}^T \\mathbf{Aa} > 0,\n",
        "$$ for every vector of real values $\\mathbf{a}$ whose values are not identically 0.\n",
        "\n",
        "### Inverse matrix\n",
        "\n",
        "An $n\\times n$ matrix $\\mathbf{A}$ is invertible if there exists a matrix $\\mathbf{B}$ such that $\\mathbf{AB}=\\mathbf{BA}=\\mathbf{I}_{n\\times n}$. The inverse of $\\mathbf{A}$ is denoted $\\mathbf{A}^{-1}$.\n",
        "\n",
        "Inverse matrices only exist for square matrices.\n",
        "\n",
        "Some other properties related to the inverse operator:\n",
        "\n",
        "-   If $n\\times n$ matrices $\\mathbf{A}$ and $\\mathbf{B}$ are invertible then $(\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A} ^{-1}$.\n",
        "-   If $\\mathbf{A}$ is invertible then $(\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}$.\n",
        "\n",
        "## Matrix derivatives\n",
        "\n",
        "We start with some basic calculus results.\n",
        "\n",
        "Let $f(y)$ be a function of a scalar value $b$ and $\\frac{df(y)}{dy}$ denote the derivative of the function with respect to $y$. Assume $c \\in \\mathbb{R}$ is a constant. Then the results in <a href=\"#tbl-deriv-scalar\" class=\"quarto-xref\">Table 1</a> are true.\n",
        "\n",
        "| $f(y)$  | $\\frac{df(y)}{dy}$ |\n",
        "|:-------:|:------------------:|\n",
        "|  $cy$   |        $c$         |\n",
        "|  $y^2$  |        $2y$        |\n",
        "| $c y^2$ |       $2cy$        |\n",
        "\n",
        "Table 1: Some basic calculus results for scalar functions taking scalar inputs.\n",
        "\n",
        "Now let’s look at the derivative of a scalar function $f$ with respect to a vector (i.e., the function takes a vector of values and produces a single real number).\n",
        "\n",
        "Let $f(\\mathbf{y})$ be a function of a $p\\times 1$ column vector $\\mathbf{y}=[y_1, y_2, \\ldots,  y_p]^T$. The derivative of $f(\\mathbf{y})$ with respect to $\\mathbf{y}$ is denoted $\\frac{\\partial f(\\mathbf{y})}{\\partial \\mathbf{y}}$ and $$\n",
        "\\frac{\\partial f(\\mathbf{y})}{\\partial \\mathbf{y}} = \\begin{bmatrix}\n",
        "\\frac{\\partial f(\\mathbf{y})}{\\partial y_1}\\\\\n",
        "\\frac{\\partial f(\\mathbf{y})}{\\partial y_2}\\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial f(\\mathbf{y})}{\\partial y_p}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "In words, the derivative of a scalar function with respect to its input vector is the vector of partial derivatives with respect to the elements of the input vector.\n",
        "\n",
        "Assume $\\mathbf{A}$ is an $m\\times p$ matrix of constant values. Then the results in <a href=\"#tbl-deriv-vector\" class=\"quarto-xref\">Table 2</a> are true.\n",
        "\n",
        "| $f(\\mathbf{y})$                      | $\\frac{df(\\mathbf{y})}{d\\mathbf{y}}$ |\n",
        "|:-----------------------------------|:-----------------------------------|\n",
        "| $\\mathbf{y}^T \\mathbf{A}$            | $\\mathbf{A}$                         |\n",
        "| $\\mathbf{y}^T \\mathbf{y}$            | $2\\mathbf{y}$                        |\n",
        "| $\\mathbf{y}^T \\mathbf{A} \\mathbf{y}$ | $2\\mathbf{A}\\mathbf{y}$              |\n",
        "\n",
        "Table 2: Some basic calculus results for scalar functions taking vector inputs.\n",
        "\n",
        "Comparing Tables <a href=\"#tbl-deriv-scalar\" class=\"quarto-xref\">Table 1</a> and <a href=\"#tbl-deriv-vector\" class=\"quarto-xref\">Table 2</a>, we can make parallels with the derivative results from the two contexts.\n",
        "\n",
        "## Additional topics\n",
        "\n",
        "### Determinant\n",
        "\n",
        "The determinant of a matrix is a special function that is applied to a square matrix and returns a scalar value. The determinant of a matrix $\\mathbf{A}$ is usually denoted $|\\mathbf{A}|$ or $\\mathrm{det}(\\mathbf{A})$. We do not discuss how to compute the determinant of a matrix, which is not needed for our purposes. You can find out more about matrix determinants at <https://en.wikipedia.org/wiki/Determinant>.\n",
        "\n",
        "### Linearly independent vectors\n",
        "\n",
        "Let $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n$ be a set of $n$ vectors of size $p\\times 1$.\n",
        "\n",
        "Then $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n$ are **linearly dependent** if there exists $\\mathbf{a}=[a_1, a_2, \\ldots, a_n]\\neq \\boldsymbol{0}_{n\\times 1}$ such \\[ a_1 \\_1 + a_2 \\_2 + + a_p *p = *{p}, \\] where $\\boldsymbol{0}_{p\\times 1}$ defines an $p\\times 1$ column vector of zeros.\n",
        "\n",
        "Let $\\mathbf{X}$ be an $n\\times p$ matrix such that $\\mathbf{x}_1^T, \\mathbf{x}_2^T, \\ldots, \\mathbf{x}_n^T$ make up its $n$ rows and vectors $\\mathbf{X}_{[1]}, \\mathbf{X}_{[2]}, \\ldots, \\mathbf{X}_{[p]}$ make up its columns, so that $$\n",
        "\\mathbf{X}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{x}_1^T\\\\\n",
        "\\mathbf{x}_2^T\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{x}_n^T\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{X}_{[1]} & \\mathbf{X}_{[1]} & \\cdots & \\mathbf{X}_{[p]}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The columns vectors of $\\mathbf{X}$ are linearly independent if there is no $\\mathbf{a}=[a_1,a_2,\\ldots,a_p]\\neq \\boldsymbol{0}_{p\\times 1}$ such that $$\n",
        "a_1 \\mathbf{X}_{[1]} + a_2 \\mathbf{X}_{[2]} + \\cdots + a_p \\mathbf{X}_{[p]} = \\boldsymbol{0}_{n\\times 1}.\n",
        "$$\n",
        "\n",
        "The rows of $\\mathbf{X}$ are linearly independent if there is no $\\mathbf{a}=[a_1,a_2,\\ldots,a_n]\\neq \\boldsymbol{0}_{n\\times 1}$ such that $$\n",
        "a_1 \\mathbf{x}_{1} + a_2 \\mathbf{x}_{2} + \\cdots + a_n \\mathbf{x}_{n} = \\boldsymbol{0}_{p\\times 1}.\n",
        "$$\n",
        "\n",
        "You can learn more about linear independence at [https://en.wikipedia.org/wiki/Linear_independence](%5Bhttps://en.wikipedia.org/wiki/Linear_independence%5D).\n",
        "\n",
        "### Rank\n",
        "\n",
        "The **rank** of a matrix is the number of linearly independent columns of the matrix.\n",
        "\n",
        "If $\\mathbf{X}$ is an $n\\times p$ matrix with linearly dependent columns but removing a single column results in a linearly independent matrix, then the rank of $\\mathbf{X}$ is $p-1$.\n",
        "\n",
        "An $n\\times p$ matrix has **full rank** if its rank equals $\\min(n, p)$, i.e., the smaller of its number of rows and columns."
      ],
      "id": "101be409-11b8-482a-9156-2784c02b1e95"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    }
  }
}