{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3 - Parameter Estimation and Model Fitting\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click or scan the QR code below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/03-estimation-notebook.ipynb\"> <img src=\"https://raw.githubusercontent.com/jfrench/LinearRegression/e4a326bec8ab0be642909f8a081610df05370801/images/qr-03-estimation.png\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We run the code below to ensure that all necessary packages are installed in our Jupyter environment."
      ],
      "id": "6bfe93f9-e744-444b-8572-476a4cff098b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(palmerpenguins, quietly = TRUE)) {\n",
        "  install.packages(\"palmerpenguins\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(palmerpenguins)\n",
        "}\n",
        "if(!require(ggplot2, quietly = TRUE)) {\n",
        "  install.packages(\"ggplot2\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(ggplot2)\n",
        "}"
      ],
      "id": "aa43b000-aa7d-4e15-aa5a-c200d0d983c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A simple motivating example\n",
        "\n",
        "We observe data related to the heights (in) of 5 mothers and their adult daughters. The data are in the table below.\n",
        "\n",
        "| observation | mother | daughter |\n",
        "|------------:|-------:|---------:|\n",
        "|           1 |   57.5 |     61.5 |\n",
        "|           2 |   60.5 |     63.5 |\n",
        "|           3 |   63.5 |     63.5 |\n",
        "|           4 |   66.5 |     66.5 |\n",
        "|           5 |   69.5 |     66.5 |\n",
        "\n",
        "Is it reasonable to use the mother’s height to predict her daughter’s height? Consider the plot below."
      ],
      "id": "89cd1f47-4b50-420f-a716-5628117e6ba7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x <- c(57.5, 60.5, 63.5, 66.5, 69.5) # mothers' heights\n",
        "y <- c(61.5, 63.5, 63.5, 66.5, 66.5) # daughters' heights\n",
        "plot(y ~ x, pch = 19, xlab = \"mother's height (in)\", ylab = \"daughter's height (in)\")"
      ],
      "id": "8dde6643-26e8-4b1c-9305-56edf4d52661"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is regression?\n",
        "\n",
        "A **regression model** is a model describing the typical relationship between a set of variables.\n",
        "\n",
        "A **regression analysis** is the process of building a regression model using a set of variables based on $n$ observations of these variables sampled from a population.\n",
        "\n",
        "In our example, we want to build a regression model for the height of adult daughters using the height of their mothers.\n",
        "\n",
        "## Response versus predictor variables\n",
        "\n",
        "The variables in a regression analysis may be divided into two types:\n",
        "\n",
        "-   The response variable.\n",
        "-   The predictor variables.\n",
        "\n",
        "The **response variable** is the outcome variable we want to predict.\n",
        "\n",
        "-   It is also called the **outcome**, **output**, or **dependent** variable.\n",
        "-   $Y$ denotes the response variable.\n",
        "-   $Y_i$ denotes the value of $Y$ for observation $i$.\n",
        "\n",
        "**Predictor variables** are the variables available to model the response variable.\n",
        "\n",
        "-   They are also called **explanatory**, **regressor**, **input**, or **independent** variables or simply as **features**.\n",
        "\n",
        "Following the convention of Weisberg (2014), we use the term **regressor** to refer to the variables used in our regression model, whether that is the original predictor variable, some transformation of a predictor, some combination of predictors, etc.\n",
        "\n",
        "-   Every predictor can be a regressor but not all regressors are a predictor.\n",
        "-   The regressor variables are denoted as $X_1, X_2, \\ldots, X_{p-1}$.\n",
        "-   $x_{i,j}$ denotes the value of $X_j$ for observation $i$\n",
        "-   If there is only a single regressor in the model, we can denote the single regressor as $X$ and the observed values of $X$ as $x_1, x_2, \\ldots, x_n$.\n",
        "\n",
        "For the height data, the 5 pairs of observed data are denoted\n",
        "\n",
        "$$\n",
        "(x_1, Y_1), (x_2, Y_2), \\ldots, (x_5, Y_5),\n",
        "$$\n",
        "\n",
        "with $(x_i, Y_i)$ denoting the data for observation $i$.\n",
        "\n",
        "-   $x_i$ denotes the mother’s height for observation $i$.\n",
        "-   $Y_i$ denotes the daughter’s height for observation $i$.\n",
        "\n",
        "## Selecting the best model\n",
        "\n",
        "Suppose we want to find the straight line that best fits the plot of mother and daughter heights.\n",
        "\n",
        "How do we determine the “best fitting” model?\n",
        "\n",
        "Consider these potential “best fitting” lines that are drawn on the scatter plot of the height data. Which one is best?"
      ],
      "id": "8b7363cd-adac-4631-a3a6-a2885a33e4e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAHgCAMAAABNUi8GAAAAdVBMVEUAAAAAADoAAGYAOjoAOpAA\nZrY6AAA6ADo6AGY6Ojo6OpA6kNtmAABmADpmAGZmOpBmkJBmtrZmtv+QOgCQZgCQkGaQtpCQ2/+2\nZgC2tma225C2/7a2//++vr7bkDrb2//b/7bb////tmb/25D//7b//9v///8DRNTyAAAACXBIWXMA\nAA7DAAAOwwHHb6hkAAASyElEQVR4nO3d60LruBVAYUMhtCXTC3RK2nLS4oS8/yPWkhXHCfFN3rK3\npPX9mGE4QQ6HNbLsBLs4AYoVaz8BoA+BQjUChWoECtUIFKoRKFQjUKhGoFCNQKEagUI1AoVqBArV\nCBSqEShUI1CoRqBQjUChGoFCNQKFagQK1QgUqhEoVCNQqEagUI1AoRqBQjUChWoECtUIFKoRKFQj\nUKhGoFCNQKEagUI1AoVqBArVCBSqEShUI1CoRqBQjUChGoFCNQKFagQK1QgUqhEoVCNQqEagUI1A\noRqBQjUChWoECtUIFKoRKFQjUKhGoFCNQKEagUI1AoVqBArVCBSqEShUI1CoRqBQjUChGoFCNQKF\nagQK1QgUqhEoVCNQqEagUI1AoRqBQjUChWoECtUIFKoJB1oga79+jX3gWoHKDoe4jM2u6njskAQK\nKb9G9mkeR6BY2JQ8CRQLm5YngWJZE/MkUCxp9PR5+ZBAsZSpe3eLQHFWFCF/KsN51tu/eRyBwrHn\nxYONPjx7uhP4t58duwECTZx75SbM4CP27u71pR+fHrsJAk1cwEBHLT5Nn3e2T6CoBQt05LGRe3n+\nx6fHboZAUxeqz5EP+3V/+wSKsxB9jpw+7cPubp9AEc6UPDsQKEIRyJNAEcz0lzXvIVAEITJ9nggU\nQUjlSaAIQC5PAoU8wTwJFNLGTp8jhyNQSJLcu1sECjnieRIoBMnnSaAQMy68aXkSKISEyZNAIUL4\n0L2FQDFfiMWnQ6CYK9Te3SJQzBM0z2CB7h4/T4dNUTx8iAwHrQLnGSpQ2+dLFedx+yYwHLQKdmzU\nCBLocftaRfpsPtw/fc0eDkoFnz5PwQJ9O32/v5oPy2ounTscVFoiz2C7+Gr23DODpmyZPEMFetw+\nftoptOw6SiLQuC2UZ7jTTGV9nYpnoeGgyqjyJPLkPCimG7l3l9nYwoFe7n8jMhyWt9Ti0+FEPSZZ\nNk9O1GOSBRefDifqMdryeXKiHqMtemzU4EQ9xll68elwoh5jrLF3tzhRj2Gr5cmJegxbMU8CxaA1\n8yRQDBg3fYbbPoGix6p7d4tA0Wn9PAkU3RTkSaDoMia94HkSKO5b+9ioQaD4ScPi0yFQ/KAnTwLF\nD0oWnw6B4oquPAkUV7TlSaBo03Lo3kKgONM3fZ4IFGcq8yRQ1JTmSaCwtOZJoDiNnD7DP427CDR7\navfuFoFmTneeBJo75XkSaN5GtLdungSaswjyJNB8aT50byHQTKlffDoEmqUo9u4WgWYonjwJNEMx\n5Umg+Ynj2KhBoHmJa/o8EWheosuTQHMSYZ4EmpEY8yTQbAzHpzFPAs3EmL37Ak/DA4FmIMrFp0Og\n6Ys4TwJNX6yLT4dA0xZ5noECtTeTtffy6rhTJ4EuI95jo4ZHoO4mckXHjbZPLlB7l05ux72mqBef\nztRAj9vz7Q2/3zvnRxOoS5Obya4m+r27NTHQ41/aTV7/V+vzVaCHjQ2U23GvJI08A65BmUHXlEqe\nwQI93+m4PlyaORymSiZPr0CrxafVeYRuVI0+fFTHUx19EmhAI6bPJZ6GDI9Ad71lSm8XE6Wzd7em\nB9q51x41SsN/DPRILE+/QLtPgDbMLHvYFGY3P3O7mCK1PH0C/X5/Hnyw7fPlgxP1yxqsL7o8PV9J\nGppC7SpgZzvmNNNikjo2avjs4ouho3gzb36/26UqJ+oXktzi0wnzbiYze+6ZQReUaJ6hAj1uHz/t\nFFp2HSURqKQUF59OqPeDurc8dR5PEaichPOc/maR7euINajkdjEg6Tx5R330kjx0byHQqCU+fZ4C\nvR9Ucrvoln6egd5RL7lddMkhz0C/kyS5XXTIIk/WoLEayi+RPAk0TsN790WexhIIND55LD4dAo1O\nTnkSaHSyWXw6BBqV3PL0CtS8HXlfFB3vo5PeLi4yOjZq+PxW59PXYfPs3jEffLto5LX4dPx+ac78\n1kfXe+WFtwsnv7275Reo+aW4PYEuKNM8/Xbxz8ft09dxyy5+Mdnm6XmQVDx8jPnlY5HtYnDxmXCe\nnGaKwOD0uczTWInHGvS3+vfgOEhaRMZ7d8s/UA6SFpB7ntMD3V0u/zXjGmIEOk72ec6ZQRfabsYG\n+sshTw6S9Mr72KgR6grLYtvNFItPhyssq0SeZwtfYXn6dnPE4vMi0BWW5babH/JsC3OFZcHt5oY8\nr3msQd095BbabmY4dL8x+ep2lxP1HMWLY/r8gfOgepDnHQSqBXneNeMmCrOuzkSgN8jzPq+Lh73a\nf77NOSNKoFeGps+FnoZC/qeZ9k9fXbfwkNxuDti7d/M/UV8+fs54zzKBNsizj88MWu/YzQxKoPOR\nZ685a9A5rykRaK0/wOzz9DvNZI/jq8lzztuaCNQgz0FBz4P2vChKoBy6jxIk0BEviBIoi89Rwsyg\n9TKVGbQHe/dxJr9ZZNytEM3FcQi0G3mOFWwNunv4INAO5DleuIOkffFKoHdxbDSB52mmp6/d4Ovw\nh80fCPQnps9JfE7UP3zszeUXBwv9fu9+v1OugZLnRH4vdZp3iXi9znk5/TT9axNAnpP5vVnEBNr7\nThHzItNhUxQPXZfJyTJQ8pzOfwbd9bzXzvb58tHzO8oZBtpbIHl28F6D7nveT2/Xp/VdQLreMppd\noAN796WeRnS83yzSufM+1fOme1de10Igs0BZfPoKcx7UzJ57ZtAGeXoLE+hx+/hpp9Cya6LNKVAW\nnzOEuvxiWT+m8x3N+QRKnrNw+cWwODaaicsvBsXicy4uvxgQe/f5uPxiMOQpgcsvBkKeMrj8Yhjk\nKYSr24XQP30u9jRSQKDy2LsLIlBp5CmKQIWRpywCFdWXIHn6IFBBHBvJ8wjUvNa5Lwr/i9dO2m48\nWHyG4PNmkaevw+bZvWM++HajQZ5B+L0WXxZv/b80J7fdSLD4DMQvUPOOuxmXV56y3SiQZzA+u/hn\nc2mw45ZdvEOeAXkdJBUPH3Pf06QxUM/LSYgduud6OYtenGZq+F3wRG76zPeCK30I9MzrkjyCe/ec\nLwnUw+Ptdn2/ES+/3cV4BCK6+CTQu5hBz6YHIntsRKB3EWhjYh+90+cC288E76i/mNJHiFNL9HmH\nz8XD6jvNzVuKxv2j4MznYvx/q3PGrY6nbFcj8lyO/+/FZ/tafE+D5CnO7wK2Rt8FbAW3qw15Lstr\nDWqm0H2Wa1DxQ3cM8L6A7bz3K0caKIvPxXEedDz27isg0LHIcxUEOg55roRXkkbh2GgtzKAjMH2u\nh0AHkeeaCHQAea5r8hr0tVmGZrEGJc+VMYP26Y6QPBdCoN369u4LPo28eQQ66kZeRhn17bhZfKrg\nc+GGEYvPXVG8Hv78FfHtuMlTB5/3gw7fyMu8FW9nZ89IbybL4lML/zcs97APObyYQKO8HTd56uH/\nKx896kn2+7+nKGdQ8tTEYw064kZezZXvOtcDegPl0F2VQG8Wce+3r998P2u7C2P6VIbzoG3kqc7C\ngV7mX5HhZJGnQj6BmpNI9dUbOplzpYdNXCfqyVMjv5sonMxqtOdg3vZpTjPFc6K+Z/pc8mngRpAL\nN9hj9/ouIJGcZmLvrpX/hRt6bqJgGnYPi+JEPXnq5bGL39tzR4dNzyLUzJ77aGZQ8lTM5yDJHP30\nX2f5uH38tFNo5zXw9ATaWSF5ahDqNFNZn0zqPJDSEih5Kpf3iXoO3dXLOlAWn/plHCh79xhkGyh5\nxiHTQMkzFnkGyrFRNHIMlOkzIvkFSp5RyS1Q8oxMZoGSZ2yyCrQrQ/LUK6NAu/fuiz4NTJJNoCw+\n45RLoOQZqTwCZfEZrRwCJc+IpR8ox0ZRSz5QFp9xSzxQ9u6xSzpQ8oxfwoGSZwrSDZQ8k5BqoJ3T\nZ+DtQliagbJ3T0aKgZJnQhIMlDxTklygHR2SZ6QSC5Rjo9QkFSiLz/SkFCh5JiidQFl8JimVQMkz\nUWkESp7JSiJQDt3TlUCgTJ8piz5Q8kxb5IGSZ+riDpQ8kxdzoF3Tp8DQ0CLeQNm7ZyFQoPuisDdM\n7Lyj59xAyTMTYQLdP3y4+3UHCpQ8cxEk0PpOx9/vT19hAr0fInmmKEig51vK756+AgRKnjkJOIOe\nzG25xQPl0D0vgdagLsvjthAOlMVnZoIdxdc7+e930UDZu2cnpvOg5JmhhQMtGpO/lDyzFCjQXbVn\nP2yK4uFDZLhT5+Jz6jCITJhAbZ8vH5cTTvOGY/rMV6DzoK/mFJP5cP/0NXs48sxYsBP17lxoOf8o\nnjxzFmgXX82ee6EZlDyzFibQ4/bx006hZddR0tjh7pZInvkIdZqprE8mPc8brmPvPu2pIGaaT9Sz\n+ITmQMkTigNl8QlDaaDkiZrKQDk2wpnGQFl8oqEvUPbuaNEWKHniiq5AyRM3VAVKnrilKND706fs\nZhEbNYGyd8c9SgIlT9ynI1DyRAcNgd5LkTxhrR8ox0bosXagLD7Ra+VAr0p0l3MgT1ysGuiv2z6r\nQskTbSsGepOi7ZM8cW21QH+kaPP0uGYTkrZWoD9nyqpPr4uKIWlrH8Vf/KJP/KQlULPHp0/8oCNQ\njo3QQUOg5IlOCgIlT3RbPVCmT/RZ+6VO8kQvRS91Aj/peakTuEPPS53AHau91FkAY6wU6HhBNhzm\nu4nnqUb0/avfeO4/oHgGJVDlg0b0VCP6/tVvPPcfUDyDEqjyQSN6qhF9/+o3nvsPKJ5BCVT5oBE9\n1Yi+f/Ubz/0HFM+gBKp80IieakTffxQbB4YQKFQjUKhGoFCNQKEagUI1AoVqBArVCBSqEShUI1Co\nRqBQjUCh2uKBHrfml06fq4/21b/fhEY9bOoxT2VRPHwIDdqM+v1ePdVX4UGN3dOX8KDt0eVGlfxR\nTbR4oIcX18++CqkU+rbL6gd93FZ/l6UZVKrQ86jf79WIe6Gfe/NUzceFUKCX7781utyokj+qqRYP\ntHz8tP/+fjd/jTuRv8vvdzO97R8/6w9kBr2Meti81R9IDnqyOxOZQG++f5ln2h5V7kc12eKB7t33\nKfldN7OyZEqXUWsyE3N70P3TP2QCvXz/L2LLm9ZgeQW6+5NbzwnuN8rH/2ztoPXfaSkTaDNqbScy\namvQ6skKrUGbQW+estCoWe3ij1vzM9mZb1vueGZfVPGY/8/rWU5oEdqMapUyP/fLoGYPKhRoM+j1\nU5YaVfjQc5p1TjOZOc7MSIeNzE/9wU2csoE+tKbjUugY6TLovopTKtDzoFdPWWxUyR/VZOsEWq0V\nJZeL9ShmUMldfDPqSWz+vH2qUoGeB20/ZblRRVf2U60U6MuH5GRXF1kNKvpX2Yxq9nZS00cz6N5d\nhlCipWbQ1lOWHFVyvzTV0oG6Cel87kZmsjtu3Viip5maUas+xQ4QLoMaQjNoM+j16FKjSv6oJlv+\nKN6esng9iS5szHrOxil6ov48qujyq3mqhtQrSc2gV6OLjZrXGnR33q/tBF8/LM9j7SWPN92obm8s\nNG7Z+rbFXupsBi0F/1Ivg0n+qCbizSJQjUChGoFCNQKFagQK1QgUqhEoVCNQqEagUI1AoRqBQjUC\nhWoECtUIFKoRKFQjUKhGoFCNQKEagUI1AoVqBArVCBSqEShUI1CoRqBQjUChGoFCNQIVVL7dXrms\nvkJc1yea/yjPn/x+f2s/4vjbStc11oNA5Zi0hgL98QXXj9rfXDmylLq6WLQIVI7p7PDHz9tP9X/B\n1aN+PLyaUSWfYYQIdNjh5Z+b1nUI38yu9/fCXtjWfvq4/fu2+rS5L9vT//76ZW/QVpzT+9um/rh5\n7Fs9zMPvDx/uT+0X2pnSXCTWPMKNeKqv0Zk1Ah1WBfRW34Zl5267Vt+rxHxkrut63Jo70Zne3uqH\nv53Od205/1HrsW/1BWHLwnzB1RfW49eBui9Z68LbehDosPriwtVcZo9ZTH/HrZ0M7c10zIW3X+2n\nXWfta2Vf/qh57Ju7DPrOhHn1hedh35ovE7shQrwIdJi7Rns9l5Vm12uLqj9x7qv6h+vsuL0Uev6j\nq8fWBZdu5rx8YfPoywfu/4KMEegwd3Mbe0fZ4vHfm3Og5/t03HZmb458ndzVY/ddgdr/Ewj0CoEO\nawJtdrqtGfR0qbB9DL6r//B6BnWfYAadgECH1WvQ3dOXLass2gEZ9wJtdvc3f3RZg+7vBfp6Gyhr\n0LWfQAQOG3cz1XryLNyxeH2YvWt3Vt9oo3X03ZR29djLUfzVF7aP4s+BchS/9hOIwGHTOpn58LFr\nzguZc5vuzKX9x64+nVle7llzKa39WHMe9PFf7r5brS9snQdtvozzoBgSZjd7785t/S/dZ4lAh0kH\n6g7W7x398Fr8LQIdJj6Dlp13kb157Z13MxEodCNQqEagUI1AoRqBQjUChWoECtUIFKoRKFQjUKhG\noFCNQKEagUI1AoVqBArVCBSqEShUI1CoRqBQjUChGoFCtf8DtnbiUGC9Rw0AAAAASUVORK5CYII=\n"
          }
        }
      ],
      "source": [],
      "id": "55836471-1d9a-48f6-b6f0-3fc47db61a6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Estimation of the simple linear regression model\n",
        "\n",
        "**Parameter estimation** is the process of using observed data to estimate model parameters.\n",
        "\n",
        "There are many different methods for parameter estimation in statistics:\n",
        "\n",
        "-   Method-of-moments.\n",
        "-   Maximum likelihood.\n",
        "-   Bayesian.\n",
        "-   Etc.\n",
        "\n",
        "The most common parameter estimation method for linear models is the **least squares method**, which is commonly called **Ordinary Least Squares (OLS)** estimation.\n",
        "\n",
        "OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly.\n",
        "\n",
        "## Defining a simple linear regression model\n",
        "\n",
        "The regression model for $Y$ as a function of $X$, denoted $E(Y \\mid X)$, is the expected value of $Y$ conditional on the value of the regressor $X$.\n",
        "\n",
        "The **simple linear regression model** for a response variable assumes the mean of $Y$, conditional on the value of a single regressor $X$, is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X) = \\beta_0 + \\beta_1 X.\n",
        "$$ The response variable $Y$ is modeled as $$\n",
        "\\begin{aligned}\n",
        "Y &= E(Y \\mid X) + \\epsilon \\\\\n",
        "&= \\beta_0 + \\beta_1 X + \\epsilon,\n",
        "\\end{aligned}\n",
        "$$ where $\\epsilon$ is the model error.\n",
        "\n",
        "The error term $\\epsilon$ is literally the deviation of the response variable from its mean.\n",
        "\n",
        "-   We typically assume that conditional on the regressor variable, the error term has mean 0 and variance $\\sigma^2$.\n",
        "-   This is written as $E(\\epsilon \\mid X) = 0$ and $\\mathrm{var}(\\epsilon \\mid X) = \\sigma^2$.\n",
        "\n",
        "The observed data are modeled as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Y_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n",
        "&= E(Y\\mid X = x_i) + \\epsilon_i,\n",
        "\\end{aligned}\n",
        "$$ for $i=1$, $2$,$\\ldots$,$n$, where $\\epsilon_i$ denotes the error for observation $i$.\n",
        "\n",
        "## Important terminology\n",
        "\n",
        "The **estimated regression model** or **fitted model** is defined as\n",
        "\n",
        "$$\n",
        "\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X,\n",
        "$$\n",
        "\n",
        "where $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the estimated value of our regression parameters.\n",
        "\n",
        "The $i\\text{th}$ **fitted value** is defined as\n",
        "\n",
        "$$\n",
        "\\hat{Y}_i = \\hat{E}(Y|X = x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i. $$\n",
        "\n",
        "-   The $i\\text{th}$ fitted value is the estimated mean of $Y$ when the regressor $X=x_i$.\n",
        "\n",
        "The $i\\text{th}$ **residual** is defined as\n",
        "\n",
        "$$\n",
        "\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i.\n",
        "$$\n",
        "\n",
        "-   The $i\\text{th}$ residual is the difference between the response and estimated mean response of observation $i$.\n",
        "\n",
        "The **residual sum of squares (RSS)** of a regression model is the sum of its squared residuals. Generically, $$\n",
        "RSS = \\sum_{i=1}^n \\hat{\\epsilon}_i^2.\n",
        "$$\n",
        "\n",
        "The RSS for a simple linear regression model, as a function of the estimated regression coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, can be written as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) &= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\\n",
        "&= \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 & \\\\\n",
        "&= \\sum_{i=1}^n (Y_i - \\hat{E}(Y|X=x_i))^2 \\\\\n",
        "&= \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The **fitted model** is the estimated model that minimizes the RSS, and is written as\n",
        "\n",
        "$$\n",
        "\\hat{Y}=\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X.\n",
        "$$\n",
        "\n",
        "-   $\\hat{Y}$ is used for brevity.\n",
        "-   $\\hat{E}(Y|X)$ is used for clarity.\n",
        "\n",
        "In a simple linear regression context, the fitted model is known as the **line of best fit**.\n",
        "\n",
        "## Visualizing terms"
      ],
      "id": "5dd8e113-a7ce-4911-92b7-715d8f74e2cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAHgCAMAAABNUi8GAAAAclBMVEUAAAAAADoAAGYAAP8AOpAA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\n0ggU0ggU0ggU0ggU0v4fhiHLjGl/ieoAAAAASUVORK5CYII=\n"
          }
        }
      ],
      "source": [],
      "id": "8cee4c3e-ba6b-436a-9860-5cb9aa892e97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the graphic above, we visualize the response values, fitted values, residuals, and fitted model in a simple linear regression context. Note that:\n",
        "\n",
        "-   The fitted model is shown as the dashed grey line and minimizes the RSS.\n",
        "-   The response values, shown as black dots, are the observed values of $Y$.\n",
        "-   The fitted values, shown as blue x’s, are the values returned by evaluating the fitted model at the observed regressor values.\n",
        "-   The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative.\n",
        "-   The RSS is the sum of the squared vertical distances between the response and fitted values.\n",
        "\n",
        "## OLS estimators of the simple linear regression parameters\n",
        "\n",
        "The estimators of $\\beta_0$ and $\\beta_1$ that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus (as long as $x_1,\\ldots,x_n$ are not all equal to the same number).\n",
        "\n",
        "Define:\n",
        "\n",
        "-   $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
        "-   $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n",
        "\n",
        "The OLS estimators of the simple linear regression coefficients that minimize the RSS are\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n x_i Y_i - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)\\biggl(\\sum_{i=1}^n Y_i\\biggr)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)^2} \\notag \\\\\n",
        "&= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\notag \\\\\n",
        "&= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})Y_i}{\\sum_{i=1}^n (x_i - \\bar{x})x_i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}.\n",
        "$$\n",
        "\n",
        "The most common estimator of the error variance is\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{RSS}{\\mathrm{df}_{RSS}}.\n",
        "$$\n",
        "\n",
        "-   $\\mathrm{df}_{RSS}$ is the **degrees of freedom** of the RSS.\n",
        "-   For simple linear regression, $\\mathrm{df}_{RSS}=n-2$.\n",
        "\n",
        "# Penguins simple linear regression example\n",
        "\n",
        "The `penguins` data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman et al. (2014). We start by loading the data into memory."
      ],
      "id": "14cf596f-1227-4374-a9e1-42513f994265"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data(penguins, package = \"palmerpenguins\")\n",
        "head(penguins)"
      ],
      "id": "29f77b89-4ea6-4523-9a72-1f74413d59a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data set includes 344 observations of 8 variables. The variables are:\n",
        "\n",
        "-   `species`: the penguin species (`factor`).\n",
        "-   `island`: the island on which the penguin was observed (`factor`).\n",
        "-   `bill_length_mm`: the penguin’s bill length in millimeters (`numeric`).\n",
        "-   `bill_depth_mm`: the penguin’s bill depth in millimeters (`numeric`).\n",
        "-   `flipper_length_mm`: the penguin’s flipper length in millimeters (`integer`).\n",
        "-   `body_mass_g`: the penguin’s body mass in grams (`integer`).\n",
        "-   `sex`: the penguin’s sex (`factor`).\n",
        "-   `year`: the study year the penguin was observed (`integer`).\n",
        "\n",
        "We begin by creating a scatter plot of `bill_length_mm` versus `body_mass_g`."
      ],
      "id": "d2f2b2a7-8b50-443d-bcee-1296cc22729a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(bill_length_mm ~ body_mass_g, data = penguins,\n",
        "     ylab = \"bill length (mm)\", xlab = \"body mass (g)\",\n",
        "     main = \"Penguin size measurements\")"
      ],
      "id": "fe3ab9b0-e779-499b-9261-21cd13eaab65"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        "-   Is there a positive/negative association between body mass and bill length?\n",
        "-   Is the relationship approximately linear?\n",
        "\n",
        "We will build a simple linear regression model that regresses `bill_length_mm` on `body_mass_g`.\n",
        "\n",
        "We want to estimate the parameters of the model\n",
        "\n",
        "$$\n",
        "E(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g})=\\beta_0+\\beta_1\\,\\mathtt{body\\_mass\\_g}.\n",
        "$$\n",
        "\n",
        "The `lm` function uses OLS estimation to fit a linear model to data. The function has two main arguments:\n",
        "\n",
        "-   `formula`: a Wilkinson and Rogers (1973) style formula describing the linear regression model. For complete details, run `?stats::formula` in the Console. If `y` is the response variable and `x` is an available numeric predictor, then `formula = y ~ x` tells `lm` to fit the simple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X$.\n",
        "-   `data`: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory."
      ],
      "id": "22ea11bc-4ec4-4f10-8678-02e690e108d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lmod <- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model\n",
        "class(lmod) # class of lmod"
      ],
      "id": "9f579475-02e8-489d-a1de-83e3c105b2a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `summary` function summarizes the results of our fitted model.\n",
        "\n",
        "When an `lm` object is supplied to the `summary` function, it returns:\n",
        "\n",
        "-   `Call`: the function call used to fit the model.\n",
        "-   `Residuals`: A 5-number summary of $\\hat{\\epsilon}_1, \\ldots, \\hat{\\epsilon}_n$.\n",
        "-   `Coefficients`: A table that lists:\n",
        "    -   The regressors in the fitted model.\n",
        "    -   `Estimate`: the estimated coefficient of each regressor.\n",
        "    -   `Std. Error`: the *estimated* standard error of the estimated coefficients.\n",
        "    -   `t value`: the computed test statistic associated with testing $H_0: \\beta_j = 0$ versus $H_a: \\beta_j \\neq 0$ for each regression coefficient in the model.\n",
        "    -   `Pr(>|t|)`: the associated p-value of each test.\n",
        "-   Various summary statistics:\n",
        "    -   `Residual standard error` is the value of $\\hat{\\sigma}$, the estimate of the error standard deviation. The degrees of freedom is $\\mathrm{df}_{RSS}$, the number of observations minus the number of estimated coefficients in the model.\n",
        "    -   `Multiple R-squared` is a measure of model fit.\n",
        "    -   `Adjusted R-squared` is a modified version of `Multiple R-squared`.\n",
        "    -   `F-statistic` is the test statistic of the test that compares the model with an only an intercept to the fitted model. The `DF` (degrees of freedom) values relate to the statistic under the null hypothesis, and the `p-value` is the p-value of the test.\n",
        "\n",
        "We use the `summary` function on `lmod` to produce the output below."
      ],
      "id": "00bbd17d-b9ae-4914-834e-4dd0b6f5268b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize results stored in lmod\n",
        "summary(lmod)"
      ],
      "id": "bb42a687-dda3-4985-a970-72d9197f3943"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the output above, we see that the estimated parameters are $\\hat{\\beta}_0=26.9$ and $\\hat{\\beta}_1=0.004$.\n",
        "\n",
        "Our fitted model is\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathtt{bill\\_length\\_mm}}=26.9+0.004 \\,\\mathtt{body\\_mass\\_g}.\n",
        "$$\n",
        "\n",
        "In the context of a simple linear regression model:\n",
        "\n",
        "-   The intercept term is the expected response when the value of the regressor is zero.\n",
        "-   The slope is the expected change in the response when the regressor increases by 1 unit.\n",
        "\n",
        "Thus, based on the model we fit to the `penguins` data, we can make the following interpretations:\n",
        "\n",
        "-   $\\hat{\\beta}_1$: If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguin’s bill length to be 0.004 millimeters longer.\n",
        "-   $\\hat{\\beta}_0$: A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "-   Does the intercept term make sense physically?\n",
        "-   If not, why is that the case? The `abline` function can be used to automatically overlay the fitted model on the observed data."
      ],
      "id": "3d56b183-fd3e-4200-a8c3-3178fed3b8ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(bill_length_mm ~ body_mass_g, data = penguins, main = \"Penguin size measurements\",\n",
        "     ylab = \"bill length (mm)\", xlab = \"body mass (g)\")\n",
        "# draw fitted line on plot\n",
        "abline(lmod)"
      ],
      "id": "8993cb23-7f94-4503-a495-e631b54a6777"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "R provides many additional methods (generic functions that do something specific when applied to a certain type of object) for `lm` objects.\n",
        "\n",
        "We now use some of the methods to extract important characteristics of our fitted model.\n",
        "\n",
        "The `coef` function extracts the estimated regression coefficients, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, from the fitted model."
      ],
      "id": "899112d9-f0dc-4acc-a4ca-ee25c4f80955"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(coeffs <- coef(lmod)) # extract, assign, and print coefficients"
      ],
      "id": "7760246e-92c6-4374-94ad-f0c398cd1295"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `residuals` function extracts the vector of residuals, $\\hat{\\epsilon}_1,\\ldots, \\hat{\\epsilon}_n$ from the fitted model."
      ],
      "id": "c1344852-d56a-4502-ba99-733605c68f5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ehat <- residuals(lmod) # extract and assign residuals\n",
        "head(ehat) # first few residuals"
      ],
      "id": "47a3486f-86fc-495a-a63a-ae878a3a7a45"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fitted` function extracts the vector of fitted values, $\\hat{Y}_1,\\ldots, \\hat{Y}_n$, from the fitted model."
      ],
      "id": "49c59c8a-48b7-4b8a-bf91-0190b1844639"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat <- fitted(lmod) # extract and assign fitted values\n",
        "head(yhat) # first few fitted values"
      ],
      "id": "c556f7c9-f833-48bc-91d5-4a875572ea38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `predict` function also extracts the vector of fitted values from the fitted model. It can be also used to predict the response of an observation for arbitrary values of the predictors."
      ],
      "id": "beb508d1-2c2b-48b4-abbc-d95912177464"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat2 <- predict(lmod) # compute and assign fitted values\n",
        "head(yhat2) # first few fitted values"
      ],
      "id": "97423b04-8173-403a-8a5a-9ba6e49ab63f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `deviance` function extracts the RSS of the fitted model."
      ],
      "id": "8b01b8f3-5502-4fb1-b170-a184bd4ee6b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(rss <- deviance(lmod)) # extract, assign, and print rss"
      ],
      "id": "65d4f180-7f80-4087-a484-d35f2314efea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `df.residual` function extracts the residual degrees of freedom from the fitted model."
      ],
      "id": "5f425c4c-8021-48e6-820f-1f39da262a35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(dfr <- df.residual(lmod)) # extract n - p"
      ],
      "id": "52bdd50b-72d3-4b88-82af-5c23418fdd84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `sigma` function extracts the estimated error standard deviation, $\\hat{\\sigma}=\\sqrt{\\hat{\\sigma}^2}$, from the fitted model. In the code below, we square $\\hat{\\sigma}$ to estimate the error variance, $\\hat{\\sigma}^2$."
      ],
      "id": "cab49526-494f-4866-8c96-fdf2ca6c803d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(sigmasqhat <- sigma(lmod)^2) # estimated error variance"
      ],
      "id": "cd1ffa09-e4f4-4a20-b4e6-820a770da088"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        "-   What are the first three fitted values?\n",
        "-   What is the RSS of the fitted model?\n",
        "-   What is the residual degrees of freedom?\n",
        "-   What is the estimated error variance of the fitted model?\n",
        "\n",
        "# Defining a linear model\n",
        "\n",
        "## Defining terms (again)\n",
        "\n",
        "-   $Y$ denotes the response variable.\n",
        "    -   The response variable is treated as a random variable.\n",
        "    -   We will observe realizations of this random variable for each observation in our data set.\n",
        "-   $X$ denotes a single regressor variable. $X_1, X_2, \\ldots, X_{p-1}$ denote distinct regressor variables if we are performing regression with multiple regressor variables.\n",
        "    -   The regressor variables are treated as non-random variables.\n",
        "    -   The observed values of the regressor variables are treated as fixed, known values.\n",
        "-   $\\mathbb{X}=\\{X_0, X_1,\\ldots,X_{p-1}\\}$ denotes the collection of all regressors.\n",
        "    -   $X_0$ is usually the constant regressor 1, which is needed to include an intercept in the regression model.\n",
        "-   $\\beta_0$, $\\beta_1$, $\\ldots$, $\\beta_{p-1}$ denote **regression coefficients**.\n",
        "    -   Regression coefficients are statistical parameters that we will estimate from our data.\n",
        "    -   The regression coefficients are treated as fixed, non-random but unknown values.\n",
        "    -   Regression coefficients are not observable.\n",
        "-   $\\epsilon$ denotes model **error**.\n",
        "    -   The model error is more accurately described as random variation of each observation from the regression model.\n",
        "    -   The error is treated as a random variable.\n",
        "    -   The error is assumed to have mean 0 for all values of the regressors, i.e., $E(\\epsilon \\mid \\mathbb{X}) = 0$.\n",
        "    -   The variance of the errors is assumed to be a constant value for all values of the regressors, i.e.,$\\mathrm{var}(\\epsilon \\mid \\mathbb{X})=\\sigma^2$.\n",
        "    -   The error is not observable.\n",
        "\n",
        "## Standard definition of linear model\n",
        "\n",
        "In general, a linear regression model can have an arbitrary number of regressors.\n",
        "\n",
        "A **multiple linear regression** model has two or more regressors.\n",
        "\n",
        "A **linear model** for $Y$ is defined by the equation\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Y &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1}\n",
        "X_{p-1} + \\epsilon \\\\\n",
        "&= E(Y \\mid \\mathbb{X}) + \\epsilon.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Notice:\n",
        "\n",
        "-   The response value equals the expected response for that combination of regressor values plus some error.\n",
        "-   $E(Y \\mid \\mathbb{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1}$.\n",
        "\n",
        "A linear regression model can be written as\n",
        "\n",
        "$$\n",
        "E(Y \\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} c_j \\beta_j.\n",
        "$$\n",
        "\n",
        "-   $c_0, c_1, \\ldots, c_{p-1}$ are known functions of the regressor variables.\n",
        "-   e.g., $c_1 = X_1 X_2 X_3$, $c_3 = X_2^2$, $c_8 = \\ln(X_1)/X_2^2$, etc.\n",
        "\n",
        "Alternatively, if $g_0,\\ldots,g_{p-1}$ are functions of $\\mathbb{X}$, then a linear regression model can be written as\n",
        "\n",
        "$$\n",
        "E(Y\\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} g_j(\\mathbb{X})\\beta_j.\n",
        "$$\n",
        "\n",
        "The key feature of the linear regression model is that the model is a linear combination of the regression coefficients.\n",
        "\n",
        "## Examples of a linear model\n",
        "\n",
        "A model is linear because of its *form* not the shape it produces.\n",
        "\n",
        "Some examples of linear regression models are:\n",
        "\n",
        "-   $E(Y|X) = \\beta_0$.\n",
        "-   $E(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 \\ln(X_1) + \\beta_2 X_2^{-1}$.\n",
        "-   $E(\\ln(Y)|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   $E(Y^{-1}|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "\n",
        "Some examples of non-linear regression models are:\n",
        "\n",
        "-   $E(Y|X) = \\beta_0 + e^{\\beta_1 X}$.\n",
        "-   $E(Y|X) = \\beta_0 + \\beta_1 X/(\\beta_2 + X)$.\n",
        "\n",
        "# Estimation of the multiple linear regression model\n",
        "\n",
        "We want to estimate the parameters of the model\n",
        "\n",
        "$$\n",
        "Y=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon.\n",
        "$$\n",
        "\n",
        "The system of equations relating the responses, the regressors, and the errors for all $n$ observations can be written as\n",
        "\n",
        "$$\n",
        "Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n.\n",
        "$$\n",
        "\n",
        "## Using matrix notation to represent a linear model\n",
        "\n",
        "We can simplify the linear model system of equations using matrix notation.\n",
        "\n",
        "We use the following notation:\n",
        "\n",
        "-   $\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]$ denotes the column vector containing the $n$ observed response values.\n",
        "-   $\\mathbf{X}$ denotes the matrix containing a column of 1s and the observed regressor values for $X_1, X_2, \\ldots, X_{p-1}$. This may be written as\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "    1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p-1} \\\\\n",
        "    1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,p-1} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "    1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p-1}\n",
        "    \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "-   $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]$ denotes the column vector containing the $p$ regression coefficients.\n",
        "-   $\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]$ denotes the column vector contained the $n$ model errors.\n",
        "\n",
        "The system of equations defining the linear model in can be written as\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n",
        "$$\n",
        "\n",
        "## Matrix definitions of residuals, fitted values, and RSS for multiple linear regression\n",
        "\n",
        "The column vector of estimated values for the coefficients contained in $\\boldsymbol{\\beta}$ is denoted\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}=[\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_{p-1}].\n",
        "$$\n",
        "\n",
        "The column vector of regressor values for the $i\\text{th}$ observation is denoted\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i=[1,x_{i,1},\\ldots,x_{i,p-1}].\n",
        "$$\n",
        "\n",
        "**Question:** Why does $\\mathbf{x}_i$ include a $1$ in its first position?\n",
        "\n",
        "The $i\\text{th}$ **fitted value** is defined as\n",
        "\n",
        "$$\n",
        "\\hat{Y}_i = \\hat{E}(Y \\mid \\mathbb{X} = \\mathbf{x}_i) \\\\\n",
        "$$\n",
        "\n",
        "and can also be written as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{Y}_i &= \\hat{E}(Y \\mid \\mathbb{X} = \\mathbf{x}_i) \\\\\n",
        "&= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i,1} + \\cdots + \\hat{\\beta}_{p-1} x_{i,p-1} \\\\\n",
        "&= \\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The notation “$\\mathbb{X} = \\mathbf{x}_i$” is a concise way of saying “$X_0 = 1, X_1=x_{i,1}, \\ldots, X_{p-1}=x_{i,p-1}$”.\n",
        "\n",
        "The column vector of fitted values is defined as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\mathbf{y}} &= [\\hat{Y}_1,\\ldots,\\hat{Y}_n] \\\\\n",
        "&= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The $i\\text{th}$ **residual** is defined as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i=Y_i-\\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The column vector of residuals is defined as $$\n",
        "\\hat{\\boldsymbol{\\epsilon}} = [\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_n]. $$\n",
        "\n",
        "Equivalent expressions for the residual vector are\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\epsilon}}=\\mathbf{y}-\\hat{\\mathbf{y}}=\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n",
        "$$\n",
        "\n",
        "The RSS for a multiple linear regression model, as a function of the estimated regression coefficients, is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "RSS(\\hat{\\boldsymbol{\\beta}}) &= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\\n",
        "&= \\hat{\\boldsymbol{\\epsilon}}^T \\hat{\\boldsymbol{\\epsilon}} \\\\\n",
        "&= (\\mathbf{y} - \\hat{\\mathbf{y}})^T (\\mathbf{y} - \\hat{\\mathbf{y}}) \\\\\n",
        "& = (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## OLS estimator in matrix form\n",
        "\n",
        "The OLS estimator of the regression coefficient vector, $\\boldsymbol{\\beta}$, is\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "This solution for $\\hat{\\boldsymbol{\\beta}}$ assumes $\\mathbf{X}$ has full-rank ($n>p$ and none of the columns of $\\mathbf{X}$ are linear combinations of other columns in $\\mathbf{X}$).\n",
        "\n",
        "The general estimator of the $\\sigma^2$ in the context of multiple linear regression is\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{RSS}{n-p}.\n",
        "$$\n",
        "\n",
        "# Penguins multiple linear regression example\n",
        "\n",
        "We use the `lm` function to fit a multiple linear regression model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm`.\n",
        "\n",
        "## Formula notation\n",
        "\n",
        "We provide further discussion of the `formula` argument of the `lm` function’s `formula` argument. Assume `y` is the response variable and `x`, `x1`, `x2`, `x3` are available numeric predictors. Then:\n",
        "\n",
        "-   `y ~ x` describes the simple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X$.\n",
        "-   `y ~ x1 + x2` describes the multiple linear regression model $E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   `y ~ x1 + x2 + x1:x2` and `y ~ x1 * x2` describe the multiple linear regression model $E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$.\n",
        "-   `y ~ -1 + x1 + x2` describes a multiple linear regression model without an intercept, in this case, $E(Y|X_1, X_2)=\\beta_1 X_1 + \\beta_2 X_2$.\n",
        "    -   The `-1` tells R not to include an intercept in the fitted model.\n",
        "-   `y ~ x + I(x^2)` describes the multiple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X + \\beta_2 X^2$.\n",
        "    -   The `I()` function is a special function that tells R to create a regressor based on the syntax inside the `()` and include that regressor in the model.\n",
        "\n",
        "## Fitting a model\n",
        "\n",
        "We fit the linear model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm` and extract some statistics."
      ],
      "id": "bfbb82cd-382d-4e87-9c03-a739af199758"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlmod <- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins) # fit model"
      ],
      "id": "2730faef-c8a0-47e7-97e9-cd4bbb5b251d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(mlmod) # extract estimated coefficients"
      ],
      "id": "8e052f5d-c182-4129-a3bb-32d39c424658"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deviance(mlmod) # extract RSS"
      ],
      "id": "f1e55c85-c366-41a0-a8b2-530ff5ec564c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted model is\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathtt{bill\\_length\\_mm}}=-3.44+0.0007 \\,\\mathtt{body\\_mass\\_g}+0.22\\,\\mathtt{flipper\\_length\\_mm}.\n",
        "$$\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "-   What is the value of the RSS?\n",
        "-   How does it compare to the RSS of the simple linear regression model we used before?\n",
        "\n",
        "# Types of linear models\n",
        "\n",
        "-   **Simple**: a model with an intercept and a single regressor.\n",
        "-   **Multiple**: a model with 2 or more regressors.\n",
        "-   **Polynomial**: a model with squared, cubic, quartic predictors, etc.\n",
        "    -   E.g, $E(Y\\mid X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3$ is a 4th-degree polynomial.\n",
        "-   **First-order**: a model in which each predictor is used to create no more than one regressor.\n",
        "-   **Main effect**: a model in which none of the regressors are functions of more than one predictor. A predictor can be used more than once, but each regressor is only a function of one predictor.\n",
        "    -   E.g., if $X_1$ and $X_2$ are different predictors, then the regression model $E(Y\\mid X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_2$ would be a main effect model, but not a first-order model since $X_1$ was used to create two regressors.\n",
        "-   **Interaction**: a model in which some of the regressors are functions of more than 1 predictor.\n",
        "    -   E.g., if $X_1$ and $X_2$ are different predictors, then the regression model $E(Y\\mid X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2$ is a very simple interaction model since the third regressor is the product of $X_1$ and $X_2$.\n",
        "-   **Analysis of variance (ANOVA)**: a model for which all predictors used in the model are categorical.\n",
        "-   **Analysis of covariance (ANCOVA)**: a model that uses at least one numeric predictor and at least one categorical predictor.\n",
        "-   **Generalized (GLM)**: a “generalized” linear regression model in which the responses do not come from a normal distribution.\n",
        "\n",
        "# Categorical predictors\n",
        "\n",
        "Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the variables.\n",
        "\n",
        "We discuss two basic linear regression models that have categorical predictors:\n",
        "\n",
        "-   **Parallel lines regression model**: a main effect regression model that has a single numeric regressor and a single categorical predictor.\n",
        "    -   The model produces parallel lines for each level of the categorical variable.\n",
        "-   **Separate lines regression model**, which adds an interaction term between the numeric regressor and categorical predictor of the parallel lines regression model.\n",
        "    -   The model produces separate lines for each level of the categorical variable.\n",
        "\n",
        "## Indicator variables\n",
        "\n",
        "In order to compute $\\hat{\\boldsymbol{\\beta}}$ both $\\mathbf{X}$ and $\\mathbf{y}$ must contain numeric values.\n",
        "\n",
        "How can we use a categorical predictor in our regression model when its values are not numeric?\n",
        "\n",
        "We must transform the categorical predictor into one or more **indicator** or **dummy variables**.\n",
        "\n",
        "An **indicator function** is a function that takes the value 1 if a certain property is true and 0 otherwise.\n",
        "\n",
        "An **indicator variable** is the variable that results from applying an indicator function to each observation of a variable.\n",
        "\n",
        "We denote indicator functions using the notation,\n",
        "\n",
        "$$\n",
        "I_S(x) =\n",
        "\\begin{cases}\n",
        "1 & \\textrm{if}\\;x \\in S\\\\\n",
        "0 & \\textrm{if}\\;x \\notin S\n",
        "\\end{cases}.\n",
        "$$\n",
        "\n",
        "-   This function returns 1 if $x$ is in the set $S$ and 0 otherwise.\n",
        "\n",
        "Some examples of indicator functions:\n",
        "\n",
        "-   $I_{\\{2,3\\}}(2) = 1$.\n",
        "-   $I_{\\{2,3\\}}(2.5) = 0$.\n",
        "-   $I_{[2,3]}(2.5) = 1$, where $[2,3]$ is the interval from 2 to 3 and not the set containing only the numbers 2 and 3.\n",
        "-   $I_{\\{\\text{red},\\text{green}\\}}(\\text{green}) = 1$.\n",
        "\n",
        "Let $C$ denote a categorical predictor with levels $L_1$ and $L_2$.\n",
        "\n",
        "-   $C$ stands for “categorical”.\n",
        "-   $L$ stands for “level”.\n",
        "-   $c_i$ denotes the value of $C$ for observation $i$.\n",
        "\n",
        "Let $D_j$ denote the indicator (dummy) variable for factor level $L_j$ of $C$.\n",
        "\n",
        "-   The value of $D_j$ for observation $i$ is denoted $d_{i,j}$, with\n",
        "\n",
        "$$\n",
        "d_{i,j} = I_{\\{L_j\\}}(c_i).\n",
        "$$\n",
        "\n",
        "-   $d_{i,j}$ is 1 if $c_i$ has factor level $L_j$ and 0 otherwise.\n",
        "\n",
        "## Parallel and separate lines models\n",
        "\n",
        "We will fit three linear regression models using a single numeric regressor $X$ and a two-level categorical predictor $C$.\n",
        "\n",
        "The standard simple linear regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X)=\\beta_0 + \\beta_1 X.\n",
        "$$\n",
        "\n",
        "The parallel lines regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X,C)=\\beta_{0}+\\beta_1 X+\\beta_2 D_2.\n",
        "$$\n",
        "\n",
        "Since $D_2=0$ when $C=L_1$ and $D_2=1$ when $C=L_2$,this model simplifies to\n",
        "\n",
        "$$\n",
        "E(Y\\mid X, C) =\n",
        "\\begin{cases}\n",
        "  \\beta_0+\\beta_1 X & \\mathrm{if}\\;C = L_1 \\\\\n",
        "  (\\beta_0 + \\beta_2) +\\beta_1 X & \\mathrm{if}\\;C = L_2.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Question:** What will the vertical distance between the lines be?\n",
        "\n",
        "The separate lines regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X,C)=\\beta_0+\\beta_1 X+\\beta_2 D_2 + \\beta_{3} XD_2.\n",
        "$$\n",
        "\n",
        "This model simplifies to\n",
        "\n",
        "$$\n",
        "E(Y\\mid X, C) =\n",
        "\\begin{cases}\n",
        "  \\beta_{0}+\\beta_1 X & \\mathrm{if}\\;C = L_1 \\\\\n",
        "  (\\beta_{0} + \\beta_{2}) +(\\beta_1 + \\beta_{3}) X & \\mathrm{if}\\;C = L_2.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "-   What is the vertical distance between the separate lines when $X = 0$?\n",
        "-   What is the difference in the rate of change of the separate lines?\n",
        "\n",
        "## More complex models with categorical predictors\n",
        "\n",
        "If we have a categorical predictor $C$ with $K$ levels $L_1, L_2, \\ldots, L_K$:\n",
        "\n",
        "-   We can add indicator variables $D_2, D_3, \\ldots, D_K$ to a simple linear regression model to create a parallel lines model for each level of $C$.\n",
        "-   We can add regressors $D_2, D_3, \\ldots, D_K$, and $X D_2, X D_3, \\ldots, X D_K$ to a simple linear regression model to create a separate lines model for each level of $C$.\n",
        "\n",
        "It is easy to imagine using multiple categorical predictors in a model, interacting one or more categorical predictors with one or more numeric regressors in model, etc.\n",
        "\n",
        "We can easily fit these more complex models using R, but they are more difficult to interpret.\n",
        "\n",
        "## Avoiding collinearity\n",
        "\n",
        "Consider the setting where $C$ has only 2 levels.\n",
        "\n",
        "Why don’t we add $D_1$ to the parallel lines model that already has $D_2$? Or $D_1$ and $D_1 X$ to the separate lines model that already has $D_2$ and $D_2 X$?\n",
        "\n",
        "-   We don’t *need* to add them.\n",
        "    -   If an observation doesn’t have level $L_2$ ($D_2=0$), then it must have level $L_1$.\n",
        "-   To avoid linear dependencies in the columns of the regressor matrix $\\mathbf{X}$!\n",
        "\n",
        "Consider a categorical variable $C$ with two only levels $L_1$ and $L_2$.\n",
        "\n",
        "-   Let $\\mathbf{d}_1=[d_{1,1}, d_{2,1}, \\ldots, d_{n,1}]$ denote the column vector of observed values for indicator variable $D_1$.\n",
        "-   Let $\\mathbf{d}_2$ be the column vector for $D_2$.\n",
        "-   Then $\\mathbf{d}_1 + \\mathbf{d}_2$ is an $n\\times 1$ vector of 1s.\n",
        "-   $D_1$ and $D_2$ will be linearly dependent with the intercept column of our $\\mathbf{X}$ matrix, which creates estimation problems.\n",
        "\n",
        "For a categorical predictor with $K$ levels, we only need indicator variables for $K-1$ levels of the categorical predictor.\n",
        "\n",
        "-   The **reference level** is the level of our categorical variable that doesn’t have an indicator variable in the regression model.\n",
        "-   R automatically chooses the first level of a categorical (`factor`) variable to be the reference level, so we adopt that convention.\n",
        "\n",
        "# Penguins example with categorical predictor\n",
        "\n",
        "We display the grouped scatter plot of `bill_length_mm` versus `body_mass_g` that distinguishes the `species` of each observation."
      ],
      "id": "4db8467d-c27d-4dc9-a34b-b1f039cf72b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(ggplot2) # load package\n",
        "# create grouped scatterplot\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm, shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\")"
      ],
      "id": "23672f15-70d8-41e6-b005-3b41b7e6c0b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        "-   Does the relationship between bill length and body mass change depending on the species of penguin?\n",
        "-   Should we fit a parallel or separate lines to this data?\n",
        "\n",
        "## Using a categorical variable in the `lm` function\n",
        "\n",
        "How do we use a categorical variable in R’s `lm` function?\n",
        "\n",
        "-   Each categorical variable should be a `factor`.\n",
        "-   The `lm` function will automatically convert a `factor` variable to the correct number of indicator variables when we include the `factor` variable in our `formula` argument.\n",
        "-   To add a main effect term for a categorical predictor, we simply add the term to our `lm` formula.\n",
        "-   To create an interaction term, we use `:` between the interacting variables.\n",
        "    -   E.g., if `c` is a `factor` variable and `x` is a `numeric` variable, we use the syntax `c:x` in our `formula` to get all the interactions between `c` and `x`.\n",
        "\n",
        "Our categorical predictor `species` has the levels `Adelie`, `Chinstrap`, and `Gentoo`.\n",
        "\n",
        "-   The first level of species is `Adelie`, so R will treat that level as the reference level.\n",
        "-   R will automatically create indicator variables for the levels `Chinstrap` and `Gentoo`.\n",
        "\n",
        "Let $D_C$ denote the indicator variable for the `Chinstrap` level and $D_G$ denote the indicator variable for the `Gentoo` level.\n",
        "\n",
        "## Fitting a parallel lines model\n",
        "\n",
        "We fit the parallel lines regression model $$\n",
        "E(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) = \\beta_{0} + \\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 D_C + \\beta_3 D_G.\n",
        "$$"
      ],
      "id": "5f5a03d9-d2a6-4984-8352-9817cb56f9ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit parallel lines model\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species, data = penguins)"
      ],
      "id": "b8cc476e-a086-47b6-84f5-f7fa0a5bf438"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmodp) # extract coefficients"
      ],
      "id": "07a115fe-5bcd-4505-a4c4-6d926394c5d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted parallel lines model is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 D_C + 3.56 D_G.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Note that $D_C$ is `speciesChinstrap` and $D_G$ is `speciesGentoo`.\n",
        "\n",
        "When an observation has `species` level `Adelie`, then the model simplifies to\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Adelie}) \\\\\n",
        "&=24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 \\cdot 0 + 3.56 \\cdot 0 \\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$ When an observation has `species` level `Chinstrap`, then the model simplifies to\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&=24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 \\cdot 1 + 3.56 \\cdot 0 \\\\\n",
        "&= 34.84 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "When an observation has `species` level `Gentoo`, then the model simplifies to\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Gentoo}) \\\\\n",
        "&=24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 \\cdot 0 + 3.56 \\cdot 1 \\\\\n",
        "&= 28.48 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We add our fitted lines for each `species` to our scatter plot.\n",
        "\n",
        "Let’s try adding our fitted values to the `penguins` data frame.\n",
        "\n",
        "-   We use the `predict` function to obtained the fitted values of our fitted model.\n",
        "-   We use the `transform` function to add those values as the `pl_fitted` variable in the `penguins` data frame."
      ],
      "id": "997a0b34-eb2a-470a-a41f-806238704a14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(pl_fitted = predict(lmodp))"
      ],
      "id": "9ea34479-f99e-49f4-b511-18af0e0186ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Why are we getting this error?\n",
        "\n",
        "To handle this error, we refit our model while setting the `na.action` argument to `na.exclude`. As stated in the Details section of the documentation for the `lm` function (run `?lm` in the Console):\n",
        "\n",
        "> $\\ldots$ when `na.exclude` is used the residuals and predictions are padded to the correct length by inserting `NA`s for cases omitted by `na.exclude`.\n",
        "\n",
        "We refit the parallel lines model below with `na.action = na.exclude` and then repeat what we did before."
      ],
      "id": "01a69cee-38d7-47a7-b00c-7f16604e259a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# refit parallel lines model with new na.action behavior\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species, data = penguins, na.action = na.exclude)"
      ],
      "id": "6663ce8f-0b04-41ce-886b-e16f386e5eff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add fitted values to penguins data frame\n",
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(pl_fitted = predict(lmodp))"
      ],
      "id": "fb9bb789-2914-4fbe-a31a-bda68a9832ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use the `geom_line` function to add the fitted lines for each `species` level to our scatter plot."
      ],
      "id": "bc56148a-869f-4da1-9cc9-7934c6792af2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create plot\n",
        "# create scatterplot\n",
        "# customize labels\n",
        "# add lines for each level of species\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm, shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\") +\n",
        "  geom_line(aes(x = body_mass_g, y = pl_fitted, color = species))"
      ],
      "id": "e4170fd7-3e4d-40ce-8767-6da551706673"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting a separate lines model\n",
        "\n",
        "We now fit the separate lines regression model\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&E(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= \\beta_{0} + \\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 D_C + \\beta_3 D_G + \\beta_4 \\mathtt{body\\_mass\\_g} D_C + \\beta_5 \\mathtt{body\\_mass\\_g} D_G.\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "id": "af0dab60-b387-4cfa-82eb-9948a6290f30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit separate lines model\n",
        "# na.omit = na.exclude used to change predict behavior\n",
        "lmods <- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species,\n",
        "            data = penguins, na.action = na.exclude)\n",
        "# extract estimated coefficients\n",
        "coef(lmods)"
      ],
      "id": "108f190e-7a1c-489f-a0f3-71df726ac742"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted separate lines model is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= 26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 D_C - 0.25 D_G \\\\\n",
        "&\\quad + 0.001 \\mathtt{body\\_mass\\_g} D_C + 0.0009 \\mathtt{body\\_mass\\_g} D_G,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "When an observation has `species` level `Adelie`, the model simplifies to\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Adelie}) \\\\\n",
        "&=26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 \\cdot 0 - 0.25 \\cdot 0\\\\\n",
        "&\\quad + 0.001 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 0 + 0.0009 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 0\\\\\n",
        "&= 26.99 + 0.003 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "When an observation has `species` level `Chinstrap`, the model simplifies to $$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&=26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 \\cdot 1 - 0.25 \\cdot 0 \\\\\n",
        "&\\quad + 0.001 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 1 + 0.0009 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 0 \\\\\n",
        "&= 32.17 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "When an observation has `species` level `Gentoo`, the model simplifies to\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&=26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 \\cdot 0 - 0.25 \\cdot 1 \\\\\n",
        "&\\quad + 0.001 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 0 + 0.0009 \\cdot \\mathtt{body\\_mass\\_g} \\cdot 1 \\\\\n",
        "&= 26.74 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We use the code below to display the fitted lines for the separate lines model on the `penguins` data."
      ],
      "id": "d2c9a842-a4cf-40f9-9c1c-9c24ede60a98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add separate lines fitted values to penguins data frame\n",
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(sl_fitted = predict(lmods))\n",
        "# use geom_line to add fitted lines to plot\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm, shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\") +\n",
        "  geom_line(aes(x = body_mass_g, y = sl_fitted, col = species))"
      ],
      "id": "75d30a89-39bd-489f-9494-5408055753aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:** Do the fitted lines match the observed data behavior reasonably well?\n",
        "\n",
        "# Evaluating model fit\n",
        "\n",
        "## The coefficient of determination\n",
        "\n",
        "The **coefficient of determination** is the most basic statistic measuring the fit of a regression model.\n",
        "\n",
        "The coefficient of determination is defined as\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2}.\n",
        "$$\n",
        "\n",
        "## Some sum-of-squares statistics\n",
        "\n",
        "To interpret the $R^2$ statistic, we need to introduce some new “sum-of-squares” statistics similar to the RSS.\n",
        "\n",
        "The **total sum of squares** (corrected for the mean) is computed as\n",
        "\n",
        "$$\n",
        "TSS = \\sum_{i=1}^n(Y_i-\\bar{Y})^2.\n",
        "$$\n",
        "\n",
        "-   The TSS is the sum of the squared deviations of the response values from the sample mean.\n",
        "-   It has a more insightful interpretation.\n",
        "\n",
        "Consider the **constant mean model**, which is the model\n",
        "\n",
        "$$\n",
        "E(Y)=\\beta_0.\n",
        "$$\n",
        "\n",
        "Using basic calculus, we can show that the OLS estimator of $\\beta_0$ for the constant mean model is $\\hat{\\beta}_0=\\bar{Y}$.\n",
        "\n",
        "For the constant mean model:\n",
        "\n",
        "-   $\\hat{Y}_i=\\hat{\\beta}_0$ for $i=1,2,\\ldots,n$.\n",
        "-   The RSS of the constant mean model is $\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2=\\sum_{i=1}^n(Y_i-\\bar{Y})^2$.\n",
        "\n",
        "*The TSS is the RSS for the constant mean model*.\n",
        "\n",
        "The **regression sum-of-squares** or **model sum-of-squares** is defined as\n",
        "\n",
        "$$\n",
        "SS_{reg} = \\sum_{i=1}^n(\\hat{Y}_i-\\bar{Y})^2.\n",
        "$$\n",
        "\n",
        "-   SS<sub>reg</sub> is the sum of the squared deviations between the fitted values of a model and the fitted values of the constant mean model.\n",
        "\n",
        "We have the following relationship between TSS, RSS, and SS<sub>reg</sub>:\n",
        "\n",
        "$$\n",
        "TSS = RSS + SS_{reg}.\n",
        "$$\n",
        "\n",
        "$SS_{reg}=TSS-RSS$.\n",
        "\n",
        "-   SS<sub>reg</sub> measures the reduction in the RSS when comparing the fitted model to the constant mean model.\n",
        "\n",
        "## Equivalent expressions for $R^2$\n",
        "\n",
        "Some equivalent expressions for $R^2$ are\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "R^2 &= 1-\\frac{\\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2} \\\\\n",
        "&= 1 - \\frac{RSS}{TSS} \\\\\n",
        "&= \\frac{TSS - RSS}{TSS} \\\\\n",
        "&= \\frac{SS_{reg}}{TSS} \\\\\n",
        "&= [\\mathrm{cor}(\\mathbf{y}, \\hat{\\mathbf{y}})]^2.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The last expression is the squared sample correlation between the observed and fitted values, and is a helpful way to express the coefficient of determination because it extends to regression models that are not linear.\n",
        "\n",
        "*The coefficient of determination is the proportional reduction in RSS when comparing the fitted model to the constant mean model*.\n",
        "\n",
        "## Comments about the coefficient of determination\n",
        "\n",
        "-   $0\\leq R^2 \\leq 1$.\n",
        "-   $R^2=0$ for the constant mean model.\n",
        "-   $R^2=1$ for a fitted model that perfectly fits the data (the fitted values match the observed response values).\n",
        "-   Generally, larger values of $R^2$ suggest that the model explains more variation in the response variable.\n",
        "-   Smaller $R^2$ values suggest the fitted model explains less of the response variation.\n",
        "-   The `Multiple R-squared` value printed by the `summary` of an `lm` object is $R^2$.\n",
        "-   To extract $R^2$ from a fitted model, we use the syntax `summary(lmod)$r.squared`, where `lmod` is oour fitted model.\n",
        "\n",
        "## Examples of $R^2$\n",
        "\n",
        "Consider the examples below relating $R^2$ to various fitted simple linear regression models."
      ],
      "id": "a9b9c073-ece0-46e2-b9b6-9ded0cb8051c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAHgCAMAAABNUi8GAAAAdVBMVEUAAAAAADoAAGYAOjoAOpAA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\nAs1/AeRJNEXgNU31AAAAAElFTkSuQmCC\n"
          }
        }
      ],
      "source": [],
      "id": "f937252d-1dd0-463b-83ac-9fee5de8fabd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The coefficient of determination for the parallel lines model fit to the `penguins` data is 0.81."
      ],
      "id": "c092f097-76ec-4e79-9434-0b78149db683"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmodp)$r.squared"
      ],
      "id": "1b105d46-e162-47c5-9c29-ed853f10feb7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By adding the `body_mass_g` and `species` predictors to the constant mean model of `bill_length_mm`, we reduced the RSS by 81%.\n",
        "\n",
        "## Cautions about $R^2$\n",
        "\n",
        "It is not wise to use $R^2$ to choose between models:\n",
        "\n",
        "-   $R^2$ never decreases as regressors are added to an existing model.\n",
        "    -   We can increase $R^2$ by simply adding regressors to our existing model even if the regressors are non-sensical.\n",
        "-   $R^2$ doesn’t tell us whether a model adequately describes the pattern of the observed data.\n",
        "    -   $R^2$ is a useful statistic for measuring model fit when there is approximately a linear relationship between the response values and fitted values.\n",
        "\n",
        "Let’s add some noise as a regressor to our parallel lines model."
      ],
      "id": "12839d1b-d8cc-4be1-8d59-0972d3a6aba6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(28) # for reproducibility\n",
        "# create regressor of random noise\n",
        "noisyx <- rnorm(344)\n",
        "# add noisyx as regressor to lmodp\n",
        "lmod_silly <- update(lmodp, . ~ . + noisyx)\n",
        "# extract R^2 from fitted model\n",
        "summary(lmod_silly)$r.squared"
      ],
      "id": "c20ad237-74b0-4964-b58b-23cba52926be"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The $R^2$ value increased from 0.8080 to 0.8088!\n",
        "\n",
        "## Anscombe’s Quartet\n",
        "\n",
        "Anscombe (1973) provides a canonical data set known as “Anscombe’s quartet” that illustrates how $R^2$ can mislead us into thinking an inappropriate model fits better than it actually does.\n",
        "\n",
        "-   The data set is comprised of 4 different data sets.\n",
        "-   When a simple linear regression model is fit to each data set:\n",
        "    -   $\\hat{\\beta}_0=3$.\n",
        "    -   $\\hat{\\beta}_1=0.5$.\n",
        "    -   $R^2=0.67$.\n",
        "\n",
        "Anscombe’s quartet is available as the `anscombe` data set in the **datasets** package. The data set includes 11 observations of 8 variables. The variables are:\n",
        "\n",
        "-   `x1`, `x2`, `x3`, `x4`: the regressor variable for each individual data set.\n",
        "-   `y1`, `y2`, `y3`, `y4`: the response variable for each individual data set.\n",
        "\n",
        "We fit a simple linear regression model to each of the 4 data sets."
      ],
      "id": "b166f1d1-e589-42cf-93ea-03c8c63e01d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lmod_a1 <- lm(y1 ~ x1, data = anscombe) # fit model to first data set\n",
        "lmod_a2 <- lm(y2 ~ x2, data = anscombe) # fit model to second data set\n",
        "lmod_a3 <- lm(y3 ~ x3, data = anscombe) # fit model to third data set\n",
        "lmod_a4 <- lm(y4 ~ x4, data = anscombe) # fit model to fourth data set"
      ],
      "id": "81fedc48-7379-47e6-a060-2ffea50d1847"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the estimated coefficients from each fitted model to confirm that the estimated coefficients are nearly identical."
      ],
      "id": "08f7c9d9-630f-4c00-8f5a-5754efd426c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmod_a1)"
      ],
      "id": "7e3dcfb5-8df9-43dc-92b4-29a40a3cf990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmod_a2)"
      ],
      "id": "7c6306e3-3461-4252-aa9b-d66fa30b8972"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmod_a3)"
      ],
      "id": "ed44aaba-957d-40d0-b2b0-f74c7f921c25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmod_a4)"
      ],
      "id": "4b5aa199-b86c-4a64-b330-06d26065dc11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract $R^2$ from each fitted model to confirm that the values are nearly identical."
      ],
      "id": "356c713d-ea33-4b0a-a7fb-13df3befbc3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmod_a1)$r.squared"
      ],
      "id": "32e5b3b7-db10-42fe-8c3d-4426eb0080f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmod_a2)$r.squared"
      ],
      "id": "6d446dcf-1c6b-456d-b0bb-46d9bbb99a0b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmod_a3)$r.squared"
      ],
      "id": "a461888d-b780-4da5-bc18-275e686c5c06"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmod_a4)$r.squared"
      ],
      "id": "a381d84a-0ec5-49e8-a846-94d5a3e5f09e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We overlay the fitted models on each data set."
      ],
      "id": "075317b9-e2ea-4465-8979-ffc9dbedac62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAHgCAMAAABNUi8GAAABGlBMVEUAAAAAADoAAGYAOpAAZrYA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\nRU5ErkJggg==\n"
          }
        }
      ],
      "source": [],
      "id": "060b4bec-1a90-4b21-8315-d3301178bfe1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        "-   Do all models describe their data equally well?\n",
        "-   If not, comment on the deficiencies of the poorly fitting models.\n",
        "\n",
        "## Adjusted R-squared\n",
        "\n",
        "Ezekiel (1930) proposed the adjusted R-squared statistic as a better statistic for measuring model fit. The adjusted $R^2$ statistic is defined as\n",
        "\n",
        "$$\n",
        "R^2_a=1-(1-R^2)\\frac{n-1}{n-p}=1-\\frac{RSS/(n-p)}{TSS/(n-1)}.\n",
        "$$\n",
        "\n",
        "-   The $R^2_a$ will only increase when a regressor substantively improves the fit of the model to the observed data.\n",
        "-   We favor models with larger values of $R^2_a$.\n",
        "\n",
        "What are the $R^2_a$ values for the 4 models we previously fit to the `penguins` data? Which is the “best” model?"
      ],
      "id": "50d24a55-4a35-4787-bf75-b09fe422da50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmod)$adj.r.squared  # simple linear regression model\n",
        "summary(mlmod)$adj.r.squared # multiple linear regression model\n",
        "summary(lmodp)$adj.r.squared # parallel lines model\n",
        "summary(lmods)$adj.r.squared # separate lines model"
      ],
      "id": "b7caa49b-74e7-4923-b4a5-e492cf891e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We double-check that the separate lines model is a sensible model."
      ],
      "id": "a313baa5-0a43-4f75-b608-a162de3396ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(penguins$bill_length_mm ~ fitted(lmods),\n",
        "     xlab = \"fitted values\", ylab = \"bill length (mm)\")\n",
        "abline(0, 1) # add 45 degree line"
      ],
      "id": "8ed04fae-63f0-4e6f-9bdc-d49d543eb10c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary of objects\n",
        "\n",
        "We provide an overview of objects used to define a linear model and their associated properties.\n",
        "\n",
        "| Object | Description | Observable? | Random? |\n",
        "|:--------------|:-------------------------------------------|:-------|:-----|\n",
        "| $Y$ | response variable | Yes | Yes |\n",
        "| $Y_i$ | response value for the $i\\text{th}$ observation | Yes | Yes |\n",
        "| $\\mathbf{y}$ | $n\\times 1$ column vector of response values | Yes | Yes |\n",
        "| $X$ | regressor variable | Yes | No |\n",
        "| $X_j$ | $j$th regressor variable | Yes | No |\n",
        "| $x_{i,j}$ | value of the $j$th regressor variable for the $i\\text{th}$ observation | Yes | No |\n",
        "| $\\mathbf{X}$ | $n\\times p$ matrix of regressor values | Yes | No |\n",
        "| $\\mathbf{x}_i$ | $p\\times 1$ vector of regressor values for the $i\\text{th}$ observation | Yes | No |\n",
        "| $\\beta_j$ | coefficient associated with the $j$th regressor variable | No | No |\n",
        "| $\\boldsymbol{\\beta}$ | $p\\times 1$ column vector of regression coefficients | No | No |\n",
        "| $\\epsilon$ | model error | No | Yes |\n",
        "| $\\epsilon_i$ | error of the $i\\text{th}$ observation | No | Yes |\n",
        "| $\\boldsymbol{\\epsilon}$ | $n\\times 1$ column vector of model errors | No | Yes |\n",
        "\n",
        "# Summary of functions\n",
        "\n",
        "An overview of important functions discussed in this chapter.\n",
        "\n",
        "| Function | Purpose |\n",
        "|:----------|:------------------------------------------------------------|\n",
        "| `lm` | Fits a linear model based on a provided `formula` |\n",
        "| `summary` | Provides summary information about the fitted model |\n",
        "| `coef` | Extracts the vector of estimated regression coefficients from the fitted model |\n",
        "| `residuals` | Extracts the vector of residuals from the fitted model |\n",
        "| `fitted` | Extracts the vector of fitted values from the fitted model |\n",
        "| `predict` | Computes the fitted values (or arbitrary predictions) based on a fitted model |\n",
        "| `deviance` | Extracts the RSS of a fitted model |\n",
        "| `sigma` | Extracts $\\hat{\\sigma}$ from the fitted model |\n",
        "| `update` | Updates a fitted model to remove or add regressors |\n",
        "\n",
        "# Going Deeper\n",
        "\n",
        "## Derivation of the OLS estimators of the simple linear regression model coefficients\n",
        "\n",
        "Assume a simple linear regression model with $n$ observations.\n",
        "\n",
        "The residual sum of squares for the simple linear regression model is\n",
        "\n",
        "$$\n",
        "RSS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)^2.\n",
        "$$\n",
        "\n",
        "### OLS estimator of $\\beta_0$\n",
        "\n",
        "First, we take the partial derivative of the RSS with respect to $\\hat\\beta_0$ and simplify.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial RSS(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0} &= \\frac{\\partial}{\\partial \\hat\\beta_0}\\sum_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)^2 & \\tiny\\text{ (substituting the formula for the RSS)} \\\\\n",
        "&= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\hat\\beta_0}(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)^2  & \\tiny\\text{ (by the linearity property of derivatives)} \\\\\n",
        "&= -2\\sum_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i). & \\tiny\\text{ (chain rule, factoring out -2)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We set the partial derivative of the RSS with respect to $\\hat{\\beta}_0$ equal to zero and solve for $\\hat{\\beta}_0$ in terms of $\\hat{\\beta}_1$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "0 &= \\frac{\\partial RSS(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_0}  &  \\\\\n",
        "0 &= -2\\sum_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) & \\tiny\\text{ (substitute partial deriviative)}\\\\\n",
        "0 &= \\sum_{i=1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i) & \\tiny\\text{ (divide both sides by -2)} \\\\\n",
        "0 &= \\sum_{i=1}^n Y_i - \\sum_{i=1}^n\\hat\\beta_0 - \\sum_{i=1}^n\\hat\\beta_1x_i &\\tiny\\text{ (by linearity of sum)} \\\\\n",
        "0 &= \\sum_{i=1}^n Y_i - n\\hat\\beta_0 - \\sum_{i=1}^n\\hat\\beta_1x_i & \\tiny(\\text{summing }\\hat\\beta_0\\ n\\text{ times equals }n\\hat\\beta_0) \\\\\n",
        "n\\hat\\beta_0 &= \\sum_{i=1}^n Y_i-\\hat{\\beta}_1\\sum_{i=1}^nx_i. &\\tiny\\text{ (algebra rearrange, factorring }\\hat{\\beta}_1\\text{)} \\\\\n",
        "\\hat\\beta_0 &= \\bar Y-\\hat\\beta_1\\bar x &\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### OLS Estimator of $\\beta_1$\n",
        "\n",
        "We start by taking the partial derivative of the RSS with respect to $\\hat{\\beta}_1$ and simplifying.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial RSS(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} &= \\frac{\\partial}{\\partial \\hat\\beta_1}\\sum_{i=1}^n (Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)^2 & \\tiny\\text{ (substitute formula for RSS)} \\\\\n",
        "&= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\hat\\beta_1}(Y_i - \\hat\\beta_0 - \\hat\\beta_1x_i)^2 & \\tiny\\text{ (linearity property of derivatives)} \\\\\n",
        "&= -2\\sum_{i=1}^n(Y_i-\\hat\\beta_0-\\hat\\beta_1x_i)x_i & \\tiny\\text{ (chain rule, factor out -2)}\n",
        "\\end{aligned}\n",
        "$$ We now set this partial derivative equal to 0 and rearrange the terms to solve for $\\hat{\\beta}_1$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "0 &= \\frac{\\partial RSS(\\hat\\beta_0, \\hat\\beta_1)}{\\partial \\hat\\beta_1} & \\\\\n",
        "0 &= -2\\sum_{i=1}^n(Y_i-\\hat\\beta_0-\\hat\\beta_1x_i)x_i &\\tiny\\text{(substitute partial derivative})\\\\\n",
        "0 &= \\sum_{i=1}^n(Y_i-(\\bar Y -\\hat \\beta_1\\bar x)-\\hat\\beta_1x_i)x_i &\\tiny\\text{(substitute OLS estimator of }\\hat\\beta_0, \\text{ divide both sides by -2}) \\\\\n",
        "0 &= \\sum_{i=1}^n x_iY_i -\\sum_{i=1}^n x_i\\bar Y+\\hat\\beta_1\\bar x\\sum_{i=1}^n x_i-\\hat\\beta_1\\sum_{i=1}^n x_i^2. &\\tiny\\text{(expand sum, use linearity of sum)} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Continuing from the previous line, we move the terms involving $\\hat{\\beta}_1$ to the other side of the equality.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat\\beta_1\\sum_{i=1}^n x_i^2-\\hat\\beta_1\\bar x\\sum_{i=1}^n x_i &=\\sum_{i=1}^n x_iY_i -\\sum_{i=1}^n x_i\\bar Y & \\tiny\\text{(move estimator to other side)}\\\\\n",
        "\\hat\\beta_1\\sum_{i=1}^n x_i^2-\\hat\\beta_1\\frac{1}{n}\\sum_{i=1}^n  x_i\\sum_{i=1}^n x_i&=\\sum_{i=1}^n x_iY_i -\\sum_{i=1}^n x_i\\frac{1}{n}\\sum_{i=1}^n  Y_i  &\\tiny\\text{(rewrite using definition of sample means)} \\\\\n",
        "\\hat\\beta_1\\sum_{i=1}^n x_i^2-\\hat\\beta_1\\frac{1}{n}\\left(\\sum_{i=1}^n  x_i\\right)^2 &=\\sum_{i=1}^n x_iY_i -\\frac{1}{n}\\sum_{i=1}^n x_i\\sum_{i=1}^n  Y_i  & \\tiny\\text{(reorder and simplify)} \\\\\n",
        "\\hat\\beta_1\\left(\\sum_{i=1}^n x_i^2-\\frac{1}{n}\\left(\\sum_{i=1}^n  x_i\\right)^2\\right)&=\\sum_{i=1}^n x_iY_i -\\frac{1}{n}\\sum_{i=1}^n x_i\\sum_{i=1}^n  Y_i, & \\tiny\\text{(factoring)}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "which allows us to obtain\n",
        "\n",
        "$$\n",
        "\\hat\\beta_1=\\frac{\\sum_{i=1}^n x_iY_i -\\frac{1}{n}\\sum_{i=1}^n x_i\\sum_{i=1}^n  Y_i}{\\sum_{i=1}^n x_i^2-\\frac{1}{n}\\left(\\sum_{i=1}^n  x_i\\right)^2}.\n",
        "$$\n",
        "\n",
        "## Derivation of the OLS estimator for the multiple linear regression model coefficients\n",
        "\n",
        "We want to determine the value of $\\hat{\\boldsymbol{\\beta}}$ that will minimize\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "RSS(\\hat{\\boldsymbol{\\beta}}) &=\\sum_{i=1}^n \\hat{\\epsilon_i}^2 \\\\\n",
        "&= \\hat{\\boldsymbol{\\epsilon}}^T\\hat{\\boldsymbol{\\epsilon}} \\\\\n",
        "&= (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "&= \\mathbf{y}^T\\mathbf{y}-2\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}+\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where the second term in the last line comes from the fact that $\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}$ is a $1\\times 1$ matrix, and is thus symmetric. Consequently, $\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y}=(\\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\mathbf{y})^T=\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\n",
        "\n",
        "To find the local extrema of $RSS(\\hat{\\boldsymbol{\\beta}})$, we set its derivative with respect to $\\hat{\\boldsymbol{\\beta}}$ equal to 0, and solve for $\\hat{\\boldsymbol{\\beta}}$.\n",
        "\n",
        "We see that\n",
        "\n",
        "$$\n",
        "\\frac{\\partial RSS(\\hat{\\boldsymbol{\\beta}})}{\\partial\\hat{\\boldsymbol{\\beta}}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n",
        "$$\n",
        "\n",
        "Setting $\\partial RSS(\\hat{\\boldsymbol{\\beta}})/\\partial\\hat{\\boldsymbol{\\beta}}=0$ and using some simple algebra, we derive the **normal equations**\n",
        "\n",
        "$$\n",
        "\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}=\\mathbf{X}^T\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "Assuming the $\\mathbf{X}^T\\mathbf{X}$ is invertible, which it will be when $\\mathbf{X}$ is full-rank, our solution is\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "To show that the OLS estimator of $\\boldsymbol{\\beta}$ minimizes $RSS(\\hat{\\boldsymbol{\\beta}})$, we technically need to show that the Hessian matrix of $RSS(\\hat{\\boldsymbol{\\beta}})$, the matrix of second-order partial derivatives, is positive definite. In our context, the Hessian matrix is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial^2 RSS(\\hat{\\boldsymbol{\\beta}})}{\\partial \\hat{\\boldsymbol{\\beta}}^2} &= \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "&= 2\\mathbf{X}^T\\mathbf{X}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The $p\\times p$ matrix $2\\mathbf{X}^T\\mathbf{X}$ is positive definite, but it is beyond the scope of the course to prove this.\n",
        "\n",
        "Therefore, the OLS estimator of $\\boldsymbol{\\beta}$,\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\n",
        "$$ minimizes the RSS.\n",
        "\n",
        "# References\n",
        "\n",
        "Anscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\n",
        "\n",
        "Ezekiel, Mordecai. 1930. “Methods of Correlation Analysis.”\n",
        "\n",
        "Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. <https://doi.org/10.1371/journal.pone.0090081>.\n",
        "\n",
        "Weisberg, Sanford. 2014. Applied Linear Regression. Fourth. Hoboken NJ: Wiley. <http://z.umn.edu/alr4ed>.\n",
        "\n",
        "Wilkinson, GN, and CE Rogers. 1973. “Symbolic Description of Factorial Models for Analysis of Variance.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 22 (3): 392–99."
      ],
      "id": "ed16db02-066f-4d10-a756-77eeee5a80c7"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    }
  }
}